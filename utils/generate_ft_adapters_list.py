import importlib

def generate_ft_adapters_list(log_run_name: str, simplest_prompt:bool=False) -> 'list[str]':
    """
    Given a run of several configurations for the qlora fine-tuning, this function returns the list of the adapters generated by the run.
    """
    adapters_list = []
    module_name = f"log.{log_run_name}"
    models_params = importlib.import_module(module_name)
    
    quantization = models_params.quantization
    load_in_4bit_list = models_params.load_in_4bit
    bnb_4bit_quant_type_list = models_params.bnb_4bit_quant_type
    bnb_4bit_compute_dtype_list = models_params.bnb_4bit_compute_dtype
    llm_int8_threshold_list = models_params.llm_int8_threshold
    r_list = models_params.r
    lora_alpha_list = models_params.lora_alpha
    lora_dropout_list = models_params.lora_dropout
    gradient_accumulation_steps_list = models_params.gradient_accumulation_steps
    learning_rate_list = models_params.learning_rate

    for model_loading_params_idx in range(len(load_in_4bit_list)):
        load_in_4bit = load_in_4bit_list[model_loading_params_idx]
        load_in_8bit = not load_in_4bit
        bnb_4bit_quant_type = bnb_4bit_quant_type_list[model_loading_params_idx]
        bnb_4bit_compute_dtype = bnb_4bit_compute_dtype_list[model_loading_params_idx]
        llm_int8_threshold = llm_int8_threshold_list[model_loading_params_idx]
        for r in r_list:
            for lora_alpha in lora_alpha_list:
                for lora_dropout in lora_dropout_list:
                    for gradient_accumulation_steps in gradient_accumulation_steps_list:
                        for learning_rate in learning_rate_list:
                            nbits = 4
                            if load_in_8bit:
                                nbits = 8   
                            if not quantization:
                                nbits = 'NoQuant'
                            extra_str_cl = ""
                            try:
                                if models_params.clent:
                                    extra_str_cl += "_clent"
                            except:
                                pass
                            if models_params.model_name.lower().startswith('qwen') or (not quantization and models_params.BASE_MODEL_CHECKPOINT == "mistralai/Mistral-7B-Instruct-v0.2") or models_params.BASE_MODEL_CHECKPOINT == "mii-community/zefiro-7b-base-ITA":
                                ADAPTERS_CHECKPOINT = f"ferrazzipietro/{models_params.model_name.lower()}__adapters_{models_params.TRAIN_LAYER}_{nbits}_{bnb_4bit_compute_dtype}_{r}_{lora_alpha}_{lora_dropout}_{gradient_accumulation_steps}_{learning_rate}{extra_str_cl}"
                            else:
                                ADAPTERS_CHECKPOINT = f"ferrazzipietro/{models_params.model_name.lower()}_adapters_{models_params.TRAIN_LAYER}_{nbits}_{bnb_4bit_compute_dtype}_{r}_{lora_alpha}_{lora_dropout}_{gradient_accumulation_steps}_{learning_rate}{extra_str_cl}"
                            if simplest_prompt:
                                ADAPTERS_CHECKPOINT = f"ferrazzipietro/{models_params.model_name}_simplest_prompt_adapters_{models_params.TRAIN_LAYER}_{nbits}_{bnb_4bit_compute_dtype}_{r}_{lora_alpha}_{lora_dropout}_{gradient_accumulation_steps}_{learning_rate}{extra_str_cl}"
                            
                            adapters_list.append(ADAPTERS_CHECKPOINT)
    return adapters_list