/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning
  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')
adapters_list:   0%|          | 0/36 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|███▎      | 1/3 [00:26<00:52, 26.16s/it][A
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:53<00:26, 26.99s/it][A
Loading checkpoint shards: 100%|██████████| 3/3 [01:11<00:00, 22.68s/it][ALoading checkpoint shards: 100%|██████████| 3/3 [01:11<00:00, 23.76s/it]

adapter_config.json:   0%|          | 0.00/548 [00:00<?, ?B/s][Aadapter_config.json: 100%|██████████| 548/548 [00:00<00:00, 36.9kB/s]

adapter_model.safetensors:   0%|          | 0.00/153M [00:00<?, ?B/s][A
adapter_model.safetensors:   7%|▋         | 10.5M/153M [00:01<00:17, 8.32MB/s][A
adapter_model.safetensors:  14%|█▎        | 21.0M/153M [00:02<00:13, 9.97MB/s][A
adapter_model.safetensors:  21%|██        | 31.5M/153M [00:03<00:11, 10.7MB/s][A
adapter_model.safetensors:  27%|██▋       | 41.9M/153M [00:03<00:09, 11.2MB/s][A
adapter_model.safetensors:  34%|███▍      | 52.4M/153M [00:04<00:08, 11.3MB/s][A
adapter_model.safetensors:  41%|████      | 62.9M/153M [00:05<00:07, 11.5MB/s][A
adapter_model.safetensors:  48%|████▊     | 73.4M/153M [00:06<00:06, 11.6MB/s][A
adapter_model.safetensors:  55%|█████▍    | 83.9M/153M [00:07<00:05, 11.6MB/s][A
adapter_model.safetensors:  62%|██████▏   | 94.4M/153M [00:08<00:05, 11.6MB/s][A
adapter_model.safetensors:  68%|██████▊   | 105M/153M [00:09<00:04, 11.5MB/s] [A
adapter_model.safetensors:  75%|███████▌  | 115M/153M [00:10<00:03, 11.6MB/s][A
adapter_model.safetensors:  82%|████████▏ | 126M/153M [00:11<00:02, 11.7MB/s][A
adapter_model.safetensors:  89%|████████▉ | 136M/153M [00:12<00:01, 11.6MB/s][A
adapter_model.safetensors:  96%|█████████▌| 147M/153M [00:12<00:00, 11.7MB/s][A
adapter_model.safetensors: 100%|██████████| 153M/153M [00:13<00:00, 11.6MB/s][Aadapter_model.safetensors: 100%|██████████| 153M/153M [00:13<00:00, 11.4MB/s]

Map:   0%|          | 0/681 [00:00<?, ? examples/s][A
Map:  36%|███▋      | 248/681 [00:00<00:00, 2389.39 examples/s][A
Map: 100%|██████████| 681/681 [00:00<00:00, 2248.89 examples/s][AMap: 100%|██████████| 681/681 [00:00<00:00, 2145.98 examples/s]

Map:   0%|          | 0/681 [00:00<?, ? examples/s][A
Map: 100%|██████████| 681/681 [00:00<00:00, 5534.63 examples/s][AMap: 100%|██████████| 681/681 [00:00<00:00, 4980.65 examples/s]

generating responses:   0%|          | 0/681 [00:00<?, ?it/s][A2024-02-26 11:17:40.246179: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-26 11:17:40.351650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-26 11:17:44.098423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
generating responses:   0%|          | 0/681 [00:15<?, ?it/s]
adapters_list:   0%|          | 0/36 [01:47<?, ?it/s]
Traceback (most recent call last):
  File "postprocessing_llama.py", line 83, in <module>
    postprocessor.add_responses_column(model=merged_model, 
  File "/extra/ferrazzi/llm/mistral_finetuning/utils/test_data_processor.py", line 176, in add_responses_column
    tmp = self._generate_model_response(self.test_data.select(indici), model, tokenizer, max_new_tokens_factor)
  File "/extra/ferrazzi/llm/mistral_finetuning/utils/test_data_processor.py", line 152, in _generate_model_response
    generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=max_new_tokens,  pad_token_id=tokenizer.eos_token_id) # max_new_tokens=max_new_tokens,
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/peft_model.py", line 1060, in generate
    outputs = self.base_model.generate(**kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/generation/utils.py", line 1592, in generate
    return self.sample(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/generation/utils.py", line 2696, in sample
    outputs = self(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1168, in forward
    outputs = self.model(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 982, in forward
    causal_mask = self._update_causal_mask(attention_mask, inputs_embeds)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 1063, in _update_causal_mask
    self.causal_mask[None, None, :, :].repeat(batch_size, 1, 1, 1).to(dtype) * torch.finfo(dtype).min
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 23.69 GiB total capacity; 20.37 GiB already allocated; 783.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
