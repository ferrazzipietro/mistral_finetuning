/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:40<00:19, 19.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:55<00:00, 17.97s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:55<00:00, 18.56s/it]
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.
generating responses:   0%|          | 0/681 [00:00<?, ?it/s]2024-03-14 08:24:15.429069: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-14 08:24:17.124689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-14 08:24:20.979020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
generating responses:   2%|▏         | 12/681 [01:23<1:17:57,  6.99s/it]generating responses:   4%|▎         | 24/681 [02:28<1:06:20,  6.06s/it]generating responses:   5%|▌         | 36/681 [03:34<1:02:06,  5.78s/it]generating responses:   7%|▋         | 48/681 [04:39<59:28,  5.64s/it]  generating responses:   9%|▉         | 60/681 [05:43<57:24,  5.55s/it]generating responses:  11%|█         | 72/681 [06:48<55:50,  5.50s/it]generating responses:  12%|█▏        | 84/681 [07:54<54:36,  5.49s/it]generating responses:  14%|█▍        | 96/681 [08:59<53:28,  5.48s/it]generating responses:  16%|█▌        | 108/681 [10:04<52:07,  5.46s/it]generating responses:  18%|█▊        | 120/681 [11:10<51:03,  5.46s/it]generating responses:  19%|█▉        | 132/681 [12:15<49:49,  5.45s/it]generating responses:  21%|██        | 144/681 [13:21<48:58,  5.47s/it]generating responses:  23%|██▎       | 156/681 [14:27<47:52,  5.47s/it]generating responses:  25%|██▍       | 168/681 [15:33<46:51,  5.48s/it]generating responses:  26%|██▋       | 180/681 [16:39<45:50,  5.49s/it]generating responses:  28%|██▊       | 192/681 [17:44<44:35,  5.47s/it]generating responses:  30%|██▉       | 204/681 [18:51<43:49,  5.51s/it]generating responses:  32%|███▏      | 216/681 [19:58<42:47,  5.52s/it]generating responses:  33%|███▎      | 228/681 [21:03<41:29,  5.50s/it]generating responses:  35%|███▌      | 240/681 [22:08<40:11,  5.47s/it]generating responses:  37%|███▋      | 252/681 [23:14<39:14,  5.49s/it]generating responses:  39%|███▉      | 264/681 [24:20<38:08,  5.49s/it]generating responses:  41%|████      | 276/681 [25:27<37:07,  5.50s/it]generating responses:  42%|████▏     | 288/681 [26:32<35:52,  5.48s/it]generating responses:  44%|████▍     | 300/681 [27:37<34:45,  5.47s/it]generating responses:  46%|████▌     | 312/681 [28:42<33:31,  5.45s/it]generating responses:  48%|████▊     | 324/681 [29:48<32:28,  5.46s/it]generating responses:  49%|████▉     | 336/681 [30:53<31:21,  5.45s/it]generating responses:  51%|█████     | 348/681 [31:58<30:07,  5.43s/it]generating responses:  53%|█████▎    | 360/681 [33:04<29:09,  5.45s/it]generating responses:  55%|█████▍    | 372/681 [34:09<28:05,  5.46s/it]generating responses:  56%|█████▋    | 384/681 [35:14<26:58,  5.45s/it]generating responses:  58%|█████▊    | 396/681 [36:20<25:51,  5.44s/it]generating responses:  60%|█████▉    | 408/681 [37:25<24:49,  5.46s/it]generating responses:  62%|██████▏   | 420/681 [38:32<23:51,  5.48s/it]generating responses:  63%|██████▎   | 432/681 [39:37<22:40,  5.46s/it]generating responses:  65%|██████▌   | 444/681 [40:42<21:33,  5.46s/it]generating responses:  67%|██████▋   | 456/681 [41:47<20:24,  5.44s/it]generating responses:  69%|██████▊   | 468/681 [42:53<19:20,  5.45s/it]generating responses:  70%|███████   | 480/681 [43:58<18:12,  5.44s/it]generating responses:  72%|███████▏  | 492/681 [45:02<17:04,  5.42s/it]generating responses:  74%|███████▍  | 504/681 [46:07<15:59,  5.42s/it]generating responses:  76%|███████▌  | 516/681 [47:12<14:53,  5.42s/it]generating responses:  78%|███████▊  | 528/681 [48:17<13:47,  5.41s/it]generating responses:  79%|███████▉  | 540/681 [49:22<12:44,  5.42s/it]generating responses:  81%|████████  | 552/681 [50:27<11:37,  5.40s/it]generating responses:  83%|████████▎ | 564/681 [51:32<10:35,  5.43s/it]generating responses:  83%|████████▎ | 564/681 [52:15<10:50,  5.56s/it]
Traceback (most recent call last):
  File "get_results_base_model_mistral.py", line 74, in <module>
    postprocessor.add_responses_column(model=model, 
  File "/extra/ferrazzi/llm/mistral_finetuning/utils/test_data_processor.py", line 175, in add_responses_column
    tmp = self._generate_model_response(self.test_data.select(indici), model, tokenizer, max_new_tokens_factor)
  File "/extra/ferrazzi/llm/mistral_finetuning/utils/test_data_processor.py", line 150, in _generate_model_response
    generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=max_new_tokens,  pad_token_id=tokenizer.eos_token_id) # max_new_tokens=max_new_tokens,
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/generation/utils.py", line 1592, in generate
    return self.sample(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/generation/utils.py", line 2696, in sample
    outputs = self(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1170, in forward
    logits = self.lm_head(hidden_states)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 160, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 293, in pre_forward
    set_module_tensor_to_device(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 347, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 500.00 MiB (GPU 0; 23.69 GiB total capacity; 20.92 GiB already allocated; 418.69 MiB free; 21.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
