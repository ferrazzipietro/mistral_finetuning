2024-02-26 16:23:28.418502: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-26 16:23:28.503261: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-26 16:23:32.941161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
wandb: Currently logged in as: ferrazzipietro. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/ferrazzi/.netrc
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /extra/ferrazzi/llm/mistral_finetuning/wandb/run-20240226_162349-w70a16fw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run en.layer12024-02-26 16:23:49
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2_adapters_en.layer1_basic_prompt
wandb: üöÄ View run at https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2_adapters_en.layer1_basic_prompt/runs/w70a16fw
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:20<00:40, 20.15s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:43<00:22, 22.13s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:01<00:00, 20.33s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:01<00:00, 20.62s/it]
Map:   0%|          | 0/1520 [00:00<?, ? examples/s]Map:  15%|‚ñà‚ñç        | 225/1520 [00:00<00:00, 2185.61 examples/s]Map:  33%|‚ñà‚ñà‚ñà‚ñé      | 502/1520 [00:00<00:00, 2519.23 examples/s]Map:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 777/1520 [00:00<00:00, 2614.95 examples/s]Map:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1152/1520 [00:00<00:00, 2168.02 examples/s]Map:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1431/1520 [00:00<00:00, 2344.67 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 2137.46 examples/s]
Map:   0%|          | 0/1520 [00:00<?, ? examples/s]Map:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1000/1520 [00:00<00:00, 1764.49 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 1856.42 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 1717.35 examples/s]
Map:   0%|          | 0/170 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [00:00<00:00, 1502.42 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [00:00<00:00, 1287.51 examples/s]
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: üöÄ View run en.layer12024-02-26 16:23:49 at: https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2_adapters_en.layer1_basic_prompt/runs/w70a16fw
wandb: Ô∏è‚ö° View job at https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2_adapters_en.layer1_basic_prompt/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MzE5NzIxMw==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240226_162349-w70a16fw/logs
Traceback (most recent call last):
  File "finetuning_mistral.py", line 117, in <module>
    model = get_peft_model(model, lora_config)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/mapping.py", line 116, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/peft_model.py", line 973, in __init__
    super().__init__(model, peft_config, adapter_name)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/peft_model.py", line 121, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 111, in __init__
    super().__init__(model, config, adapter_name)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 94, in __init__
    self.inject_adapter(self.model, adapter_name)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 251, in inject_adapter
    self._create_and_replace(peft_config, adapter_name, target, target_name, parent, **optional_kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 193, in _create_and_replace
    new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/tuners/lora/model.py", line 281, in _create_new_module
    new_module = Linear4bit(adapter_name, target, **fourbit_kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/tuners/lora/bnb.py", line 191, in __init__
    self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/tuners/lora/layer.py", line 64, in update_layer
    if r <= 0:
TypeError: '<=' not supported between instances of 'list' and 'int'
