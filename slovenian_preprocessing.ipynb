{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "\n",
    "class DataPreprocessor():\n",
    "\n",
    "\n",
    "    def __init__(self, model_checkpoint:str, tokenizer: AutoTokenizer, token_llama:str='') -> None:\n",
    "\n",
    "        self.offset = None\n",
    "        self.instruction_on_response_format = ''\n",
    "        self.n_shots = None\n",
    "        #self.model_type = model_checkpoint.split('/')[1].lower().split('-')[0]\n",
    "        self.model_type = 'qwen' if model_checkpoint.split('/')[0] == 'Qwen' else model_checkpoint.split('/')[1].lower().split('-')[0]\n",
    "        if self.model_type == 'meta': self.model_type = 'llama3'\n",
    "        # if self.model_type == 'zefiro':\n",
    "        #     self.model_type  = 'mistral'\n",
    "        if self.model_type not in ['mistral', 'llama', 'llama3', 'gemma', 'qwen', 'zefiro', 'phi', 'minerva']:\n",
    "            raise ValueError(\"The model type must be either 'mistral', 'llama', 'llama3', 'gemma', 'zefiro', 'qwen', 'minerva' or 'phi'\")\n",
    "\n",
    "        print('MODEL TYPE:', self.model_type)\n",
    "        if isinstance(tokenizer, str):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, token = token_llama)\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "        \n",
    "        self.special_tokens_instruction_dict = {'mistral': {'user_start':'[INST]',\n",
    "                                                            'user_end':'[/INST]',\n",
    "                                                            'model_start':'',\n",
    "                                                            'model_end':''},\n",
    "                                                'llama': {'user_start':'[INST]',\n",
    "                                                          'user_end':'[/INST]',\n",
    "                                                          'model_start':'',\n",
    "                                                          'model_end':''},\n",
    "                                                'llama3': {'user_start':'<|start_header_id|>user<|end_header_id|>\\n\\n',\n",
    "                                                          'user_end':'<|eot_id|>',\n",
    "                                                          'model_start':'<|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
    "                                                          'model_end':''},\n",
    "                                                'gemma': {'user_start':'<start_of_turn>user',\n",
    "                                                          'user_end':'<end_of_turn>',\n",
    "                                                          'model_start':'<start_of_turn>model',\n",
    "                                                          'model_end':'<end_of_turn>'},\n",
    "                                                'qwen': {'user_start':'<|im_start|>user',\n",
    "                                                          'user_end':'<|im_end|>',\n",
    "                                                          'model_start':'<|im_start|>assistant',\n",
    "                                                          'model_end':'<|im_end|>'},\n",
    "                                                'zefiro': {'user_start':'<|user|>',\n",
    "                                                           'user_end':'',# 'user_end':'</s>',\n",
    "                                                           'model_start':'<|assistant|>',\n",
    "                                                           'model_end':''},# 'model_end':'</s>'},\n",
    "                                                'phi': {'user_start':'<|user|>',\n",
    "                                                           'user_end':'<|end|>\\n',\n",
    "                                                           'model_start':'<|assistant|>',\n",
    "                                                           'model_end':''},\n",
    "                                                'minerva': {'user_start':'',\n",
    "                                                            'user_end':'',\n",
    "                                                           'model_start':'',\n",
    "                                                           'model_end':''}}\n",
    "        self.special_tokens_instruction = self.special_tokens_instruction_dict[self.model_type]\n",
    "\n",
    "        self.one_shot_example = \"\"\"{user_start} {instruction_on_response_format} <<<{example_query}>>> {user_end}{model_start} {example_response} {model_end}\n",
    "\"\"\"\n",
    "        self.one_shot_example_no_offset = \"\"\"{user_start} {instruction_on_response_format} <<<{example_query}>>> {user_end}{model_start} {example_response} {model_end}\n",
    "\"\"\"\n",
    "\n",
    "        self.prompt_template = \"\"\"{user_start} {instruction_on_response_format} <<{query}>>> {user_end}{model_start}\"\"\"\n",
    "\n",
    "        self.prompt_template_no_offset = \"\"\"{user_start} {instruction_on_response_format} <<{query}>>> {user_end}{model_start}\"\"\"\n",
    "\n",
    "    def _base_prompt_input(self, input: str, instruction_on_response_format:str) -> str:\n",
    "        \"\"\"\n",
    "        Format the input into a base prompt for the finetuning\n",
    "\n",
    "        Args:\n",
    "            input: the input text\n",
    "            instruction_on_response_format: the instruction on the response format. E.g. \"The response must be a list of dictionaries, where each dictionary contains the keys 'text' and 'offset'\"\n",
    "\n",
    "        Returns:\n",
    "            the formatted base prompt\n",
    "        \"\"\"\n",
    "        base_prompt = self.prompt_template_no_offset.format(\n",
    "            instruction_on_response_format=instruction_on_response_format, \n",
    "            query=input,\n",
    "            user_start=self.special_tokens_instruction['user_start'],\n",
    "            user_end=self.special_tokens_instruction['user_end'],\n",
    "            model_start=self.special_tokens_instruction['model_start'],\n",
    "            model_end=self.special_tokens_instruction['model_end'])\n",
    "            \n",
    "        return base_prompt\n",
    "\n",
    "    def _simplest_base_prompt_input(self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Format the input and output into a prompt for the finetuning, in the simplest way possible, containing only the sentence and the response\n",
    "\n",
    "        Args:\n",
    "            input: the input text\n",
    "            output: the output text\n",
    "\n",
    "        Returns:\n",
    "            the formatted prompt\n",
    "        \"\"\"\n",
    "        base_prompt = self.special_tokens_instruction['user_start'] + input + self.special_tokens_instruction['user_end'] + self.special_tokens_instruction['model_start']\n",
    "        return base_prompt\n",
    "\n",
    "    def _format_prompt(self, input: str, instruction_on_response_format:str, simplest_prompt: bool, output:str='') -> str:\n",
    "        \"\"\"\n",
    "        Format the input and output into a prompt for the finetuning\n",
    "\n",
    "        Args:\n",
    "            input: the input text\n",
    "            instruction_on_response_format: the instruction on the response format. E.g. \"The response must be a list of dictionaries, where each dictionary contains the keys 'text' and 'offset'\"\n",
    "            offset: whether to require the offset in the response\n",
    "            output: the output text\n",
    "\n",
    "        Returns:\n",
    "            the formatted prompt\n",
    "        \"\"\"\n",
    "        if output == '':\n",
    "            raise ValueError(\"The output must be provided when generating prompts for the finetuning\")\n",
    "        \n",
    "        if simplest_prompt:\n",
    "            prompt_input = self._simplest_base_prompt_input(input)\n",
    "        else:\n",
    "            prompt_input = self._base_prompt_input(input, instruction_on_response_format)\n",
    "        \n",
    "        bos_token = self.tokenizer.bos_token\n",
    "        eos_token = self.tokenizer.eos_token\n",
    "        if self.model_type == 'qwen':\n",
    "            bos_token = ''\n",
    "            eos_token = ''\n",
    "        # print(bos_token, prompt_input, output, self.special_tokens_instruction['model_end'], eos_token)\n",
    "        prompt = bos_token + prompt_input + output + self.special_tokens_instruction['model_end'] + eos_token\n",
    "                            \n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def _format_entities_in_response(self, entities_list: [dict], offset: bool) -> str:\n",
    "        \"\"\"\n",
    "        Format the response into a string\n",
    "\n",
    "        Args:\n",
    "            entities_list: the list of entities to format\n",
    "            offset: whether to require the offset in the response\n",
    "            \n",
    "        Returns:\n",
    "            the formatted response\n",
    "        \"\"\"\n",
    "        formatted_response = '['\n",
    "        if offset:\n",
    "            for entity in entities_list:\n",
    "                formatted_response = formatted_response + '{\"entity\": \"' + entity['text'] + f'\", \"offset\": {entity[\"offsets\"]}' + '}, '\n",
    "        else:\n",
    "            for entity in entities_list: \n",
    "                formatted_response = formatted_response + '{\"entity\": \"' + entity['text'] + '\"}, '\n",
    "        formatted_response = formatted_response[:-2]\n",
    "        formatted_response = formatted_response + '] '\n",
    "        return formatted_response\n",
    "    \n",
    "    def _apply_to_one_example(self, example, offset: bool, simplest_prompt: bool, instruction_on_response_format:str) -> dict:\n",
    "        \"\"\"\n",
    "        Apply the data preprocessing to one example\n",
    "\n",
    "        Args:\n",
    "            example: the example (data row) to preprocess\n",
    "            instruction_on_response_format: the instruction on the response format. E.g. \"The response must be a list of dictionaries, where each dictionary contains the keys 'text' and 'offset'\"\n",
    "            offset: whether to require the offset in the response\n",
    "            simplest_prompt: whether to generate the prompt or just concatenate the sentence and the response\n",
    "\n",
    "        Returns:\n",
    "            the preprocessed example\n",
    "        \"\"\"\n",
    "        output = self._format_entities_in_response(entities_list=example['entities'], offset=offset)\n",
    "        prompt = self._format_prompt(input=example['sentence'], \n",
    "                                     simplest_prompt=simplest_prompt,\n",
    "                                     instruction_on_response_format=instruction_on_response_format,\n",
    "                                     output=output)\n",
    "        example['prompt'] = prompt\n",
    "        return example\n",
    "    \n",
    "    def apply(self, data: Dataset, instruction_on_response_format:str, offset: bool,  simplest_prompt:bool, num_proc: int=1) -> Dataset:\n",
    "        \"\"\"\n",
    "        Apply the data preprocessing to one split/layer if the dataset. It formats the prompt in the right shape, processing the entities.\n",
    "\n",
    "        Args:\n",
    "            data: the dataset to preprocess\n",
    "            instruction_on_response_format: the instruction on the response format to be given to the model. E.g. \"The response must be a list of dictionaries, where each dictionary contains the keys 'text' and 'offset'\"\n",
    "            n_shots: the number of examples to provide as few shot prompting   \n",
    "            offset: whether to require the offset in the response  \n",
    "            num_proc: the number of processes to use for the parallel processing\n",
    "\n",
    "        Returns:\n",
    "            the preprocessed split/layer\n",
    "        \"\"\"\n",
    "        data = data.map(lambda example:  self._apply_to_one_example(example=example, \n",
    "                                                                    simplest_prompt=simplest_prompt,\n",
    "                                                                    instruction_on_response_format = instruction_on_response_format, \n",
    "                                                                    offset = offset), \n",
    "                        num_proc=num_proc) #batched=True)\n",
    "        self.offset = offset\n",
    "        self.instruction_on_response_format = instruction_on_response_format\n",
    "        self.simplest_prompt = simplest_prompt\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def preprocess_data_one_layer(self, hf_dataset: Dataset, instruction_on_response_format:str='', offset:bool=False, simplest_prompt:bool=False) -> Dataset:\n",
    "        \"\"\"\n",
    "        Preprocess one layer/split of the dataset the trasformations defined in self.apply()\n",
    "\n",
    "        Args:\n",
    "            hf_dataset: one layer/split of the dataset to preprocess\n",
    "\n",
    "        Returns:\n",
    "            the preprocessed dataset\n",
    "        \"\"\"\n",
    "        if not simplest_prompt and instruction_on_response_format == '':\n",
    "            raise ValueError(\"The instruction_on_response_format must be provided when not using the simplest_prompt\")\n",
    "            \n",
    "        hf_dataset = self.apply(data=hf_dataset, \n",
    "                                instruction_on_response_format=instruction_on_response_format, \n",
    "                                offset=offset,\n",
    "                                simplest_prompt=simplest_prompt)\n",
    "        return hf_dataset\n",
    "    \n",
    "    def split_layer_into_train_val_test_(self, dataset: Dataset, split_name: str, test_subset_of_validation: bool=False) -> (Dataset, Dataset):\n",
    "        \"\"\"\n",
    "        Split the layer into train, validation and test sets, according to the split defined at https://github.com/hltfbk/E3C-Corpus/tree/main/documentation\n",
    "\n",
    "        Args:\n",
    "            dataset: the dataset to split. Must be a split of the original Hugging Face dataset\n",
    "            split_name: the name of the layer\n",
    "            test_subset_of_validation: wether the test set is a subset of the validation set. Set this to True if you want to use the test set as a way of checking on the training throw wandb\n",
    "                                to mantain the diviosn it train-test of the original repository. Default is False.\n",
    "        \n",
    "        Returns:\n",
    "            the train and test sets\n",
    "        \"\"\"\n",
    "        mapping = {'en.layer1': 'train_labels_en.txt', \n",
    "                'es.layer1': 'train_labels_es.txt',\n",
    "                'eu.layer1': 'train_labels_eu.txt',\n",
    "                'it.layer1': 'train_labels_it.txt',\n",
    "                'fr.layer1': 'train_labels_fr.txt',}\n",
    "        labels_path = mapping[split_name]\n",
    "        with open(os.path.join('data', labels_path), 'r') as file:\n",
    "            file_content = file.read()\n",
    "        labels = file_content.split(\", \")\n",
    "        labels = [label[1:-1] for label in labels]\n",
    "        idxs_train = [idx for idx, x in enumerate(dataset['original_id']) if x in labels]\n",
    "        idxs_val = [idx for idx, x in enumerate(dataset['original_id']) if x not in labels]\n",
    "        random.seed(42)\n",
    "        idxs_test = random.sample(idxs_val, int(len(idxs_val) * 0.2))\n",
    "        train_data = dataset.select(idxs_train)\n",
    "        test_data = dataset.select(idxs_test)\n",
    "        if test_subset_of_validation:\n",
    "            val_data = dataset.select(idxs_val)\n",
    "        else:\n",
    "            idxs_val = [idx for idx in idxs_val if idx not in idxs_test]\n",
    "            val_data = dataset.select(idxs_val)\n",
    "\n",
    "        if self.offset:\n",
    "            prompt_template = self.prompt_template\n",
    "        else:\n",
    "            prompt_template = self.prompt_template_no_offset\n",
    "        \n",
    "        def remove_answer_from_prompt(example):\n",
    "            prompt_no_answ = prompt_template.format(instruction_on_response_format=self.instruction_on_response_format, query=example['sentence'],\n",
    "                                                    user_start=self.special_tokens_instruction['user_start'],\n",
    "                                                    user_end=self.special_tokens_instruction['user_end'],\n",
    "                                                    model_start=self.special_tokens_instruction['model_start'],\n",
    "                                                    model_end=self.special_tokens_instruction['model_end'])\n",
    "            example['prompt_with_answer'] = example['prompt']\n",
    "            example['prompt'] = prompt_no_answ\n",
    "            return example\n",
    "\n",
    "        test_data = test_data.map(remove_answer_from_prompt, batched=False)\n",
    "\n",
    "        return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from utils.slovenian_preprocessor import Slovenian_preprocessor\n",
    "import pandas as pd\n",
    "import string\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class Slovenian_preprocessor():\n",
    "\n",
    "\n",
    "    def __init__(self, data, model_checkpoint:str, tokenizer: AutoTokenizer, token_llama:str='') -> None:\n",
    "\n",
    "        self.data = data\n",
    "        self.offset = None\n",
    "        self.instruction_on_response_format = ''\n",
    "        self.n_shots = None\n",
    "        #self.model_type = model_checkpoint.split('/')[1].lower().split('-')[0]\n",
    "        self.model_type = 'qwen' if model_checkpoint.split('/')[0] == 'Qwen' else model_checkpoint.split('/')[1].lower().split('-')[0]\n",
    "        if self.model_type == 'meta': self.model_type = 'llama3'\n",
    "        # if self.model_type == 'zefiro':\n",
    "        #     self.model_type  = 'mistral'\n",
    "        if self.model_type not in ['mistral', 'llama', 'llama3', 'gemma', 'qwen', 'zefiro', 'phi', 'minerva']:\n",
    "            raise ValueError(\"The model type must be either 'mistral', 'llama', 'llama3', 'gemma', 'zefiro', 'qwen', 'minerva' or 'phi'\")\n",
    "\n",
    "        print('MODEL TYPE:', self.model_type)\n",
    "        if isinstance(tokenizer, str):\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, token = token_llama)\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "        \n",
    "        self.special_tokens_instruction_dict = {'mistral': {'user_start':'[INST]',\n",
    "                                                            'user_end':'[/INST]',\n",
    "                                                            'model_start':'',\n",
    "                                                            'model_end':''},\n",
    "                                                'llama': {'user_start':'[INST]',\n",
    "                                                          'user_end':'[/INST]',\n",
    "                                                          'model_start':'',\n",
    "                                                          'model_end':''},\n",
    "                                                'llama3': {'user_start':'<|start_header_id|>user<|end_header_id|>\\n\\n',\n",
    "                                                          'user_end':'<|eot_id|>',\n",
    "                                                          'model_start':'<|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
    "                                                          'model_end':''},\n",
    "                                                'gemma': {'user_start':'<start_of_turn>user',\n",
    "                                                          'user_end':'<end_of_turn>',\n",
    "                                                          'model_start':'<start_of_turn>model',\n",
    "                                                          'model_end':'<end_of_turn>'},\n",
    "                                                'qwen': {'user_start':'<|im_start|>user',\n",
    "                                                          'user_end':'<|im_end|>',\n",
    "                                                          'model_start':'<|im_start|>assistant',\n",
    "                                                          'model_end':'<|im_end|>'},\n",
    "                                                'zefiro': {'user_start':'<|user|>',\n",
    "                                                           'user_end':'',# 'user_end':'</s>',\n",
    "                                                           'model_start':'<|assistant|>',\n",
    "                                                           'model_end':''},# 'model_end':'</s>'},\n",
    "                                                'phi': {'user_start':'<|user|>',\n",
    "                                                           'user_end':'<|end|>\\n',\n",
    "                                                           'model_start':'<|assistant|>',\n",
    "                                                           'model_end':''},\n",
    "                                                'minerva': {'user_start':'',\n",
    "                                                            'user_end':'',\n",
    "                                                           'model_start':'',\n",
    "                                                           'model_end':''}}\n",
    "        self.special_tokens_instruction = self.special_tokens_instruction_dict[self.model_type]\n",
    "\n",
    "        self.one_shot_example = \"\"\"{user_start} {instruction_on_response_format} <<<{example_query}>>> {user_end}{model_start} {example_response} {model_end}\n",
    "\"\"\"\n",
    "        self.one_shot_example_no_offset = \"\"\"{user_start} {instruction_on_response_format} <<<{example_query}>>> {user_end}{model_start} {example_response} {model_end}\n",
    "\"\"\"\n",
    "\n",
    "        self.prompt_template = \"\"\"{user_start} {instruction_on_response_format} <<{query}>>> {user_end}{model_start}\"\"\"\n",
    "\n",
    "        self.prompt_template_no_offset = \"\"\"{user_start} {instruction_on_response_format} <<{query}>>> {user_end}{model_start}\"\"\"\n",
    "\n",
    "    def _base_prompt_input(self, input: str, instruction_on_response_format:str) -> str:\n",
    "        \"\"\"\n",
    "        Format the input into a base prompt for the finetuning\n",
    "\n",
    "        Args:\n",
    "            input: the input text\n",
    "            instruction_on_response_format: the instruction on the response format. E.g. \"The response must be a list of dictionaries, where each dictionary contains the keys 'text' and 'offset'\"\n",
    "\n",
    "        Returns:\n",
    "            the formatted base prompt\n",
    "        \"\"\"\n",
    "        base_prompt = self.prompt_template_no_offset.format(\n",
    "            instruction_on_response_format=instruction_on_response_format, \n",
    "            query=input,\n",
    "            user_start=self.special_tokens_instruction['user_start'],\n",
    "            user_end=self.special_tokens_instruction['user_end'],\n",
    "            model_start=self.special_tokens_instruction['model_start'],\n",
    "            model_end=self.special_tokens_instruction['model_end'])\n",
    "            \n",
    "        return base_prompt\n",
    "\n",
    "    def _simplest_base_prompt_input(self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Format the input and output into a prompt for the finetuning, in the simplest way possible, containing only the sentence and the response\n",
    "\n",
    "        Args:\n",
    "            input: the input text\n",
    "            output: the output text\n",
    "\n",
    "        Returns:\n",
    "            the formatted prompt\n",
    "        \"\"\"\n",
    "        base_prompt = self.special_tokens_instruction['user_start'] + input + self.special_tokens_instruction['user_end'] + self.special_tokens_instruction['model_start']\n",
    "        return base_prompt\n",
    "\n",
    "    def _format_prompt(self, input: str, instruction_on_response_format:str, simplest_prompt: bool, output:str='') -> str:\n",
    "        \"\"\"\n",
    "        Format the input and output into a prompt for the finetuning\n",
    "\n",
    "        Args:\n",
    "            input: the input text\n",
    "            instruction_on_response_format: the instruction on the response format. E.g. \"The response must be a list of dictionaries, where each dictionary contains the keys 'text' and 'offset'\"\n",
    "            offset: whether to require the offset in the response\n",
    "            output: the output text\n",
    "\n",
    "        Returns:\n",
    "            the formatted prompt\n",
    "        \"\"\"\n",
    "        if output == '':\n",
    "            raise ValueError(\"The output must be provided when generating prompts for the finetuning\")\n",
    "        \n",
    "        if simplest_prompt:\n",
    "            prompt_input = self._simplest_base_prompt_input(input)\n",
    "        else:\n",
    "            prompt_input = self._base_prompt_input(input, instruction_on_response_format)\n",
    "        \n",
    "        bos_token = self.tokenizer.bos_token\n",
    "        eos_token = self.tokenizer.eos_token\n",
    "        if self.model_type == 'qwen':\n",
    "            bos_token = ''\n",
    "            eos_token = ''\n",
    "        # print(bos_token, prompt_input, output, self.special_tokens_instruction['model_end'], eos_token)\n",
    "        prompt = bos_token + prompt_input + output + self.special_tokens_instruction['model_end'] + eos_token\n",
    "                            \n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def _format_entities_in_response(self, entities_list: [dict], offset: bool) -> str:\n",
    "        \"\"\"\n",
    "        Format the response into a string\n",
    "\n",
    "        Args:\n",
    "            entities_list: the list of entities to format\n",
    "            offset: whether to require the offset in the response\n",
    "            \n",
    "        Returns:\n",
    "            the formatted response\n",
    "        \"\"\"\n",
    "        formatted_response = '['\n",
    "        if offset:\n",
    "            for entity in entities_list:\n",
    "                formatted_response = formatted_response + '{\"entity\": \"' + entity['entity'] + f'\", \"offset\": {entity[\"offsets\"]}' + '}, '\n",
    "        else:\n",
    "            for entity in entities_list: \n",
    "                formatted_response = formatted_response + '{\"entity\": \"' + entity['entity'] + '\"}, '\n",
    "        formatted_response = formatted_response[:-2]\n",
    "        formatted_response = formatted_response + '] '\n",
    "        return formatted_response\n",
    "    \n",
    "    def _apply_to_one_example(self, example, offset: bool, simplest_prompt: bool, instruction_on_response_format:str) -> dict:\n",
    "        \"\"\"\n",
    "        Apply the data preprocessing to one example\n",
    "\n",
    "        Args:\n",
    "            example: the example (data row) to preprocess\n",
    "            instruction_on_response_format: the instruction on the response format. E.g. \"The response must be a list of dictionaries, where each dictionary contains the keys 'text' and 'offset'\"\n",
    "            offset: whether to require the offset in the response\n",
    "            simplest_prompt: whether to generate the prompt or just concatenate the sentence and the response\n",
    "\n",
    "        Returns:\n",
    "            the preprocessed example\n",
    "        \"\"\"\n",
    "        output = self._format_entities_in_response(entities_list=example['entities'], offset=offset)\n",
    "        prompt = self._format_prompt(input=example['text'], \n",
    "                                     simplest_prompt=simplest_prompt,\n",
    "                                     instruction_on_response_format=instruction_on_response_format,\n",
    "                                     output=output)\n",
    "        example['prompt'] = prompt\n",
    "        return example\n",
    "    \n",
    "    def apply(self, instruction_on_response_format:str, offset: bool,  simplest_prompt:bool, num_proc: int=1): # -> Dataset:\n",
    "        \"\"\"\n",
    "        Apply the data preprocessing to one split/layer if the dataset. It formats the prompt in the right shape, processing the entities.\n",
    "\n",
    "        Args:\n",
    "            data: the dataset to preprocess\n",
    "            instruction_on_response_format: the instruction on the response format to be given to the model. E.g. \"The response must be a list of dictionaries, where each dictionary contains the keys 'text' and 'offset'\"\n",
    "            n_shots: the number of examples to provide as few shot prompting   \n",
    "            offset: whether to require the offset in the response  \n",
    "            num_proc: the number of processes to use for the parallel processing\n",
    "\n",
    "        Returns:\n",
    "            the preprocessed split/layer\n",
    "        \"\"\"\n",
    "        self.data = self.data.map(lambda example:  self._apply_to_one_example(example=example, \n",
    "                                                                    simplest_prompt=simplest_prompt,\n",
    "                                                                    instruction_on_response_format = instruction_on_response_format, \n",
    "                                                                    offset = offset), \n",
    "                        num_proc=num_proc) #batched=True)\n",
    "        self.offset = offset\n",
    "        self.instruction_on_response_format = instruction_on_response_format\n",
    "        self.simplest_prompt = simplest_prompt\n",
    "        # return data\n",
    "\n",
    "    \n",
    "    def preprocess(self):\n",
    "        self.data['label'] = self.data['label'].astype(str)\n",
    "        text = ''\n",
    "        overall_entities = []\n",
    "        entities = []\n",
    "        current_entity = ''\n",
    "        prev_word=''\n",
    "        for _, row in self.data.iterrows():\n",
    "            # print(f\"word: {row['word']}, label: {row['label']}, prev_word: {prev_word}\")\n",
    "            if prev_word == '.' and len(text.split())>5:\n",
    "                if current_entity:\n",
    "                    entities.append({'entity': current_entity})\n",
    "                    current_entity = ''\n",
    "                # print(f\"ENTRO NELLA COND\")\n",
    "                overall_entities.append({'text': text, 'entities': entities})\n",
    "                text = ''\n",
    "                entities = []\n",
    "            word, label = row['word'], row['label']\n",
    "            if text!='' or (word.strip() in string.punctuation) or (prev_word.strip() in string.punctuation):\n",
    "                space = ' '\n",
    "            else:\n",
    "                space = ''\n",
    "            text = text + space + word\n",
    "            if label != 'O':\n",
    "                if label.startswith('B-'):\n",
    "                    if current_entity:\n",
    "                        entities.append({'entity': current_entity})\n",
    "                    current_entity = word\n",
    "                elif label.startswith('I-') and current_entity:\n",
    "                    current_entity += ' ' + word\n",
    "            prev_word = word\n",
    "        data_df = pd.DataFrame(columns=['text', 'entities'])\n",
    "        for el in overall_entities:\n",
    "            el['text'] = el['text'].strip() \n",
    "            data_df = pd.concat([data_df, pd.DataFrame([el])], ignore_index=True)\n",
    "        self.data = Dataset.from_pandas(data_df, split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TYPE: llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 668/668 [00:00<00:00, 12207.35 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'entities', 'prompt'],\n",
       "    num_rows: 668\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "data_train = pd.read_csv('/Users/pietroferrazzi/Desktop/dottorato/mistral_finetuning/data/slovenian/E3C_Slovenian_Train_SL_L1.csv', header=None, names=['word', 'label'])\n",
    "preprocessor = Slovenian_preprocessor(data_train, \"meta-llama/Llama-2-7b-chat-hf\", \"meta-llama/Llama-2-7b-chat-hf\", token_llama=HF_TOKEN)\n",
    "preprocessor.preprocess()\n",
    "preprocessor.data = preprocessor.apply(preprocessor.data, 'cds', offset=False, simplest_prompt=False)\n",
    "preprocessor.data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '61 - letni moški se je pojavil s kroničnim levim ušesom izcedekom , glavobolom na levi strani , diplopijo povezano z vrtoglavico , tinitusom in slušno okvaro .',\n",
       " 'entities': [{'entity': 'levim ušesom izcedekom'},\n",
       "  {'entity': 'glavobolom na levi strani'},\n",
       "  {'entity': 'diplopijo'},\n",
       "  {'entity': 'vrtoglavico'},\n",
       "  {'entity': 'tinitusom'},\n",
       "  {'entity': 'slušno okvaro'}],\n",
       " 'prompt': '<s>[INST] cds <<61 - letni moški se je pojavil s kroničnim levim ušesom izcedekom , glavobolom na levi strani , diplopijo povezano z vrtoglavico , tinitusom in slušno okvaro .>>> [/INST][{\"entity\": \"levim ušesom izcedekom\"}, {\"entity\": \"glavobolom na levi strani\"}, {\"entity\": \"diplopijo\"}, {\"entity\": \"vrtoglavico\"}, {\"entity\": \"tinitusom\"}, {\"entity\": \"slušno okvaro\"}] </s>'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/668 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m dssa\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplest_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 196\u001b[0m, in \u001b[0;36mDataPreprocessor.apply\u001b[0;34m(self, data, instruction_on_response_format, offset, simplest_prompt, num_proc)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Dataset, instruction_on_response_format:\u001b[38;5;28mstr\u001b[39m, offset: \u001b[38;5;28mbool\u001b[39m,  simplest_prompt:\u001b[38;5;28mbool\u001b[39m, num_proc: \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Apply the data preprocessing to one split/layer if the dataset. It formats the prompt in the right shape, processing the entities.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        the preprocessed split/layer\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_to_one_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43msimplest_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimplest_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43minstruction_on_response_format\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstruction_on_response_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#batched=True)\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m offset\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_on_response_format \u001b[38;5;241m=\u001b[39m instruction_on_response_format\n",
      "File \u001b[0;32m~/Desktop/dottorato/mistral_finetuning/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/dottorato/mistral_finetuning/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/dottorato/mistral_finetuning/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3158\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/dottorato/mistral_finetuning/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:3517\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3515\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3516\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3517\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3519\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/dottorato/mistral_finetuning/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:3416\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3415\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3416\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3418\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3419\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3420\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[3], line 196\u001b[0m, in \u001b[0;36mDataPreprocessor.apply.<locals>.<lambda>\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Dataset, instruction_on_response_format:\u001b[38;5;28mstr\u001b[39m, offset: \u001b[38;5;28mbool\u001b[39m,  simplest_prompt:\u001b[38;5;28mbool\u001b[39m, num_proc: \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Apply the data preprocessing to one split/layer if the dataset. It formats the prompt in the right shape, processing the entities.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        the preprocessed split/layer\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m example:  \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_to_one_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43msimplest_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimplest_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43minstruction_on_response_format\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minstruction_on_response_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m    200\u001b[0m                     num_proc\u001b[38;5;241m=\u001b[39mnum_proc) \u001b[38;5;66;03m#batched=True)\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m offset\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_on_response_format \u001b[38;5;241m=\u001b[39m instruction_on_response_format\n",
      "Cell \u001b[0;32mIn[3], line 174\u001b[0m, in \u001b[0;36mDataPreprocessor._apply_to_one_example\u001b[0;34m(self, example, offset, simplest_prompt, instruction_on_response_format)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_to_one_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example, offset: \u001b[38;5;28mbool\u001b[39m, simplest_prompt: \u001b[38;5;28mbool\u001b[39m, instruction_on_response_format:\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Apply the data preprocessing to one example\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m        the preprocessed example\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_entities_in_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentities_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentities\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_prompt(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mexample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    176\u001b[0m                                  simplest_prompt\u001b[38;5;241m=\u001b[39msimplest_prompt,\n\u001b[1;32m    177\u001b[0m                                  instruction_on_response_format\u001b[38;5;241m=\u001b[39minstruction_on_response_format,\n\u001b[1;32m    178\u001b[0m                                  output\u001b[38;5;241m=\u001b[39moutput)\n\u001b[1;32m    179\u001b[0m     example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt\n",
      "Cell \u001b[0;32mIn[3], line 156\u001b[0m, in \u001b[0;36mDataPreprocessor._format_entities_in_response\u001b[0;34m(self, entities_list, offset)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities_list: \n\u001b[0;32m--> 156\u001b[0m         formatted_response \u001b[38;5;241m=\u001b[39m formatted_response \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mentity\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    157\u001b[0m formatted_response \u001b[38;5;241m=\u001b[39m formatted_response[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    158\u001b[0m formatted_response \u001b[38;5;241m=\u001b[39m formatted_response \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "processor.apply(preprocessor.data, \" dssa\", offset=False, simplest_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for i, row in preprocessor.data.iterrows():\n",
    "    if len(row['text'] + ' ' + str(row['entities'])) > max_length:\n",
    "        max_length = len(row['text'] + ' ' + str(row['entities']))\n",
    "        max_row = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
