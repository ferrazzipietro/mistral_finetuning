{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'quant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-7b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m, token\u001b[38;5;241m=\u001b[39mHF_TOKEN)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", quantization_config=quantization_config)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/gemma-7b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# return_dict=True, \u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#torch_dtype=torch.float16,\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquant\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mferrazzipietro/e3c-sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m, token\u001b[38;5;241m=\u001b[39mHF_TOKEN)\n\u001b[1;32m     55\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset[layer]\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/transformers/modeling_utils.py:3375\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3370\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3371\u001b[0m )\n\u001b[1;32m   3373\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3374\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3375\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3378\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'quant'"
     ]
    }
   ],
   "source": [
    "# pip install bitsandbytes accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from dotenv import dotenv_values\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "from config import base_model\n",
    "from config.finetuning import preprocessing_params\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "max_new_tokens_factor_list = base_model.max_new_tokens_factor_list\n",
    "n_shots_inference_list = base_model.n_shots_inference_list\n",
    "layer = base_model.TRAIN_LAYER\n",
    "language = layer.split('.')[0]\n",
    "save_directory = base_model.save_directory \n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_type=torch.bfloat16,)\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             # load_in_8bit=True,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#             # llm_int8_threshold= 6.0,\n",
    "#             # llm_int8_skip_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"],\n",
    "#             )\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=False,\n",
    "            load_in_8bit=True,\n",
    "            #bnb_4bit_use_double_quant=True,\n",
    "            #bnb_4bit_quant_type=\"nf4\",\n",
    "            #bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # llm_int8_threshold= 6.0,\n",
    "            # llm_int8_has_fp16_weight = False,\n",
    "            # llm_int8_skip_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"],\n",
    "            )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\", token=HF_TOKEN)\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", quantization_config=quantization_config)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"google/gemma-7b-it\", low_cpu_mem_usage=True,\n",
    "            quantization_config = quantization_config,\n",
    "            # return_dict=True, \n",
    "            #torch_dtype=torch.float16,\n",
    "            device_map= \"auto\",\n",
    "            token=HF_TOKEN)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos><start_of_turn>Extract the entities contained in this text: There was no evidence of lung lesions  <end_of_turn> <start_of_turn>model response:**\\n\\nThe text does not mention any other entities than \"lung lesions,\" therefore I cannot extract']\n"
     ]
    }
   ],
   "source": [
    "input_text = [\"<bos><start_of_turn>Extract the entities contained in this text: There was no evidence of lung lesions  <end_of_turn> <start_of_turn>model\",\n",
    "              \"<bos><start_of_turn>Extract the entities contained in this text: We present a case of a 32-year-old woman with a history of gradual enlargement of the anterior neck.  <end_of_turn> <start_of_turn>model\"]\n",
    "\n",
    "# input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(input_ids, max_new_tokens=10)\n",
    "# print(tokenizer.batch_decode(outputs))\n",
    "\n",
    "encodeds = tokenizer.encode(input_text[0], return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "model_inputs = encodeds.to('cuda')\n",
    "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20,  pad_token_id=tokenizer.eos_token_id) # max_new_tokens=max_new_tokens,\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Hormonal study and dynamic biochemical tests performed indicated ECS.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<The patient underwent right parotidectomy, and histopathologic examination confirmed ACC.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide.>>> <end_of_turn><start_of_turn>model']\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "input_sentences = postprocessor.test_data.select(range(4))['sentence']\n",
    "prompts = postprocessor.test_data.select(range(4))['inference_prompt']\n",
    "input_sentences_tokenized = tokenizer(input_sentences, return_tensors=\"pt\", padding=True)\n",
    "print(prompts)\n",
    "max_new_tokens = int(len(max(input_sentences_tokenized, key=len)) * 4)\n",
    "# if self.preprocessor.model_type == 'gemma':\n",
    "#     add_special_tokens = True\n",
    "encodeds = tokenizer(prompts, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "model_inputs = encodeds.to(device)\n",
    "generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=max_new_tokens,  pad_token_id=tokenizer.eos_token_id) # max_new_tokens=max_new_tokens,\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "#decoded = [self._postprocess_model_output(i) for i in decoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Hormonal study and dynamic biochemical tests performed indicated ECS.>>> <end_of_turn><start_of_turn>model**Result:**\\n\\n```json\\n[{\"entity\": \"ECS\"}]\\n```\\n\\n**Explanation:**\\n\\nThe text \"Hormonal study and dynamic biochemical tests performed indicated ECS.\" contains only one entity: ECS. Therefore, the extracted entities are:\\n\\n- ECS<eos><eos><eos>',\n",
       " '<pad><pad><pad><pad><bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.>>> <end_of_turn><start_of_turn>model**Result:**\\n\\n```json\\n[{\"entity\": \"right parotid malignancy\"}, {\"entity\": \"liver metastases\"}]\\n```\\n\\n**Explanation:**\\n\\n* The text contains two entities: \"right parotid malignancy\" and \"liver metastases.\"\\n* The extracted entities are',\n",
       " '<pad><pad><pad><pad><pad><pad><bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<The patient underwent right parotidectomy, and histopathologic examination confirmed ACC.>>> <end_of_turn><start_of_turn>model```python\\nimport nltk\\n\\n# Text\\ntext = \"The patient underwent right parotidectomy, and histopathologic examination confirmed ACC.\"\\n\\n# Tokenize the text\\ntokens = nltk.word_tokenize(text)\\n\\n# Extract entities\\nentities = nltk.chunk.',\n",
       " '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide.>>> <end_of_turn><start_of_turn>model**Result:**\\n\\n```json\\n[{\"entity\": \"hypercortisolism\"}, {\"entity\": \"metyrapone\"}, {\"entity\": \"ketoconazole\"}, {\"entity\": \"lanreotide\"}]\\n```\\n\\n**Explanation:**\\n\\nThe text mentions the following entities:\\n\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad><pad><pad><pad><pad><pad><pad><bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Hormonal study and dynamic biochemical tests performed indicated ECS.>>> <end_of_turn><start_of_turn>model**Output:**\\n\\n```json\\n[{\"entity\": \"ECS\"}]\\n```\\n\\n**Explanation',\n",
       " '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.>>> <end_of_turn><start_of_turn>model**Result:**\\n\\n```json\\n[{\"entity\":\"Primary right parotid malignancy\"}, {\"entity\":\"']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "prompts = \"<start_of_turn>Extract the entities contained in this text: There was no evidence of lung lesions  <end_of_turn> <start_of_turn>model\"\n",
    "prompts =['<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Hormonal study and dynamic biochemical tests performed indicated ECS.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<The patient underwent right parotidectomy, and histopathologic examination confirmed ACC.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide.>>> <end_of_turn><start_of_turn>model']\n",
    "prompts =['<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide.>>> <end_of_turn><start_of_turn>model', \n",
    "          '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Hormonal study and dynamic biochemical tests performed indicated ECS.>>> <end_of_turn><start_of_turn>model']\n",
    "prompts = ['<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Hormonal study and dynamic biochemical tests performed indicated ECS.>>> <end_of_turn><start_of_turn>model', \n",
    "           '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.>>> <end_of_turn><start_of_turn>model']\n",
    "# input_sentences = examples['sentence']\n",
    "# prompts = examples['inference_prompt']\n",
    "# input_sentences_tokenized = tokenizer(input_sentences, return_tensors=\"pt\", padding=True)\n",
    "# max_new_tokens = int(len(max(input_sentences_tokenized, key=len)) * max_new_tokens_factor)\n",
    "encodeds = tokenizer(prompts, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "model_inputs = encodeds.to(device)\n",
    "generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=20,  pad_token_id=tokenizer.eos_token_id) # max_new_tokens=max_new_tokens,\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "decoded\n",
    "# decoded = [self._postprocess_model_output(i) for i in decoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = examples['sentence']\n",
    "prompts = examples['inference_prompt']\n",
    "input_sentences_tokenized = tokenizer(input_sentences, return_tensors=\"pt\", padding=True)\n",
    "print(prompts)\n",
    "max_new_tokens = int(len(max(input_sentences_tokenized, key=len)) * max_new_tokens_factor)\n",
    "# if self.preprocessor.model_type == 'gemma':\n",
    "#     add_special_tokens = True\n",
    "encodeds = tokenizer(prompts, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "model_inputs = encodeds.to(device)\n",
    "generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=max_new_tokens,  pad_token_id=tokenizer.eos_token_id) # max_new_tokens=max_new_tokens,\n",
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<The goiter measured 18 x 11 cm.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<She never suffered from thyroid dysfunction.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<The incision performed was a Kocher cervicotomy.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Its lower end plunges behind the sternal manubrium.>>> <end_of_turn><start_of_turn>model']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#decoded = [self._postprocess_model_output(i) for i in decoded]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded)\n\u001b[0;32m---> 18\u001b[0m \u001b[43m_generate_model_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m, in \u001b[0;36m_generate_model_response\u001b[0;34m(examples, model, tokenizer, max_new_tokens_factor)\u001b[0m\n\u001b[1;32m     11\u001b[0m encodeds \u001b[38;5;241m=\u001b[39m tokenizer(prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m encodeds\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# max_new_tokens=max_new_tokens,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#decoded = [self._postprocess_model_output(i) for i in decoded]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2734\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "def _generate_model_response(examples, model, tokenizer, max_new_tokens_factor:float) -> str:\n",
    "    device = \"cuda\"\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    input_sentences = examples['sentence']\n",
    "    prompts = examples['inference_prompt']\n",
    "    input_sentences_tokenized = tokenizer(input_sentences, return_tensors=\"pt\", padding=True)\n",
    "    print(prompts)\n",
    "    max_new_tokens = int(len(max(input_sentences_tokenized, key=len)) * max_new_tokens_factor)\n",
    "    # if self.preprocessor.model_type == 'gemma':\n",
    "    #     add_special_tokens = True\n",
    "    encodeds = tokenizer(prompts, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "    model_inputs = encodeds.to(device)\n",
    "    generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=max_new_tokens,  pad_token_id=tokenizer.eos_token_id) # max_new_tokens=max_new_tokens,\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    #decoded = [self._postprocess_model_output(i) for i in decoded]\n",
    "    return (decoded)\n",
    "\n",
    "_generate_model_response(postprocessor.test_data.select(range(4)), model, tokenizer, 4.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Hormonal study and dynamic biochemical tests performed indicated ECS.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<The patient underwent right parotidectomy, and histopathologic examination confirmed ACC.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<A 50-years-old woman, hypertensive, hospitalized for a large cervical mass appeared 30 years ago.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<In the family history, her mother, sisters and cousins underwent a surgery for MNG.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Despite of the large volume of the mass, the patient never described signs of cervical compression whatsoever respiratory, digestive, laryngeal, vascular or neurologic signs.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<She never suffered from thyroid dysfunction.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<The mass took the front and the two sides of the neck.>>> <end_of_turn><start_of_turn>model', '<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<Its surface was embossed and covered by a thin normal skin.>>> <end_of_turn><start_of_turn>model']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#decoded = [self._postprocess_model_output(i) for i in decoded]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded)\n\u001b[0;32m---> 18\u001b[0m \u001b[43m_generate_model_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36m_generate_model_response\u001b[0;34m(examples, model, tokenizer, max_new_tokens_factor)\u001b[0m\n\u001b[1;32m     11\u001b[0m encodeds \u001b[38;5;241m=\u001b[39m tokenizer(prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m encodeds\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# max_new_tokens=max_new_tokens,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#decoded = [self._postprocess_model_output(i) for i in decoded]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2734\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "def _generate_model_response(examples, model, tokenizer, max_new_tokens_factor:float) -> str:\n",
    "    device = \"cuda\"\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    input_sentences = examples['sentence']\n",
    "    prompts = examples['inference_prompt']\n",
    "    input_sentences_tokenized = tokenizer(input_sentences, return_tensors=\"pt\", padding=True)\n",
    "    print(prompts)\n",
    "    max_new_tokens = int(len(max(input_sentences_tokenized, key=len)) * max_new_tokens_factor)\n",
    "    # if self.preprocessor.model_type == 'gemma':\n",
    "    #     add_special_tokens = True\n",
    "    encodeds = tokenizer(prompts, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "    model_inputs = encodeds.to(device)\n",
    "    generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=max_new_tokens,  pad_token_id=tokenizer.eos_token_id) # max_new_tokens=max_new_tokens,\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    #decoded = [self._postprocess_model_output(i) for i in decoded]\n",
    "    return (decoded)\n",
    "\n",
    "_generate_model_response(postprocessor.test_data.select(range(11)), model, tokenizer, 4.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 1520/1520 [00:00<00:00, 4663.03 examples/s]\n",
      "Map: 100%|| 170/170 [00:00<00:00, 6629.03 examples/s]\n",
      "Map: 100%|| 24/24 [00:00<00:00, 2750.89 examples/s]\n",
      "Map: 100%|| 24/24 [00:00<00:00, 2978.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRY:  data/llama/maxNewTokensFactor8_nShotsInference0_BaseModel.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preprocessor = DataPreprocessor(model_checkpoint=\"google/gemma-7b-it\", tokenizer=tokenizer)\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset, instruction_on_response_format=preprocessing_params.instruction_on_response_format)\n",
    "_, val_data, _ = preprocessor.split_layer_into_train_val_test_(dataset, layer)\n",
    "\n",
    "postprocessor = TestDataProcessor(test_data=val_data.select(range(24)), \n",
    "                                          preprocessor=preprocessor, \n",
    "                                          n_shots_inference=0, \n",
    "                                          language=language, \n",
    "                                          tokenizer=tokenizer)\n",
    "postprocessor.add_inference_prompt_column()\n",
    "postprocessor.add_ground_truth_column()\n",
    "print('TRY: ', f\"{save_directory}/maxNewTokensFactor{8}_nShotsInference{0}_BaseModel.csv\")\n",
    "sorted_data = postprocessor.test_data.to_pandas().sort_values(by='inference_prompt', key=lambda x: x.str.len())\n",
    "postprocessor.test_data = dataset.from_pandas(sorted_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating responses:   0%|          | 0/24 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpostprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_responses_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmax_new_tokens_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mistral_finetuning/utils/test_data_processor.py:176\u001b[0m, in \u001b[0;36mTestDataProcessor.add_responses_column\u001b[0;34m(self, model, tokenizer, batch_size, max_new_tokens_factor)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indexes[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    175\u001b[0m     indici \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(idx, indexes[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m--> 176\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_model_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindici\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     responses_col\u001b[38;5;241m.\u001b[39mextend(tmp)\n\u001b[1;32m    178\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(batch_size)\n",
      "File \u001b[0;32m~/mistral_finetuning/utils/test_data_processor.py:152\u001b[0m, in \u001b[0;36mTestDataProcessor._generate_model_response\u001b[0;34m(self, examples, model, tokenizer, max_new_tokens_factor)\u001b[0m\n\u001b[1;32m    150\u001b[0m encodeds \u001b[38;5;241m=\u001b[39m tokenizer(prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    151\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m encodeds\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 152\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# max_new_tokens=max_new_tokens,\u001b[39;00m\n\u001b[1;32m    153\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids)\n\u001b[1;32m    154\u001b[0m decoded \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_model_output(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m decoded]\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/transformers/generation/utils.py:1592\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1585\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1586\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1587\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1589\u001b[0m     )\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1610\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1611\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1617\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2734\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "postprocessor.add_responses_column(model=model, \n",
    "                                tokenizer=tokenizer, \n",
    "                                batch_size=5, \n",
    "                                max_new_tokens_factor=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. Text: <<A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia.>>> <end_of_turn><start_of_turn>model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data['inference_prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TestDataProcessor():\n",
    "    def __init__(self, test_data: Dataset, preprocessor:DataPreprocessor, n_shots_inference:int, language:str, tokenizer) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the TestDataProcessor class.\n",
    "        pass to this the same DataPreprocessor used for the training data. This will ensure that the inference prompt is formatted in the same way as the training prompt.\n",
    "        \"\"\"\n",
    "        self.test_data = test_data\n",
    "        self.preprocessor = preprocessor\n",
    "        self.language = language\n",
    "        self.tokenizer = tokenizer\n",
    "        self.few_shots_dict = {'en':{'questions':['We present a case of a 32-year-old woman with a history of gradual enlargement of the anterior neck.',\n",
    "                                                   'Patient information: a 9-month-old boy presented to the emergency room with a 3-day history of refusal to bear weight on the right lower extremity and febrile peaks of up to 38.5C for 24 hours.',\n",
    "                                                   'There was no evidence of lung lesions.',\n",
    "                                                   'Locally diminished actin coloration indicated atrophy of smooth muscle fibers.'],\n",
    "                                        'responses':['[{\"entity\": \"present\"}, {\"entity\": \"history\"}, {\"entity\": \"enlargement\"}]',\n",
    "                                                     '[{\"entity\": \"presented\"}, {\"entity\": \"refusal\"}, {\"entity\": \"bear\"}, {\"entity\": \"peaks\"}]',\n",
    "                                                      '[{\"entity\": \"evidence\"}, {\"entity\": \"lung lesions\"]',\n",
    "                                                      '[{\"entity\": \"coloration\"}, {\"entity\": \"indicated\"}, {\"entity\": \"atrophy\"}, {\"entity\": \"atrophy of smooth muscle fibers\"}, {\"entity\": \"smooth muscle fibers\"'],\n",
    "                                        'responses_offset': ['[{\"entity\": \"present\", \"offset\": [3, 10]}, {\"entity\": \"history\", \"offset\": [48, 55]}, {\"entity\": \"enlargement\", \"offset\": [67, 78]}]',\n",
    "                                                             '[{\"entity\": \"presented\", \"offset\": [39, 48]}, {\"entity\": \"refusal\", \"offset\": [95, 102]}, {\"entity\": \"bear\", \"offset\": [106, 110]}, {\"entity\": \"peaks\", \"offset\": [159, 164]}]',\n",
    "                                                             '[{ \"entity\": \"evidence\", \"offsets\": [13, 21]}, {\"entity\": \"lung lesions\", \"offsets\": [25, 37]} ]',\n",
    "                                                             '[{\"entity\": \"coloration\", \"offsets\": [25, 35]}, {\"entity\": \"indicated\", \"offsets\": [36, 45]}, {\"entity\": \"atrophy\",\"offsets\": [46, 53]}, {\"entity\": \"atrophy of smooth muscle fibers\", \"offsets\": [46, 77]}, {\"entity\": \"smooth muscle fibers\", \"offsets\": [57, 77]} ]'],\n",
    "                                    },\n",
    "                                'it':{'questions':['In considerazione dellinefficacia della terapia somministrata, in assenza di ulteriori opzioni terapeutiche standard potenzialmente efficaci e dopo colloquio con i genitori si decide di avviare la paziente a trapianto aploidentico, possibilmente NK allo reattivo, da genitore.',\n",
    "                                                    'Lesame istologico dimostrava mucosa gastrica atrofica con flogosi cronica, marcato edema ed incremento del connettivo del corion, focale metaplasia intestinale, il tutto sovrastante un tessuto fibromuscolare.'],\n",
    "                                       'responses':['[{\"entity\": \"inefficacia\"}, {\"entity\": \"opzioni\"}, {\"entity\": \"colloquio\"}, {\"entity\": \"avviare\"}, {\"entity\": \"trapianto\"}, {\"entity\": \"genitori\"}, {\"entity\": \"paziente\"}, {\"entity\": \"genitore\"}]',\n",
    "                                                    '[{\"entity\": \"mucosa gastrica atrofica\"}, {\"entity\": \"flogosi\\r\\cronica\"}]'],\n",
    "                                       'responses_offset':['[{\"entity\": \"inefficacia\", \"offset\": [23, 34]}, {\"entity\": \"opzioni\", \"offset\": [88,95]}, {\"entity\": \"colloquio\", \"offset\": [149,158]}, {\"entity\": \"avviare\", \"offset\": [187,194]}, {\"entity\": \"trapianto\", \"offset\": [209,218]}, {\"entity\": \"genitori\", \"offset\": [163,173]}, {\"entity\": \"paziente\", \"offset\": [195,106]}, {\"entity\": \"genitore\", \"offset\": [268,276]}]',\n",
    "                                                           '[{\"entity\": \"mucosa gastrica atrofica\", \"offset\": [30,54]}, {\"entity\": \"flogosi\\r\\cronica\", \"offset\": [59,75]}]']}\n",
    "                                }\n",
    "        if len(self.few_shots_dict[self.language]['questions']) < n_shots_inference:\n",
    "            raise ValueError(f'The number of shots for the inference prompt is greater than the number of examples available.')\n",
    "        if len(self.few_shots_dict[self.language]['responses']) < n_shots_inference:\n",
    "            raise ValueError(f'The number of shots for the inference prompt is greater than the number of responses available.')\n",
    "        self.n_shots_inference = n_shots_inference\n",
    "    \n",
    "    def _extract_ground_truth(self, prompt:str) -> str:\n",
    "        end_of_prompt_string = self.preprocessor.special_tokens_instruction['user_end'] + self.preprocessor.special_tokens_instruction['model_start']\n",
    "        out = prompt.split(end_of_prompt_string, 1)\n",
    "        return {'ground_truth': out[1][0:-4].strip()}\n",
    "    \n",
    "    def _format_prompt_inference(self, input: str, instruction_on_response_format:str, n_shots:int, offset: bool, output:str='', list_of_examples: [str]=[], list_of_responses:[str]=[]) -> str:\n",
    "        \"\"\"\n",
    "        Format the input and output into a prompt for the finetuning\n",
    "\n",
    "        Args:\n",
    "            task: the task for which the prompt is generated, either 'finetuning' or 'inference'\n",
    "            input: the input text\n",
    "            instruction_on_response_format: the instruction on the response format. E.g. \"The response must be a list of dictionaries, where each dictionary contains the keys 'text' and 'offset'\"\n",
    "            n_shots: the number of examples to provide as few shot prompting\n",
    "            offset: whether to require the offset in the response\n",
    "            tokenizer: the tokenizer to use\n",
    "            output: the output text\n",
    "            list_of_examples: the list of examples to provide as few shot prompting\n",
    "            list_of_responses: the list of responses to provide as few shot prompting\n",
    "\n",
    "        Returns:\n",
    "            the formatted prompt\n",
    "        \"\"\"\n",
    "        if output != '':\n",
    "            raise ValueError(\"The output must be an empty string when generating prompts for the inference\")\n",
    "\n",
    "        if len(list_of_examples) != len(list_of_responses):\n",
    "            raise ValueError(\"The number of examples and responses must be the same\")\n",
    "        if n_shots != len(list_of_examples):\n",
    "            raise ValueError(\"The number of examples and shots must be the same\")\n",
    "        if n_shots != len(list_of_responses):\n",
    "            raise ValueError(\"The number of responses and shots must be the same\")\n",
    "        \n",
    "        if offset:\n",
    "            base_prompt = self.preprocessor.prompt_template.format(\n",
    "                instruction_on_response_format=instruction_on_response_format, \n",
    "                query=input,\n",
    "                user_start=self.preprocessor.special_tokens_instruction['user_start'],\n",
    "                user_end=self.preprocessor.special_tokens_instruction['user_end'],\n",
    "                model_start=self.preprocessor.special_tokens_instruction['model_start'],\n",
    "                model_end=self.preprocessor.special_tokens_instruction['model_end']) \n",
    "            one_shot_example = self.preprocessor.one_shot_example\n",
    "        else:\n",
    "            base_prompt = self.preprocessor.prompt_template_no_offset.format(\n",
    "                instruction_on_response_format=instruction_on_response_format, \n",
    "                query=input,\n",
    "                user_start=self.preprocessor.special_tokens_instruction['user_start'],\n",
    "                user_end=self.preprocessor.special_tokens_instruction['user_end'],\n",
    "                model_start=self.preprocessor.special_tokens_instruction['model_start'],\n",
    "                model_end=self.preprocessor.special_tokens_instruction['model_end'])\n",
    "            one_shot_example = self.preprocessor.one_shot_example_no_offset\n",
    "            \n",
    "        prompt = ''\n",
    "        for shot_example in range(n_shots):\n",
    "            prompt += one_shot_example.format(\n",
    "                instruction_on_response_format=instruction_on_response_format, \n",
    "                example_query=list_of_examples[shot_example], \n",
    "                example_response=list_of_responses[shot_example])\n",
    "        \n",
    "        bos_token = self.preprocessor.tokenizer.bos_token\n",
    "        prompt = bos_token + prompt + base_prompt + output \n",
    "                            \n",
    "        return prompt\n",
    "    \n",
    "    def _extract_inference_prompt(self, sentence:str) -> str:\n",
    "        if self.preprocessor.offset:\n",
    "            few_shots_responses = self.few_shots_dict[self.language]['responses_offset']\n",
    "        else:\n",
    "            few_shots_responses = self.few_shots_dict[self.language]['responses']\n",
    "        if self.n_shots_inference == 0:\n",
    "            list_of_examples = []\n",
    "            list_of_responses = []\n",
    "        else:\n",
    "            list_of_examples = self.few_shots_dict[self.language]['questions'][0:self.n_shots_inference]\n",
    "            list_of_responses = few_shots_responses[0:self.n_shots_inference]\n",
    "        inference_prompt = self._format_prompt_inference(input=sentence, \n",
    "                                                        instruction_on_response_format=self.preprocessor.instruction_on_response_format,\n",
    "                                                        offset=self.preprocessor.offset,\n",
    "                                                        output='',\n",
    "                                                        n_shots=self.n_shots_inference,\n",
    "                                                        list_of_examples=list_of_examples,\n",
    "                                                        list_of_responses=list_of_responses)\n",
    "        return {'inference_prompt': inference_prompt}\n",
    "    \n",
    "    def add_inference_prompt_column(self) -> None:\n",
    "        \"\"\"\n",
    "        Add the inferencePrompt and groundTruth columns to the test_data dataframe.\n",
    "        \"\"\"\n",
    "        self.test_data = self.test_data.map(lambda x: self._extract_inference_prompt(x['sentence']))\n",
    "    \n",
    "    def add_ground_truth_column(self) -> None:\n",
    "        \"\"\"\n",
    "        Add the groundTruth column to the test_data dataframe.\n",
    "        \"\"\"\n",
    "        self.test_data = self.test_data.map(lambda x: self._extract_ground_truth(x['prompt']))\n",
    "\n",
    "    def _generate_model_response(self, examples, model, tokenizer, max_new_tokens_factor:float) -> str:\n",
    "        device = \"cuda\"\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        input_sentences = examples['sentence']\n",
    "        prompts = examples['inference_prompt']\n",
    "        input_sentences_tokenized = tokenizer(input_sentences, return_tensors=\"pt\", padding=True)\n",
    "        print(prompts)\n",
    "        max_new_tokens = int(len(max(input_sentences_tokenized, key=len)) * max_new_tokens_factor)\n",
    "        # if self.preprocessor.model_type == 'gemma':\n",
    "        #     add_special_tokens = True\n",
    "        encodeds = tokenizer(prompts, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "        model_inputs = encodeds.to(device)\n",
    "        generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=max_new_tokens,  pad_token_id=tokenizer.eos_token_id) # max_new_tokens=max_new_tokens,\n",
    "        decoded = tokenizer.batch_decode(generated_ids)\n",
    "        decoded = [self._postprocess_model_output(i) for i in decoded]\n",
    "        return (decoded)\n",
    "                \n",
    "    def add_responses_column(self, model, tokenizer, batch_size:int, max_new_tokens_factor:float) -> None:\n",
    "        \"\"\"\n",
    "        Adds a column with the response of the model to the actual query.\n",
    "        \n",
    "        params:\n",
    "        model: the model to use to generate the response\n",
    "        tokenizer: the tokenizer to use to generate the response\n",
    "        batch_size: the batch size to use to process the examples. Increasing this makes it faster but requires more GPU. Default is 8.\n",
    "        max_new_tokens_factor: the factor conotrolling the number of new tokens to generate. This is a factor of the length of the input sentence.\n",
    "        \"\"\"\n",
    "        responses_col = []\n",
    "        total_rows = len(self.test_data)\n",
    "        indexes = [i for i in range(len(self.test_data)) if i % batch_size == 0]\n",
    "        max_index = self.test_data.shape[0]\n",
    "\n",
    "\n",
    "        with tqdm(total=total_rows, desc=\"generating responses\") as pbar:\n",
    "            for i, idx in enumerate(indexes[:-1]):\n",
    "                indici = list(range(idx, indexes[i+1]))\n",
    "                tmp = self._generate_model_response(self.test_data.select(indici), model, tokenizer, max_new_tokens_factor)\n",
    "                responses_col.extend(tmp)\n",
    "                pbar.update(batch_size)\n",
    "            indici = list(range(indexes[i+1], max_index))\n",
    "            tmp = self._generate_model_response(self.test_data.select(indici), model, tokenizer, max_new_tokens_factor)\n",
    "            responses_col.extend(tmp)\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "        self.test_data = self.test_data.add_column('model_responses', responses_col)\n",
    "    \n",
    "    def _postprocess_model_output(self, model_output: str) -> str:\n",
    "        \"\"\"\n",
    "        Postprocess the model output to remove the instruction and return the model response.\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the model output as it is returned by the model. The processing of the output is done in the function\n",
    "\n",
    "        return:\n",
    "        str: the model response, i.e. the model output without the instruction\n",
    "\n",
    "        \"\"\"\n",
    "        end_of_prompt_string = self.preprocessor.special_tokens_instruction['user_end'] + self.preprocessor.special_tokens_instruction['model_start']\n",
    "        return model_output.split(end_of_prompt_string, 1)[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
