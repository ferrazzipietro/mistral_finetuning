{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pferrazzi/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from tqdm import tqdm \n",
    "from datasets import Dataset\n",
    "from config import postprocessing\n",
    "from log import enlayer1_3epochs_4bits__ft_params as models_params\n",
    "from utils.generate_ft_adapters_list import generate_ft_adapters_list\n",
    "from utils.evaluator import Evaluator\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "similar_is_equal_list = postprocessing.similar_is_equal_list\n",
    "similar_is_equal_threshold_list = postprocessing.similar_is_equal_threshold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class OutputCleaner():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "  \n",
    "    def _remove_space_from_dict_keys(self, model_ouput_list: list) -> list:\n",
    "        \"\"\"\n",
    "        Remove the spaces from the keys of a dictionary. E.g., [{\"entity \": \"value\"}] -> [{\"entity\": \"value\"}]\n",
    "\n",
    "        Args:\n",
    "        model_ouput_list (dict): the list of dictionaries to be cleaned\n",
    "\n",
    "        return:\n",
    "        list: the cleaned list of dicts\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for dict in model_ouput_list:\n",
    "            out.append({k.replace(' ', ''): v for k, v in dict.items()})\n",
    "        return out\n",
    "    \n",
    "    def _drop_duplicates(self, model_response: list) -> str:\n",
    "        \"\"\"\n",
    "        Drop the duplicates from a list. This is useful when the model output contains the same entity multiple times.\n",
    "\n",
    "        Args:\n",
    "        model_response (str): the model response with no duplicates\n",
    "        \"\"\"\n",
    "        try :\n",
    "            return list({v['entity']:v for v in model_response}.values())\n",
    "        except Exception as error:\n",
    "            model_response = self._remove_space_from_dict_keys(model_response)\n",
    "            return list({v['entity']:v for v in model_response}.values())\n",
    "        \n",
    "    def _assess_model_output(self, model_response: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the model output is in the right format. If not, return False.\n",
    "        \n",
    "        Args:\n",
    "        model_output (str): the postprocessed model output after beeing passed to _postprocess_model_output()\n",
    "\n",
    "        return:\n",
    "        bool: True if the format is correct, False otherwise\n",
    "        \"\"\"\n",
    "        good_format = True\n",
    "        try :\n",
    "            res = json.loads(model_response)\n",
    "            # print( res)\n",
    "        except:\n",
    "            good_format = False\n",
    "        return good_format\n",
    "    \n",
    "    def _clean_model_output(self, example: dict) -> str:\n",
    "        \"\"\"\n",
    "        Postprocess the model output to return a json like formatted string that can be used to compute the F1 score.\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the model output as it is returned by the model. The processing of the output is done in the function\n",
    "\n",
    "        return:\n",
    "        str: the model response, i.e. the model output without the instruction\n",
    "\n",
    "        \"\"\"\n",
    "        def has_unclosed_square_brackets(s):\n",
    "            count = 0\n",
    "            for char in s:\n",
    "                if char == '[':\n",
    "                    count += 1\n",
    "                elif char == ']':\n",
    "                    count -= 1\n",
    "                    if count < 0:\n",
    "                        return True\n",
    "            return count > 0\n",
    "        \n",
    "        def has_unopen_square_brackets(s):\n",
    "            count = 0\n",
    "            for char in s:\n",
    "                if char == '[':\n",
    "                    count -= 1\n",
    "                elif char == ']':\n",
    "                    count += 1\n",
    "                    if count > 0:\n",
    "                        return True\n",
    "            return count > 0\n",
    "        \n",
    "        def is_list_of_lists(string):\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, list) for item in tmp):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_strings(string):\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, str) for item in tmp):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def is_string(string):\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, str):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        model_output = example['model_responses']\n",
    "\n",
    "        if model_output is None:\n",
    "            return {'model_output':'[{\"entity\":\"\"}]'}\n",
    "        \n",
    "        if is_list_of_lists(model_output):\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp = str(tmp[0])\n",
    "            return {'model_output':tmp}\n",
    "\n",
    "        if is_list_of_strings(model_output):\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp = [{\"entity\":el} for el in tmp]\n",
    "            tmp = str(tmp)\n",
    "            # print('TMP: ', tmp)\n",
    "            # raise Exception\n",
    "            return {'model_output': tmp}\n",
    "        \n",
    "        if is_string(model_output):\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp = [{\"entity\":tmp}]\n",
    "            tmp = str(tmp)\n",
    "            return {'model_output':tmp}\n",
    "\n",
    "        \n",
    "        if self._assess_model_output(model_output):\n",
    "            return {'model_output':model_output}\n",
    "        \n",
    "        if has_unopen_square_brackets(model_output):\n",
    "            last_bracket_index = model_output.rfind('],') # keep the closed list\n",
    "            model_output = '[' + model_output[:last_bracket_index+1] \n",
    "            return {'model_output':model_output} \n",
    "        \n",
    "        tmp = re.findall(r'\\[\\{(.+?)\\}\\]', model_output)\n",
    "        if len(tmp) != 0:\n",
    "            tmp = '[{' + tmp[0] + '}]'\n",
    "            if self._assess_model_output(tmp):\n",
    "                return {'model_output':tmp}\n",
    "            \n",
    "        if has_unclosed_square_brackets(model_output):\n",
    "            last_bracket_index = model_output.rfind('},') # find the last complete entity\n",
    "            model_output = model_output[:last_bracket_index+1] + ']' \n",
    "            return {'model_output':model_output} \n",
    "\n",
    "\n",
    "        if model_output.strip()[0] == '{':\n",
    "            tmp = '[' + model_output + ']'\n",
    "            if self._assess_model_output(tmp):\n",
    "                return {'model_output':tmp}\n",
    "            else:\n",
    "                last_bracket_index = model_output.rfind('},') # find the last complete entity\n",
    "                model_output = '[' + model_output[:last_bracket_index+1] + ']'\n",
    "                return {'model_output':model_output}\n",
    "            \n",
    "        if model_output.strip().startswith('[['):\n",
    "            tmp = model_output[1:]\n",
    "            if self._assess_model_output(tmp):\n",
    "                return {'model_output':tmp}\n",
    "        print('THIS IS A BROKEN ROW: ', model_output)\n",
    "\n",
    "        return {'model_output':model_output}\n",
    "\n",
    "    \n",
    "    def apply_cleaning(self, data) -> None:\n",
    "        \"\"\"\n",
    "        Apply the cleaning to the model output and return the cleaned response in a new cloumn called 'model_output\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the model output as it is returned by the model. The processing of the output is done in the function\n",
    "\n",
    "        return:\n",
    "        str: the model response, i.e. the model output without the instruction\n",
    "        \"\"\"\n",
    "        return data.map(lambda x: self._clean_model_output(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "#adapters_list = generate_ft_adapters_list(\"enlayer1_3epochs_4bits__ft_params\")\n",
    "evaluators = {}\n",
    "csv_files = glob.glob('data/test_data_processed/*.csv')\n",
    "#csv_files = ['data/test_data_processed/maxNewTokensFactor4_nShotsInference2_Mistral-7B-Instruct-v0.2_adapters_en.layer1_4_torch.bfloat16_16_32_0.05_2_0.0002.csv']\n",
    "evaluation_results = pd.DataFrame(columns=['file', 'similar_is_equal', 'similar_is_equal_threshold', 'f1_score', 'precision', 'recall'])\n",
    "\n",
    "print(evaluation_results)\n",
    "for file in csv_files:\n",
    "    if file.strip().endswith('0.002.csv'):\n",
    "        continue\n",
    "    print(\"FILE: \" , file)\n",
    "    eval_data = Dataset.from_csv(file) \n",
    "    output_cleaner = OutputCleaner()\n",
    "    cleaned_data = output_cleaner.apply_cleaning(eval_data)\n",
    "    for similar_is_equal in similar_is_equal_list:\n",
    "        if similar_is_equal:\n",
    "            for similar_is_equal_threshold in similar_is_equal_threshold_list:\n",
    "                evaluator = Evaluator(data=cleaned_data, offset=False, output_cleaner=output_cleaner)\n",
    "                evaluator.generate_evaluation_table(similar_is_equal=similar_is_equal_threshold, similar_is_equal_threshold=similar_is_equal_threshold)\n",
    "                evaluators[f\"{file}_SimilarIsEqual{similar_is_equal}_Threshold{similar_is_equal_threshold}\"] = evaluator\n",
    "                evaluation_results.loc[len(evaluation_results)] = {'file': file, 'similar_is_equal': similar_is_equal, 'similar_is_equal_threshold': similar_is_equal_threshold, 'f1_score': evaluator.evaluation_table['f1'], 'precision': evaluator.evaluation_table['precision'], 'recall': evaluator.evaluation_table['recall']}\n",
    "        elif not similar_is_equal:\n",
    "            evaluator = Evaluator(data=cleaned_data, offset=False, output_cleaner=output_cleaner)\n",
    "            evaluator.generate_evaluation_table(similar_is_equal=similar_is_equal, similar_is_equal_threshold=100)\n",
    "            evaluators[f\"{file}_SimilarIsEqual{similar_is_equal}\"] = evaluator\n",
    "            evaluation_results.loc[len(evaluation_results)] = {'file': file, 'similar_is_equal': similar_is_equal, 'similar_is_equal_threshold': similar_is_equal_threshold, 'f1_score': evaluator.evaluation_table['f1'], 'precision': evaluator.evaluation_table['precision'], 'recall': evaluator.evaluation_table['recall']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/test_data_processed/maxNewTokensFactor4_nShotsInference2_Mistral-7B-Instruct-v0.2_adapters_en.layer1_4_torch.bfloat16_16_32_0.05_4_0.0002.csv',\n",
       " 'data/test_data_processed/maxNewTokensFactor4_nShotsInference2_Mistral-7B-Instruct-v0.2_adapters_en.layer1_4_torch.bfloat16_16_32_0.05_4_0.0002.csv',\n",
       " 'data/test_data_processed/maxNewTokensFactor4_nShotsInference2_Mistral-7B-Instruct-v0.2_adapters_en.layer1_4_torch.bfloat16_16_32_0.05_4_0.0002.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluators\n",
    "evaluation_results[ evaluation_results['f1_score'] == evaluation_results['f1_score'].max()]\n",
    "evaluation_results[ evaluation_results['f1_score'] == evaluation_results['f1_score'].max()]['file'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m adapt, vals \u001b[38;5;129;01min\u001b[39;00m \u001b[43mevaluators\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madapt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluators' is not defined"
     ]
    }
   ],
   "source": [
    "for adapt, vals in evaluators.items():\n",
    "    print(f'adapt: {adapt}\\n {vals[\"f1\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/test_data_processed/maxNewTokensFactor4_nShotsInference2_Mistral-7B-Instruct-v0.2_adapters_en.layer1_4_torch.bfloat16_32_32_0.01_4_0.0002.csv'\n",
    "eval_data = Dataset.from_csv(file) \n",
    "#display(eval_data.to_pandas().head(3))\n",
    "output_cleaner = OutputCleaner()\n",
    "similar_is_equal = True\n",
    "similar_is_equal_threshold = 80\n",
    "cleaned_data = output_cleaner.apply_cleaning(eval_data)\n",
    "evaluator = Evaluator(data=cleaned_data, offset=False, output_cleaner=output_cleaner)\n",
    "evaluator.generate_evaluation_table(similar_is_equal=similar_is_equal, similar_is_equal_threshold=similar_is_equal_threshold)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
