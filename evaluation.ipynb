{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pferrazzi/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from utils.evaluator import Evaluator\n",
    "from utils.output_cleaner import OutputCleaner\n",
    "from datasets import Dataset\n",
    "from config import postprocessing\n",
    "from log import enlayer1_3epochs_4bits__ft_params as models_params\n",
    "from utils.generate_ft_adapters_list import generate_ft_adapters_list\n",
    "\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "similar_is_equal_list = postprocessing.similar_is_equal_list\n",
    "similar_is_equal_threshold_list = postprocessing.similar_is_equal_threshold_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_cleaner \u001b[38;5;241m=\u001b[39m OutputCleaner()\n\u001b[0;32m----> 2\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m output_cleaner\u001b[38;5;241m.\u001b[39mapply_cleaning(\u001b[43meval_data\u001b[49m)\n\u001b[1;32m      3\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator(data\u001b[38;5;241m=\u001b[39mcleaned_data, offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mgenerate_evaluation_table(similar_is_equal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, similar_is_equal_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_data' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "adapters_list = generate_ft_adapters_list(\"enlayer1_3epochs_4bits__ft_params\")\n",
    "evaluators = {}\n",
    "csv_files = glob.glob('data/test_data_processed/*.csv')\n",
    "\n",
    "for file in csv_files:\n",
    "    eval_data = Dataset.from_csv(file) \n",
    "    output_cleaner = OutputCleaner()\n",
    "    cleaned_data = output_cleaner.apply_cleaning(eval_data)\n",
    "    for similar_is_equal in similar_is_equal_list:\n",
    "        for similar_is_equal_threshold in similar_is_equal_threshold_list:\n",
    "            for adapters in adapters_list:\n",
    "                evaluator = Evaluator(data=cleaned_data, offset=False)\n",
    "                evaluator.generate_evaluation_table(similar_is_equal=True, similar_is_equal_threshold=80)\n",
    "                evaluators[f\"{adapters}_SimilarIsEqual{similar_is_equal}_Threshold{similar_is_equal_threshold}\"] = evaluator.evaluation_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cleaner = OutputCleaner()\n",
    "cleaned_data = output_cleaner.apply_cleaning(eval_data)\n",
    "\n",
    "evaluator = Evaluator(data=cleaned_data, offset=False)\n",
    "evaluator.generate_evaluation_table(similar_is_equal=True, similar_is_equal_threshold=80)\n",
    "tmp = evaluator.evaluation_table\n",
    "tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
