{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietroferrazzi/Desktop/dottorato/mistral_finetuning/.env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/pietroferrazzi/Desktop/dottorato/mistral_finetuning/.env/lib/python3.9/site-packages/datasets/load.py:1429: FutureWarning: The repository for bio-datasets/e3c contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bio-datasets/e3c\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dotenv import dotenv_values\n",
    "from utils.process_split import Process_split \n",
    "\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "#DATASET_CHEKPOINT = dotenv_values(\".env.base\")['DATASET_CHEKPOINT']\n",
    "hf_e3c = load_dataset(\"bio-datasets/e3c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ft = pd.DataFrame(columns=['input', 'output', 'layer', 'text'])\n",
    "splits_dict = {}\n",
    "splits = ['en.layer1', 'en.layer2', 'en.layer2.validation', 'en.layer3',\n",
    "          'es.layer1', 'es.layer2', 'es.layer2.validation', 'es.layer3',\n",
    "          'eu.layer1', 'eu.layer2', 'eu.layer2.validation', 'eu.layer3',\n",
    "          'it.layer1', 'it.layer2', 'it.layer2.validation', 'it.layer3',\n",
    "          'fr.layer1', 'fr.layer2', 'fr.layer2.validation', 'fr.layer3']\n",
    "\n",
    "for split_name in splits:\n",
    "    processed_split = Process_split.apply(hf_e3c, split_name, enitites_separator_in_output=\"|||\")\n",
    "    processed_split['text'] = processed_split['input'] + processed_split['output']\n",
    "    data_ft = Dataset.from_pandas(processed_split)\n",
    "    splits_dict[split_name] = data_ft\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 105.11ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 116.24ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 293.95ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 131.23ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 136.69ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 149.19ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 329.61ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 28.56ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 172.21ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 258.80ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 335.25ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 44.83ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 155.71ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 139.90ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 444.03ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 17.51ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:05<00:00,  5.61s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 143.99ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 140.17ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 320.32ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:01<00:00,  8.95ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:01<00:00,  8.47ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:25<00:00, 12.80s/it]\n",
      "README.md: 100%|██████████| 2.69k/2.69k [00:00<00:00, 864kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ferrazzipietro/e3c_finetuning/commit/d94d445d194b20f7947d713b155431c15a915862', commit_message='Upload dataset', commit_description='', oid='d94d445d194b20f7947d713b155431c15a915862', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddict = DatasetDict(splits_dict)\n",
    "ddict.push_to_hub(\"ferrazzipietro/e3c_finetuning\", token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400.75\n"
     ]
    }
   ],
   "source": [
    "# DA TENERE SOTTO CONTROLLO LA LUNGHEZZA MAX DI CONTESTO....\n",
    "longest_text = max(splits_dict['it.layer1']['text'], key=len)\n",
    "print(len(longest_text)*0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'offset: [47, 55] text: ematuria ||| offset: [88, 94] text: trauma ||| offset: [131, 136] text: massa ||| offset: [161, 169] text: partenza ||| offset: [210, 215] text: massa ||| offset: [250, 262] text: secondarismi ||| offset: [293, 306] text: ureterectomia ||| offset: [311, 318] text: referto ||| offset: [332, 338] text: tumore ||| offset: [393, 402] text: anaplasia ||| offset: [425, 431] text: inizia ||| offset: [432, 445] text: chemioterapia ||| offset: [454, 464] text: protocollo ||| offset: [506, 510] text: stop ||| offset: [511, 518] text: terapia ||| offset: [520, 522] text: TC ||| offset: [571, 579] text: immagini ||| offset: [644, 651] text: Biopsia ||| offset: [686, 695] text: Metastasi ||| offset: [804, 813] text: anaplasia ||| offset: [830, 836] text: inizia ||| offset: [837, 844] text: terapia ||| offset: [865, 874] text: raccoglie ||| offset: [892, 905] text: chemioterapia ||| offset: [923, 935] text: carboplatino ||| offset: [947, 956] text: etoposide ||| offset: [969, 977] text: melfalan ||| offset: [1003, 1014] text: reinfusione ||| offset: [1081, 1093] text: radioterapia ||| offset: [1153, 1163] text: formazioni ||| offset: [1174, 1182] text: diametro ||| offset: [1236, 1251] text: metastasectomia ||| offset: [1253, 1262] text: istologia ||| offset: [1265, 1274] text: Metastasi ||| offset: [1353, 1356] text: TVD ||| offset: [1381, 1391] text: incremento ||| offset: [1487, 1494] text: terapia ||| offset: [1572, 1582] text: incremento ||| offset: [1648, 1658] text: Paclitaxel ||| offset: [1693, 1703] text: incremento ||| offset: [1781, 1792] text: inefficacia ||| offset: [1846, 1853] text: opzioni ||| offset: [1907, 1916] text: colloquio ||| offset: [1945, 1952] text: avviare ||| offset: [1967, 1976] text: trapianto ||| offset: [2055, 2065] text: reattività ||| offset: [2101, 2109] text: idoneità ||| offset: [2143, 2151] text: donatore ||| offset: [2180, 2186] text: inizia ||| offset: [2218, 2227] text: melphalan ||| offset: [2239, 2247] text: thiotepa ||| offset: [2258, 2269] text: fludarabina ||| offset: [2281, 2284] text: ATG ||| offset: [2307, 2318] text: reinfusione ||| offset: [2412, 2421] text: linfociti ||| offset: [2427, 2433] text: infusi ||| offset: [2485, 2498] text: Attecchimento ||| offset: [2555, 2564] text: trapianto ||| offset: [2570, 2575] text: segni ||| offset: [2579, 2583] text: GvHD ||| offset: [2595, 2604] text: tossicità ||| offset: [2624, 2635] text: trattamento ||| offset: [2637, 2647] text: Dimissione ||| offset: [2668, 2677] text: trapianto ||| offset: [2710, 2719] text: trapianto ||| offset: [2745, 2751] text: numero ||| offset: [2864, 2866] text: TC ||| offset: [2884, 2893] text: riduzione ||| offset: [2943, 2952] text: Invariate ||| offset: [2967, 2974] text: lesioni ||| offset: [47, 68] text: ematuria macroscopica ||| offset: [88, 105] text: trauma addominale ||| offset: [332, 347] text: tumore di Wilms ||| offset: [686, 695] text: Metastasi ||| offset: [708, 723] text: Tumore di Wilms ||| offset: [1265, 1274] text: Metastasi ||| offset: [1287, 1302] text: tumore di Wilms ||| offset: [111, 117] text: Addome ||| offset: [181, 187] text: addome ||| offset: [190, 196] text: torace ||| offset: [360, 375] text: del rene destro ||| offset: [523, 533] text: del torace ||| offset: [605, 627] text: del mantello polmonare ||| offset: [652, 683] text: del lobo polmonare superiore dx ||| offset: [875, 879] text: CSEP ||| offset: [1131, 1137] text: Torace ||| offset: [1193, 1220] text: al segmento basale laterale ||| offset: [1373, 1379] text: torace ||| offset: [1554, 1560] text: torace ||| offset: [1675, 1681] text: torace ||| offset: [2755, 2777] text: cellule CD16+CD56+/CD3 ||| offset: [2867, 2873] text: torace ||| offset: [0, 21] text: C AM, femmina, 7 anni ||| offset: [1921, 1931] text: i genitori ||| offset: [1953, 1964] text: la paziente ||| offset: [2026, 2034] text: genitore ||| offset: [2069, 2087] text: entrambi ge nitori ||| offset: [2117, 2128] text: della madre ||| offset: [2157, 2166] text: sul padre ||| offset: [2434, 2445] text: al paziente ||| offset: [1187, 1191] text: 5 mm ||| offset: [2783, 2785] text: 2% ||| offset: [2789, 2821] text: 86% del totale dei GB circolanti ||| offset: [2827, 2833] text: 70/mcl ||| offset: [2836, 2844] text: 3200/mcl ||| offset: [23, 33] text: Marzo 2008 ||| offset: [274, 284] text: Marzo 2008 ||| offset: [413, 424] text: Aprile 2008 ||| offset: [491, 505] text: Settembre 2008 ||| offset: [816, 829] text: Novembre 2008 ||| offset: [881, 891] text: Marzo 2009 ||| offset: [1069, 1080] text: Maggio 2009 ||| offset: [1115, 1127] text: Ottobre 2009 ||| offset: [1339, 1352] text: Novembre 2009 ||| offset: [1460, 1473] text: Febbraio 2010 ||| offset: [1636, 1647] text: Maggio 2010 ||| offset: [2168, 2179] text: Agosto 2010 ||| offset: [2538, 2550] text: giornata +14 ||| offset: [2651, 2663] text: giornata +24 ||| offset: [2686, 2696] text: giorno +14 ||| offset: [2702, 2705] text: +50 ||| offset: [2846, 2863] text: 28 Settembre 2010 </s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_dict['it.layer1']['output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5348.7\n"
     ]
    }
   ],
   "source": [
    "longest_string = max(splits_dict['it.layer1']['output'], key=len)\n",
    "print(len(longest_string)*0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2879.7999999999997\n"
     ]
    }
   ],
   "source": [
    "longest_string = max(splits_dict['it.layer1']['input'], key=len)\n",
    "print(len(longest_string)*0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ft = Dataset.from_pandas(data_ft)\n",
    "data_ft.push_to_hub(DATASET_CHEKPOINT, token=HF_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
