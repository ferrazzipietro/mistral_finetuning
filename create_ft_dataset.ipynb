{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietroferrazzi/Desktop/dottorato/mistral_finetuning/.env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dotenv import dotenv_values\n",
    "from utils.process_split import Process_split \n",
    "\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "#DATASET_CHEKPOINT = dotenv_values(\".env.base\")['DATASET_CHEKPOINT']\n",
    "hf_e3c = load_dataset(\"bio-datasets/e3c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    en.layer1: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 84\n",
       "    })\n",
       "    en.layer2: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 171\n",
       "    })\n",
       "    en.layer2.validation: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 19\n",
       "    })\n",
       "    en.layer3: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 9779\n",
       "    })\n",
       "    es.layer1: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 81\n",
       "    })\n",
       "    es.layer2: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 162\n",
       "    })\n",
       "    es.layer2.validation: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 18\n",
       "    })\n",
       "    es.layer3: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 1876\n",
       "    })\n",
       "    eu.layer1: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    eu.layer2: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 111\n",
       "    })\n",
       "    eu.layer2.validation: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    eu.layer3: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 1232\n",
       "    })\n",
       "    it.layer1: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 86\n",
       "    })\n",
       "    it.layer2: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 174\n",
       "    })\n",
       "    it.layer2.validation: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 18\n",
       "    })\n",
       "    it.layer3: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 10213\n",
       "    })\n",
       "    fr.layer1: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 81\n",
       "    })\n",
       "    fr.layer2: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 168\n",
       "    })\n",
       "    fr.layer2.validation: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 18\n",
       "    })\n",
       "    fr.layer3: Dataset({\n",
       "        features: ['input', 'output', 'language', 'layer', 'text'],\n",
       "        num_rows: 25740\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_split = Process_split.apply(hf_e3c, 'it.layer1', enitites_separator_in_output=\"|||\")\n",
    "processed_split['text'] = processed_split['input'] + processed_split['output']\n",
    "data_ft = Dataset.from_pandas(processed_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ft = pd.DataFrame(columns=['input', 'output', 'layer', 'text'])\n",
    "splits_dict = {}\n",
    "splits = ['en.layer1', 'en.layer2', 'en.layer2.validation', 'en.layer3',\n",
    "          'es.layer1', 'es.layer2', 'es.layer2.validation', 'es.layer3',\n",
    "          'eu.layer1', 'eu.layer2', 'eu.layer2.validation', 'eu.layer3',\n",
    "          'it.layer1', 'it.layer2', 'it.layer2.validation', 'it.layer3',\n",
    "          'fr.layer1', 'fr.layer2', 'fr.layer2.validation', 'fr.layer3']\n",
    "\n",
    "for split_name in splits:\n",
    "    processed_split = Process_split.apply(hf_e3c, split_name, enitites_separator_in_output=\"|||\")\n",
    "    processed_split['text'] = processed_split['input'] + processed_split['output']\n",
    "    data_ft = Dataset.from_pandas(processed_split)\n",
    "    splits_dict[split_name] = data_ft\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 138.73ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 146.84ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 352.73ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 138.95ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.25s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 152.95ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 125.61ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 375.53ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 30.65ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.13s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 108.68ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 245.70ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 356.78ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 41.81ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.49s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 160.63ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 172.36ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 311.89ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 18.95ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:25<00:00, 25.81s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 146.58ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 137.40ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 388.51ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:01<00:00,  9.31ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:01<00:00,  9.25ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [01:49<00:00, 54.89s/it]\n",
      "/Users/pietroferrazzi/Desktop/dottorato/mistral_finetuning/.env/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'list_files_info' (from 'huggingface_hub.hf_api') is deprecated and will be removed from version '0.23'. Use `list_repo_tree` and `get_paths_info` instead.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ddict = DatasetDict(splits_dict)\n",
    "ddict.push_to_hub(\"ferrazzipietro/e3c_finetuning_processed\", token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400.75\n"
     ]
    }
   ],
   "source": [
    "# DA TENERE SOTTO CONTROLLO LA LUNGHEZZA MAX DI CONTESTO....\n",
    "longest_text = max(splits_dict['it.layer1']['text'], key=len)\n",
    "print(len(longest_text)*0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'offset: [47, 55] text: ematuria ||| offset: [88, 94] text: trauma ||| offset: [131, 136] text: massa ||| offset: [161, 169] text: partenza ||| offset: [210, 215] text: massa ||| offset: [250, 262] text: secondarismi ||| offset: [293, 306] text: ureterectomia ||| offset: [311, 318] text: referto ||| offset: [332, 338] text: tumore ||| offset: [393, 402] text: anaplasia ||| offset: [425, 431] text: inizia ||| offset: [432, 445] text: chemioterapia ||| offset: [454, 464] text: protocollo ||| offset: [506, 510] text: stop ||| offset: [511, 518] text: terapia ||| offset: [520, 522] text: TC ||| offset: [571, 579] text: immagini ||| offset: [644, 651] text: Biopsia ||| offset: [686, 695] text: Metastasi ||| offset: [804, 813] text: anaplasia ||| offset: [830, 836] text: inizia ||| offset: [837, 844] text: terapia ||| offset: [865, 874] text: raccoglie ||| offset: [892, 905] text: chemioterapia ||| offset: [923, 935] text: carboplatino ||| offset: [947, 956] text: etoposide ||| offset: [969, 977] text: melfalan ||| offset: [1003, 1014] text: reinfusione ||| offset: [1081, 1093] text: radioterapia ||| offset: [1153, 1163] text: formazioni ||| offset: [1174, 1182] text: diametro ||| offset: [1236, 1251] text: metastasectomia ||| offset: [1253, 1262] text: istologia ||| offset: [1265, 1274] text: Metastasi ||| offset: [1353, 1356] text: TVD ||| offset: [1381, 1391] text: incremento ||| offset: [1487, 1494] text: terapia ||| offset: [1572, 1582] text: incremento ||| offset: [1648, 1658] text: Paclitaxel ||| offset: [1693, 1703] text: incremento ||| offset: [1781, 1792] text: inefficacia ||| offset: [1846, 1853] text: opzioni ||| offset: [1907, 1916] text: colloquio ||| offset: [1945, 1952] text: avviare ||| offset: [1967, 1976] text: trapianto ||| offset: [2055, 2065] text: reattività ||| offset: [2101, 2109] text: idoneità ||| offset: [2143, 2151] text: donatore ||| offset: [2180, 2186] text: inizia ||| offset: [2218, 2227] text: melphalan ||| offset: [2239, 2247] text: thiotepa ||| offset: [2258, 2269] text: fludarabina ||| offset: [2281, 2284] text: ATG ||| offset: [2307, 2318] text: reinfusione ||| offset: [2412, 2421] text: linfociti ||| offset: [2427, 2433] text: infusi ||| offset: [2485, 2498] text: Attecchimento ||| offset: [2555, 2564] text: trapianto ||| offset: [2570, 2575] text: segni ||| offset: [2579, 2583] text: GvHD ||| offset: [2595, 2604] text: tossicità ||| offset: [2624, 2635] text: trattamento ||| offset: [2637, 2647] text: Dimissione ||| offset: [2668, 2677] text: trapianto ||| offset: [2710, 2719] text: trapianto ||| offset: [2745, 2751] text: numero ||| offset: [2864, 2866] text: TC ||| offset: [2884, 2893] text: riduzione ||| offset: [2943, 2952] text: Invariate ||| offset: [2967, 2974] text: lesioni ||| offset: [47, 68] text: ematuria macroscopica ||| offset: [88, 105] text: trauma addominale ||| offset: [332, 347] text: tumore di Wilms ||| offset: [686, 695] text: Metastasi ||| offset: [708, 723] text: Tumore di Wilms ||| offset: [1265, 1274] text: Metastasi ||| offset: [1287, 1302] text: tumore di Wilms ||| offset: [111, 117] text: Addome ||| offset: [181, 187] text: addome ||| offset: [190, 196] text: torace ||| offset: [360, 375] text: del rene destro ||| offset: [523, 533] text: del torace ||| offset: [605, 627] text: del mantello polmonare ||| offset: [652, 683] text: del lobo polmonare superiore dx ||| offset: [875, 879] text: CSEP ||| offset: [1131, 1137] text: Torace ||| offset: [1193, 1220] text: al segmento basale laterale ||| offset: [1373, 1379] text: torace ||| offset: [1554, 1560] text: torace ||| offset: [1675, 1681] text: torace ||| offset: [2755, 2777] text: cellule CD16+CD56+/CD3 ||| offset: [2867, 2873] text: torace ||| offset: [0, 21] text: C AM, femmina, 7 anni ||| offset: [1921, 1931] text: i genitori ||| offset: [1953, 1964] text: la paziente ||| offset: [2026, 2034] text: genitore ||| offset: [2069, 2087] text: entrambi ge nitori ||| offset: [2117, 2128] text: della madre ||| offset: [2157, 2166] text: sul padre ||| offset: [2434, 2445] text: al paziente ||| offset: [1187, 1191] text: 5 mm ||| offset: [2783, 2785] text: 2% ||| offset: [2789, 2821] text: 86% del totale dei GB circolanti ||| offset: [2827, 2833] text: 70/mcl ||| offset: [2836, 2844] text: 3200/mcl ||| offset: [23, 33] text: Marzo 2008 ||| offset: [274, 284] text: Marzo 2008 ||| offset: [413, 424] text: Aprile 2008 ||| offset: [491, 505] text: Settembre 2008 ||| offset: [816, 829] text: Novembre 2008 ||| offset: [881, 891] text: Marzo 2009 ||| offset: [1069, 1080] text: Maggio 2009 ||| offset: [1115, 1127] text: Ottobre 2009 ||| offset: [1339, 1352] text: Novembre 2009 ||| offset: [1460, 1473] text: Febbraio 2010 ||| offset: [1636, 1647] text: Maggio 2010 ||| offset: [2168, 2179] text: Agosto 2010 ||| offset: [2538, 2550] text: giornata +14 ||| offset: [2651, 2663] text: giornata +24 ||| offset: [2686, 2696] text: giorno +14 ||| offset: [2702, 2705] text: +50 ||| offset: [2846, 2863] text: 28 Settembre 2010 </s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_dict['it.layer1']['output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5348.7\n"
     ]
    }
   ],
   "source": [
    "longest_string = max(splits_dict['it.layer1']['output'], key=len)\n",
    "print(len(longest_string)*0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2879.7999999999997\n"
     ]
    }
   ],
   "source": [
    "longest_string = max(splits_dict['it.layer1']['input'], key=len)\n",
    "print(len(longest_string)*0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ft = Dataset.from_pandas(data_ft)\n",
    "data_ft.push_to_hub(DATASET_CHEKPOINT, token=HF_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
