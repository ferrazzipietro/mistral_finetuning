{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietroferrazzi/Desktop/dottorato/mistral_finetuning/.env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 15.7k/15.7k [00:00<00:00, 15.5MB/s]\n",
      "Downloading readme: 100%|██████████| 4.02k/4.02k [00:00<00:00, 34.0MB/s]\n",
      "Downloading data: 230MB [00:33, 6.96MB/s] \n",
      "Generating en.layer1 split: 100%|██████████| 84/84 [00:00<00:00, 373.29 examples/s]\n",
      "Generating en.layer2 split: 100%|██████████| 171/171 [00:00<00:00, 770.02 examples/s]\n",
      "Generating en.layer2.validation split: 100%|██████████| 19/19 [00:00<00:00, 897.32 examples/s]\n",
      "Generating en.layer3 split: 100%|██████████| 9779/9779 [00:01<00:00, 5639.26 examples/s]\n",
      "Generating es.layer1 split: 100%|██████████| 81/81 [00:00<00:00, 372.22 examples/s]\n",
      "Generating es.layer2 split: 100%|██████████| 162/162 [00:00<00:00, 695.83 examples/s]\n",
      "Generating es.layer2.validation split: 100%|██████████| 18/18 [00:00<00:00, 839.76 examples/s]\n",
      "Generating es.layer3 split: 100%|██████████| 1876/1876 [00:00<00:00, 13372.66 examples/s]\n",
      "Generating eu.layer1 split: 100%|██████████| 90/90 [00:00<00:00, 241.21 examples/s]\n",
      "Generating eu.layer2 split: 100%|██████████| 111/111 [00:00<00:00, 1412.29 examples/s]\n",
      "Generating eu.layer2.validation split: 100%|██████████| 10/10 [00:00<00:00, 551.93 examples/s]\n",
      "Generating eu.layer3 split: 100%|██████████| 1232/1232 [00:00<00:00, 5777.01 examples/s]\n",
      "Generating fr.layer1 split: 100%|██████████| 81/81 [00:00<00:00, 392.20 examples/s]\n",
      "Generating fr.layer2 split: 100%|██████████| 168/168 [00:00<00:00, 723.62 examples/s]\n",
      "Generating fr.layer2.validation split: 100%|██████████| 18/18 [00:00<00:00, 779.53 examples/s]\n",
      "Generating fr.layer3 split: 100%|██████████| 25740/25740 [00:07<00:00, 3554.59 examples/s]\n",
      "Generating it.layer1 split: 100%|██████████| 86/86 [00:00<00:00, 454.65 examples/s]\n",
      "Generating it.layer2 split: 100%|██████████| 174/174 [00:00<00:00, 871.16 examples/s]\n",
      "Generating it.layer2.validation split: 100%|██████████| 18/18 [00:00<00:00, 366.40 examples/s]\n",
      "Generating it.layer3 split: 100%|██████████| 10213/10213 [00:02<00:00, 4351.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dotenv import dotenv_values\n",
    "from utils.data_generation import DataGenerator \n",
    "\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "#DATASET_CHEKPOINT = dotenv_values(\".env.base\")['DATASET_CHEKPOINT']\n",
    "hf_e3c = load_dataset(\"ferrazzipietro/e3c\", token = HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing en.layer1\n",
      "Processing en.layer2\n",
      "Processing en.layer2.validation\n",
      "Processing en.layer3\n",
      "Processing es.layer1\n",
      "Processing es.layer2\n",
      "Processing es.layer2.validation\n",
      "Processing es.layer3\n",
      "Processing eu.layer1\n",
      "Processing eu.layer2\n",
      "Processing eu.layer2.validation\n",
      "Processing eu.layer3\n",
      "Processing it.layer1\n",
      "Processing it.layer2\n",
      "Processing it.layer2.validation\n",
      "Processing it.layer3\n",
      "Processing fr.layer1\n",
      "Processing fr.layer2\n",
      "Processing fr.layer2.validation\n",
      "Processing fr.layer3\n"
     ]
    }
   ],
   "source": [
    "splits_dict = {}\n",
    "splits = ['en.layer1', 'en.layer2', 'en.layer2.validation', 'en.layer3',\n",
    "          'es.layer1', 'es.layer2', 'es.layer2.validation', 'es.layer3',\n",
    "          'eu.layer1', 'eu.layer2', 'eu.layer2.validation', 'eu.layer3',\n",
    "          'it.layer1', 'it.layer2', 'it.layer2.validation', 'it.layer3',\n",
    "          'fr.layer1', 'fr.layer2', 'fr.layer2.validation', 'fr.layer3']\n",
    "process_split = DataGenerator()\n",
    "\n",
    "for split_name in splits:\n",
    "    print(f\"Processing {split_name}\")\n",
    "    processed_split = process_split.split_into_sentences(hf_e3c[split_name], split_name)\n",
    "    data_ft = Dataset.from_pandas(processed_split)\n",
    "    splits_dict[split_name] = data_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 138.07ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 175.97ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 243.77ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 191.42ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 168.25ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 187.98ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 261.10ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 45.40ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 176.15ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 146.21ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 188.29ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 70.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 314.51ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 184.35ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 216.45ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 46.13ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.36s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 378.58ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 212.56ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 414.13ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 26/26 [00:01<00:00, 24.56ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:13<00:00, 13.81s/it]\n",
      "/Users/pietroferrazzi/Desktop/dottorato/mistral_finetuning/.env/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'list_files_info' (from 'huggingface_hub.hf_api') is deprecated and will be removed from version '0.23'. Use `list_repo_tree` and `get_paths_info` instead.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "README.md: 100%|██████████| 2.97k/2.97k [00:00<00:00, 1.03MB/s]\n"
     ]
    }
   ],
   "source": [
    "ddict = DatasetDict(splits_dict)\n",
    "ddict.push_to_hub(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'La paciente no presentaba adenopatías palpables significativas en el cuello y el resto del examen físico no presentaba particularidades.',\n",
       " 'entities': [{'id': '4347',\n",
       "   'offsets': [26, 37],\n",
       "   'role': '',\n",
       "   'semantic_type_id': 'C0497156',\n",
       "   'text': 'adenopatías',\n",
       "   'type': 'CLINENTITY'}],\n",
       " 'original_text': 'Mujer de 63 años sin antecedentes de importancia que consulta por una masa de consistencia blanda de 3 cm de diámetro en la región parotídea izquierda con una evolución de 4 meses, acompañada de una parálisis periférica de la rama cervical del nervio facial con desviación de la comisura labial homolateral. La tumoración, móvil e indolora, se ponía en evidencia fácilmente con la palpación bimanual y era predominantemente intraoral. La paciente no presentaba adenopatías palpables significativas en el cuello y el resto del examen físico no presentaba particularidades.  En interconsulta con Neurología se descartó la posibilidad de una parálisis facial central mediante una tomografía computarizada del cerebro.  La ecografía de las partes blandas fue informada como una masa heterogénea sólida con densidad de las partes blandas en la región parotídea y sin presencia de ganglios de rango adenomegálico en el cuello.  La punción aspiración con aguja fina bajo control ecográfico fue informada como negativa con material hemático en 2 oportunidades. Se decidió entonces realizar una biopsia por congelación bajo anestesia general con un abordaje intraoral por el cual la tumoración se presentaba accesible haciendo protrusión bajo la mucosa oral junto al ostium del conducto de Stenon.  Tras incidir en la mucosa se halló un tumor laxo, color rojo vinoso, de aspecto vascular, que se extirpó sin dificultad por la misma incisión, comprobándose su continuidad con el tejido parotídeo de características normales. La biopsia por congelación informó una lesión compatible con un tumor vascular benigno. Se cerró la mucosa con puntos separados de ácido poliglicólico y se dio por finalizado el procedimiento. La evolución fue favorable con alta hospitalaria a las 24 h y recuperación completa de la parálisis facial a los 10 d del postoperatorio. No presenta evidencias de enfermedad tras 5 años de seguimiento. El informe histológico diferido fue hemangioma capilar de la parótida..',\n",
       " 'original_id': 'ES100719'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_dict['es.layer2'][41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'it.layer1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# DA TENERE SOTTO CONTROLLO LA LUNGHEZZA MAX DI CONTESTO....\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m longest_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[43msplits_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mit.layer1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(longest_text)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.75\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'it.layer1'"
     ]
    }
   ],
   "source": [
    "# DA TENERE SOTTO CONTROLLO LA LUNGHEZZA MAX DI CONTESTO....\n",
    "longest_text = max(splits_dict['it.layer1']['text'], key=len)\n",
    "print(len(longest_text)*0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'offset: [47, 55] text: ematuria ||| offset: [88, 94] text: trauma ||| offset: [131, 136] text: massa ||| offset: [161, 169] text: partenza ||| offset: [210, 215] text: massa ||| offset: [250, 262] text: secondarismi ||| offset: [293, 306] text: ureterectomia ||| offset: [311, 318] text: referto ||| offset: [332, 338] text: tumore ||| offset: [393, 402] text: anaplasia ||| offset: [425, 431] text: inizia ||| offset: [432, 445] text: chemioterapia ||| offset: [454, 464] text: protocollo ||| offset: [506, 510] text: stop ||| offset: [511, 518] text: terapia ||| offset: [520, 522] text: TC ||| offset: [571, 579] text: immagini ||| offset: [644, 651] text: Biopsia ||| offset: [686, 695] text: Metastasi ||| offset: [804, 813] text: anaplasia ||| offset: [830, 836] text: inizia ||| offset: [837, 844] text: terapia ||| offset: [865, 874] text: raccoglie ||| offset: [892, 905] text: chemioterapia ||| offset: [923, 935] text: carboplatino ||| offset: [947, 956] text: etoposide ||| offset: [969, 977] text: melfalan ||| offset: [1003, 1014] text: reinfusione ||| offset: [1081, 1093] text: radioterapia ||| offset: [1153, 1163] text: formazioni ||| offset: [1174, 1182] text: diametro ||| offset: [1236, 1251] text: metastasectomia ||| offset: [1253, 1262] text: istologia ||| offset: [1265, 1274] text: Metastasi ||| offset: [1353, 1356] text: TVD ||| offset: [1381, 1391] text: incremento ||| offset: [1487, 1494] text: terapia ||| offset: [1572, 1582] text: incremento ||| offset: [1648, 1658] text: Paclitaxel ||| offset: [1693, 1703] text: incremento ||| offset: [1781, 1792] text: inefficacia ||| offset: [1846, 1853] text: opzioni ||| offset: [1907, 1916] text: colloquio ||| offset: [1945, 1952] text: avviare ||| offset: [1967, 1976] text: trapianto ||| offset: [2055, 2065] text: reattività ||| offset: [2101, 2109] text: idoneità ||| offset: [2143, 2151] text: donatore ||| offset: [2180, 2186] text: inizia ||| offset: [2218, 2227] text: melphalan ||| offset: [2239, 2247] text: thiotepa ||| offset: [2258, 2269] text: fludarabina ||| offset: [2281, 2284] text: ATG ||| offset: [2307, 2318] text: reinfusione ||| offset: [2412, 2421] text: linfociti ||| offset: [2427, 2433] text: infusi ||| offset: [2485, 2498] text: Attecchimento ||| offset: [2555, 2564] text: trapianto ||| offset: [2570, 2575] text: segni ||| offset: [2579, 2583] text: GvHD ||| offset: [2595, 2604] text: tossicità ||| offset: [2624, 2635] text: trattamento ||| offset: [2637, 2647] text: Dimissione ||| offset: [2668, 2677] text: trapianto ||| offset: [2710, 2719] text: trapianto ||| offset: [2745, 2751] text: numero ||| offset: [2864, 2866] text: TC ||| offset: [2884, 2893] text: riduzione ||| offset: [2943, 2952] text: Invariate ||| offset: [2967, 2974] text: lesioni ||| offset: [47, 68] text: ematuria macroscopica ||| offset: [88, 105] text: trauma addominale ||| offset: [332, 347] text: tumore di Wilms ||| offset: [686, 695] text: Metastasi ||| offset: [708, 723] text: Tumore di Wilms ||| offset: [1265, 1274] text: Metastasi ||| offset: [1287, 1302] text: tumore di Wilms ||| offset: [111, 117] text: Addome ||| offset: [181, 187] text: addome ||| offset: [190, 196] text: torace ||| offset: [360, 375] text: del rene destro ||| offset: [523, 533] text: del torace ||| offset: [605, 627] text: del mantello polmonare ||| offset: [652, 683] text: del lobo polmonare superiore dx ||| offset: [875, 879] text: CSEP ||| offset: [1131, 1137] text: Torace ||| offset: [1193, 1220] text: al segmento basale laterale ||| offset: [1373, 1379] text: torace ||| offset: [1554, 1560] text: torace ||| offset: [1675, 1681] text: torace ||| offset: [2755, 2777] text: cellule CD16+CD56+/CD3 ||| offset: [2867, 2873] text: torace ||| offset: [0, 21] text: C AM, femmina, 7 anni ||| offset: [1921, 1931] text: i genitori ||| offset: [1953, 1964] text: la paziente ||| offset: [2026, 2034] text: genitore ||| offset: [2069, 2087] text: entrambi ge nitori ||| offset: [2117, 2128] text: della madre ||| offset: [2157, 2166] text: sul padre ||| offset: [2434, 2445] text: al paziente ||| offset: [1187, 1191] text: 5 mm ||| offset: [2783, 2785] text: 2% ||| offset: [2789, 2821] text: 86% del totale dei GB circolanti ||| offset: [2827, 2833] text: 70/mcl ||| offset: [2836, 2844] text: 3200/mcl ||| offset: [23, 33] text: Marzo 2008 ||| offset: [274, 284] text: Marzo 2008 ||| offset: [413, 424] text: Aprile 2008 ||| offset: [491, 505] text: Settembre 2008 ||| offset: [816, 829] text: Novembre 2008 ||| offset: [881, 891] text: Marzo 2009 ||| offset: [1069, 1080] text: Maggio 2009 ||| offset: [1115, 1127] text: Ottobre 2009 ||| offset: [1339, 1352] text: Novembre 2009 ||| offset: [1460, 1473] text: Febbraio 2010 ||| offset: [1636, 1647] text: Maggio 2010 ||| offset: [2168, 2179] text: Agosto 2010 ||| offset: [2538, 2550] text: giornata +14 ||| offset: [2651, 2663] text: giornata +24 ||| offset: [2686, 2696] text: giorno +14 ||| offset: [2702, 2705] text: +50 ||| offset: [2846, 2863] text: 28 Settembre 2010 </s>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_dict['it.layer1']['output'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5348.7\n"
     ]
    }
   ],
   "source": [
    "longest_string = max(splits_dict['it.layer1']['output'], key=len)\n",
    "print(len(longest_string)*0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2879.7999999999997\n"
     ]
    }
   ],
   "source": [
    "longest_string = max(splits_dict['it.layer1']['input'], key=len)\n",
    "print(len(longest_string)*0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ft = Dataset.from_pandas(data_ft)\n",
    "data_ft.push_to_hub(DATASET_CHEKPOINT, token=HF_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
