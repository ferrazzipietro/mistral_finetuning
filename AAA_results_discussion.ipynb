{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is the best model for Llama 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/llama/7B_NoQuant_FT/maxNewTokensFactor8_nShotsInference0_llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_16_32_0.01_2_0.0002.csv\"\n",
    "apadpetrs_checkpoint = \"ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_16_32_0.01_2_0.0002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/mistral_finetuning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pferrazzi/mistral_finetuning/.venv/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "class OutputAnalist():\n",
    "    def __init__(self, data, verbose=False) -> None:\n",
    "        self.verbose = verbose\n",
    "        self.data = data\n",
    "        self.counter_dict = {\n",
    "            'perfect_output':0,\n",
    "            'is_empty_list':0,\n",
    "            'is_list_of_lists':0,\n",
    "            'is_list_of_dicts':0,\n",
    "            'is_list_of_lists_and_dict':0,\n",
    "            'is_list_of_strings':0,\n",
    "            'is_list_of_empty_dict':0,\n",
    "            'is_list_with_one_empty_dict':0,\n",
    "            'is_list_of_dicts_with_empty_lists':0,\n",
    "            'is_list_of_dicts_with_one_key_multiple_values':0,\n",
    "            'is_list_of_dicts_with_multiple_keys_included_entity':0,\n",
    "            'is_list_of_dict_numeric_values':0,\n",
    "            'is_list_of_dicts_none_values':0,\n",
    "            'is_list_of_dicts_and_strings':0,\n",
    "            'is_list_of_dicts_and_lists_of_strings':0,\n",
    "            'is_list_of_dicts_with_value_list':0,\n",
    "            'is_string':0,\n",
    "            'is_list_of_strings_representing_dicts':0,\n",
    "            'is_list_of_dicts_of_lists':0,\n",
    "            'is_numeric':0,\n",
    "            'are_entities_extracted_as_dict_keys_instead_of_values':0,\n",
    "            'uknown':0\n",
    "        }\n",
    "  \n",
    "    def _remove_space_from_dict_keys(self, model_ouput_list: list) -> list:\n",
    "        \"\"\"\n",
    "        Remove the spaces from the keys of a dictionary. E.g., [{\"entity \": \"value\"}] -> [{\"entity\": \"value\"}]\n",
    "\n",
    "        Args:\n",
    "        model_ouput_list (dict): the list of dictionaries to be cleaned\n",
    "\n",
    "        return:\n",
    "        list: the cleaned list of dicts\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for dict in model_ouput_list:\n",
    "            # print('DICT: ', dict)\n",
    "            out.append({k.replace(' ', ''): v for k, v in dict.items()})\n",
    "        return out\n",
    "    \n",
    "    def _drop_duplicates(self, model_response: list) -> str:\n",
    "        \"\"\"\n",
    "        Drop the duplicates from a list. This is useful when the model output contains the same entity multiple times.\n",
    "\n",
    "        Args:\n",
    "        model_response (str): the model response with no duplicates\n",
    "        \"\"\"\n",
    "        # print('DROPPING DUPLICATES: ', model_response)\n",
    "        try :\n",
    "            return list({v['entity']:v for v in model_response}.values())\n",
    "        except Exception as error:\n",
    "            model_response = self._remove_space_from_dict_keys(model_response)\n",
    "            # print('ERROR: ', model_response)\n",
    "            return list({v['entity']:v for v in model_response}.values())\n",
    "        \n",
    "    def _assess_model_output(self, model_response: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the model output is in the right format. If not, return False.\n",
    "        \n",
    "        Args:\n",
    "        model_output (str): the postprocessed model output after beeing passed to _postprocess_model_output()\n",
    "\n",
    "        return:\n",
    "        bool: True if the format is correct, False otherwise\n",
    "        \"\"\"\n",
    "        good_format = True\n",
    "        try :\n",
    "            res = json.loads(model_response)\n",
    "            # print( res)\n",
    "        except:\n",
    "            good_format = False\n",
    "        return good_format\n",
    "\n",
    "            \n",
    "    def _remove_json_special_chars(self, string):\n",
    "        \"\"\"\n",
    "        Remove the special characters from a string. This is useful when the model output contains special characters that are not allowed in the json format.\n",
    "        \"\"\"\n",
    "        #Â print('sto pulendo: ', string)\n",
    "        chars = ['\\xa0', '\\x80', '\\x93', '\\U00100000', '\\r\\n', '\\U00100000I', '\\\\u001d', '\\\\\"']\n",
    "        for char in chars:\n",
    "            string = string.replace(char, ' ')\n",
    "        char_no_space = ['\\xad']\n",
    "        for char in char_no_space:\n",
    "            string = string.replace(char, '')\n",
    "        string = string.replace('\\\\u0010', '^')\n",
    "        return string\n",
    "    \n",
    "    \n",
    "    def _clean_ground_truth(self, example: dict) -> dict:\n",
    "        ground_truth = example['ground_truth']\n",
    "        # print('inner ground truth: ', ground_truth)\n",
    "        ground_truth = self._remove_json_special_chars(ground_truth)\n",
    "        ground_truth = ground_truth.replace('</s>', '').replace('<|im_e', '').replace('<|end_of_text|>', '').replace('<|endoftext|>', '')\n",
    "        return({'ground_truth': ground_truth})\n",
    "\n",
    "    def _clean_model_output(self, example: dict,  wrong_keys_to_entity:bool, latest_version:bool=True) -> dict:\n",
    "        \"\"\"\n",
    "        Postprocess the model output to return a json like formatted string that can be used to compute the F1 score.\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the model output as it is returned by the model. The processing of the output is done in the function\n",
    "        wrong_keys_to_entity (bool): if True, the function also extracts the dictionaries with keys different from 'entity', converting the keys into 'entity'. If not, all keys that are not 'entity' are dropped\n",
    "\n",
    "        return:\n",
    "        dict: the model response\n",
    "\n",
    "        \"\"\"\n",
    "       \n",
    "        def is_empty_list(string:str)  -> bool:\n",
    "            if string=='[]':\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_lists(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, list) for item in tmp):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_lists_and_dict(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                found_dict = False\n",
    "                found_list = False\n",
    "                for element in tmp:\n",
    "                    if isinstance(element, list):\n",
    "                        found_list = True\n",
    "                    elif isinstance(element, dict):\n",
    "                        found_dict = True\n",
    "                    if found_list and found_dict:\n",
    "                        return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_strings(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, str) for item in tmp):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        def is_list_of_empty_dict(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                #print('TMP: ', tmp)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    if all(str(item) == \"{}\" for item in tmp):\n",
    "                        return True\n",
    "            return False\n",
    "\n",
    "        def is_list_with_one_empty_dict(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list):\n",
    "                    for item in tmp:\n",
    "                        if item == {}:\n",
    "                            return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_with_empty_lists(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        for v in item.values():\n",
    "                            if v == []:\n",
    "                                return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_with_one_key_multiple_values(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        if len(item) == 1 and len(item.values()) > 1:\n",
    "                            return True\n",
    "            return False\n",
    "\n",
    "        def is_list_of_dicts_with_multiple_keys_included_entity(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        if len(item) > 1 and 'entity' in item.keys():\n",
    "                            return True\n",
    "            return False\n",
    "\n",
    "        def is_list_of_dict_numeric_values(string:str)  -> bool:\n",
    "            #print('STRING: ', string)\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                #print('TMP: ', tmp)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        if len(item.values()) > 0:\n",
    "                            val = list(item.values())[0] \n",
    "                            if isinstance(val, int) or isinstance(val, float):\n",
    "                                return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_none_values(string:str) -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        if len(item.values()) > 0:\n",
    "                            val = list(item.values())[0] \n",
    "                            if val is None:\n",
    "                                return True\n",
    "            return False\n",
    "\n",
    "        def is_list_of_dicts_and_strings(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                #print('ASSESSED')\n",
    "                tmp = json.loads(string)\n",
    "                found_dict = False\n",
    "                found_string = False\n",
    "                for element in tmp:\n",
    "                    if isinstance(element, str):\n",
    "                        found_string = True\n",
    "                    elif isinstance(element, dict):\n",
    "                        found_dict = True\n",
    "                    if found_string and found_dict:\n",
    "                        return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_and_lists_of_strings(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                # print('TMP: ', tmp)\n",
    "                if isinstance(tmp, list):\n",
    "                    if all(isinstance(item, dict) for item in tmp):\n",
    "                        return False\n",
    "                    for item in tmp:\n",
    "                        # print('ITEM: ', item)\n",
    "                        if isinstance(item, dict):\n",
    "                            \n",
    "                            if len(item.values()) == 0:\n",
    "                               return False\n",
    "                            if item.get('entity') is None:\n",
    "                                return False\n",
    "                        elif isinstance(item, list):\n",
    "                            if len(item) != 1:\n",
    "                                return False\n",
    "                            if not isinstance(item[0], str):\n",
    "                                return False\n",
    "                        else:\n",
    "                            return False\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_with_value_list(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        for v in item.values():\n",
    "                            if isinstance(v, list):\n",
    "                                return True\n",
    "            return False\n",
    "        \n",
    "        def is_string(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, str):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_strings_representing_dicts(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                # print('TMP: ', tmp)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, str) for item in tmp):\n",
    "                    tmp_list = []\n",
    "                    for item in tmp:\n",
    "                        # print('ITEM: ', item)\n",
    "                        if self._assess_model_output(item):\n",
    "                          tmp_list.append(json.loads(item))\n",
    "                    if all(isinstance(item, dict) for item in tmp_list):\n",
    "                        return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_of_lists(string:str)  -> bool:\n",
    "            # print('STRING: ', string)\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                #Â print('TMP: ', tmp)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        # print('item: ',item)\n",
    "                        tmp2 = list(item.values())[0]\n",
    "                        if len(tmp2) > 0:\n",
    "                            if isinstance(list(item.values())[0], list):\n",
    "                                return True\n",
    "            return False\n",
    "        \n",
    "        def is_numeric(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, (int, float)):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def are_entities_extracted_as_dict_keys_instead_of_values(string:str, example:dict) -> bool:\n",
    "            if is_list_of_dicts(string):\n",
    "                tmp = json.loads(string)\n",
    "                keys = [key for item in tmp for key in item.keys()]\n",
    "                if 'entity' not in keys:\n",
    "                    if all(entity in example['sentence'] for entity in keys):\n",
    "                        return True\n",
    "            return False\n",
    "        \n",
    "        \n",
    "        \n",
    "        def convert_wrong_keys_into_entity(string:str) -> List[str]:\n",
    "            if is_list_of_dicts(string):\n",
    "                tmp = json.loads(string)\n",
    "                tmp = [str({\"entity\":v}) for el in tmp for v in el.values()]\n",
    "                return tmp\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "\n",
    "        def only_dicts_with_key_entity(string:str, wrong_keys_to_entity:bool) -> Tuple[bool, str]:\n",
    "            \"\"\"\n",
    "            Extract only the dictionaries with the key 'entity' in the list of dictionaries in the string\n",
    "            \n",
    "            Args:\n",
    "            string (str): the string to be cleaned\n",
    "            wrong_keys_to_entity (bool): if True, the function also extracts the dictionaries with keys different from 'entity', converting the keys into 'entity'\n",
    "            \"\"\"\n",
    "            els_between_curly = re.findall(r'\\{(.+?)\\}', string)\n",
    "            clean = [el for el in els_between_curly if el.startswith('\"entity\"') or el.startswith(\"'entity'\")]\n",
    "            clean = ['{' + el + '}' for el in clean]\n",
    "            dirty = []\n",
    "            if wrong_keys_to_entity:\n",
    "                dirty = [el for el in els_between_curly if (not el.startswith('\"entity\"')) and (not el.startswith(\"'entity'\"))]\n",
    "                dirty = ['{' + el + '}' for el in dirty]\n",
    "                dirty = '[' + ', '.join(dirty) + ']'\n",
    "                cleaned_dirty = convert_wrong_keys_into_entity(dirty)\n",
    "                out = '[' + ', '.join(clean) + ', '.join(cleaned_dirty) +  ']'\n",
    "            else:\n",
    "                out = '[' + ', '.join(clean) + ']'\n",
    "            # out = out.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\" \\'\", \" \\\"\")\n",
    "            operations_performed = False\n",
    "            if len(clean) != len(els_between_curly):\n",
    "                operations_performed = True\n",
    "            if is_empty_list(out):\n",
    "                return operations_performed, '[{\"entity\":\"\"}]'\n",
    "            return operations_performed, str(out)\n",
    "        \n",
    "        if self.verbose: print('EXAMPLE:  ', example['model_responses'])\n",
    "        model_output = example['model_responses']\n",
    "        if self.verbose: print('ORIGINAL MODEL OUTPUT:', model_output)\n",
    "        # print('ORIGINAL MODEL OUTPUT:', model_output)\n",
    "        if self.verbose: print('GROUND TRUTH: ', example['ground_truth'])\n",
    "        # model_output = self._exceptions_handler(model_output)\n",
    "\n",
    "        if is_list_of_dicts(model_output):\n",
    "            self.counter_dict['perfect_output'] += 1\n",
    "            if self.verbose: print('is_list_of_dicts')\n",
    "            tmp = json.loads(model_output)\n",
    "            return {'model_output':str(tmp)}\n",
    "    \n",
    "        if model_output is None or is_empty_list(model_output):\n",
    "            return {'model_output':'[{\"entity\":\"\"}]'}\n",
    "        \n",
    "        #Â model_output = self._special_cases_handler(model_output)\n",
    "        model_output = self._remove_json_special_chars(model_output)\n",
    "        if self.verbose:print('PULITO: ', model_output)\n",
    "\n",
    "                \n",
    "        if are_entities_extracted_as_dict_keys_instead_of_values(model_output, example):\n",
    "            self.counter_dict['are_entities_extracted_as_dict_keys_instead_of_values'] += 1\n",
    "            if self.verbose: print('ENTITIES EXTRACTED AS DICT KEYS INSTEAD OF VALUES')\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp = [{\"entity\":k} for el in tmp for k in el.keys() ]\n",
    "            tmp = str(tmp)\n",
    "            return {'model_output':tmp}\n",
    "        \n",
    "        if is_list_of_dicts_and_lists_of_strings(model_output):\n",
    "            self.counter_dict['is_list_of_dicts_and_lists_of_strings'] += 1\n",
    "            if self.verbose: print('is_list_of_dicts_and_lists_of_strings')\n",
    "            tmp = json.loads(model_output)\n",
    "            out = []\n",
    "            for item in tmp:\n",
    "                if self.verbose: print('ITEM: ', item)\n",
    "                if isinstance(item, dict):\n",
    "                    out.append(item)\n",
    "                elif isinstance(item, list):\n",
    "                    out.append({\"entity\":item[0]})\n",
    "            return {'model_output':str(out)}\n",
    "\n",
    "        if is_numeric(model_output):\n",
    "            self.counter_dict['is_numeric'] += 1\n",
    "            # print('IS NUMERIC')\n",
    "            return {'model_output':'[{\"entity\":\"\"}]'}\n",
    "\n",
    "        #Â print('QUI HO QUESTO: ', model_output)\n",
    "        if is_list_of_strings_representing_dicts(model_output):\n",
    "            self.counter_dict['is_list_of_strings_representing_dicts'] += 1\n",
    "            if self.verbose: print('is_list_of_strings_representing_dicts 1')                \n",
    "            tmp = json.loads(model_output)\n",
    "            tmp_list = []\n",
    "            for item in tmp:\n",
    "                if self._assess_model_output(item):\n",
    "                  tmp_list.append(json.loads(item))\n",
    "            if self.verbose: print('TEMPOOOO 2 ',tmp)\n",
    "            return {'model_output':str(tmp_list)}\n",
    "        \n",
    "        if is_list_of_dicts_with_one_key_multiple_values(model_output):\n",
    "            self.counter_dict['is_list_of_dicts_with_one_key_multiple_values'] += 1\n",
    "            if self.verbose: print('is_list_of_dicts_with_one_key_multiple_values')\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp = [{\"entity\":v[0]} for el in tmp for v in el.values()]\n",
    "            return {'model_output':str(tmp)}\n",
    "       \n",
    "        if is_list_of_dicts_with_multiple_keys_included_entity(model_output):\n",
    "            self.counter_dict['is_list_of_dicts_with_multiple_keys_included_entity'] += 1\n",
    "            if self.verbose: print('is_list_of_dicts_with_multiple_keys_included_entity')\n",
    "            tmp = json.loads(model_output)\n",
    "            out = []\n",
    "            for item in tmp:\n",
    "                if item.get('entity') is not None:\n",
    "                    out.append({\"entity\":item.get('entity')})\n",
    "            return {'model_output':str(out)}\n",
    "        \n",
    "        \n",
    "        if is_list_of_lists_and_dict(model_output):\n",
    "            self.counter_dict['is_list_of_lists_and_dict'] += 1\n",
    "            if self.verbose: print('is_list_of_lists_and_dict')\n",
    "            tmp = json.loads(model_output)\n",
    "            for el in tmp:\n",
    "                if isinstance(el, list):\n",
    "                    tmp = str(el)\n",
    "                    #Â print('is_list_of_lists_and_dict')\n",
    "                    return {'model_output':tmp}\n",
    "                \n",
    "        if is_list_of_lists(model_output):\n",
    "            self.counter_dict['is_list_of_lists'] += 1\n",
    "            if self.verbose: print('is_list_of_lists')\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp2 = str(tmp[0]).replace(\"'\", \"\\\"\")\n",
    "            if is_list_of_dicts_and_strings(tmp2):\n",
    "                tmp = tmp[0]\n",
    "                out = [item for item in tmp if isinstance(item, dict)]\n",
    "                return {'model_output':str(out)} \n",
    "            tmp = str(tmp[0])\n",
    "            return {'model_output':tmp}\n",
    "        \n",
    "\n",
    "        if is_list_of_strings(model_output):\n",
    "            self.counter_dict['is_list_of_strings'] += 1\n",
    "            if self.verbose: print('is_list_of_strings')\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp = [{\"entity\":el} for el in tmp]\n",
    "            tmp = str(tmp)\n",
    "            #Â print('is_list_of_strings')\n",
    "            if self.verbose: print('TEMPOOOO ',tmp)\n",
    "            return {'model_output': tmp}\n",
    "        \n",
    "        if is_string(model_output):\n",
    "            self.counter_dict['is_string'] += 1\n",
    "            # model_output = model_output.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\" \\'\", \" \\\"\")\n",
    "            if self.verbose: print('PULO: ', model_output)\n",
    "            tmp = json.loads(model_output)\n",
    "            if all(el in tmp for el in ['{', 'entity', '}']):\n",
    "                return {'model_output':tmp}\n",
    "            tmp = [{\"entity\":tmp}]\n",
    "            tmp = str(tmp)\n",
    "            #print('is_string')\n",
    "            return {'model_output':tmp}\n",
    "\n",
    "        \n",
    "        if latest_version:\n",
    "            model_output = self._extract_text_between_curl_brackets(model_output)\n",
    "            model_output = self._clean_text_between_curl_brackets(model_output)\n",
    "\n",
    "            # print('QUI HO il SECONDO QUESTO: ', model_output)\n",
    "\n",
    "            if is_list_of_strings_representing_dicts(model_output):\n",
    "                self.counter_dict['is_list_of_strings_representing_dicts'] += 1\n",
    "                if self.verbose: print('is_list_of_strings_representing_dicts 2')                \n",
    "                tmp = json.loads(model_output)\n",
    "                tmp_list = []\n",
    "                for item in tmp:\n",
    "                    if self._assess_model_output(item):\n",
    "                        tmp_list.append(json.loads(item))\n",
    "                return {'model_output':str(tmp_list)}\n",
    "            \n",
    "            if is_list_of_dicts_with_one_key_multiple_values(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_with_one_key_multiple_values'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_with_one_key_multiple_values')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [{\"entity\":v[0]} for el in tmp for v in el.values()]\n",
    "                return {'model_output':str(tmp)}\n",
    "            \n",
    "            if is_list_of_dicts_and_lists_of_strings(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_and_lists_of_strings'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_and_lists_of_strings')\n",
    "                tmp = json.loads(model_output)\n",
    "                out = []\n",
    "                for item in tmp:\n",
    "                    #Â print('ITEM: ', item)\n",
    "                    if isinstance(item, dict):\n",
    "                        out.append(item)\n",
    "                    elif isinstance(item, list):\n",
    "                        out.append({\"entity\":item[0]})\n",
    "                return {'model_output':str(out)}\n",
    "            \n",
    "            if self.verbose: print('QUI HO il TEERZO QUESTO: ', model_output)\n",
    "\n",
    "            if is_list_of_dicts_with_empty_lists(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_with_empty_lists'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_with_empty_lists')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [{\"entity\":v} for el in tmp for v in el.values() if v != []]\n",
    "                #Â print('TMP: ', tmp)\n",
    "                if is_list_of_dicts_with_value_list(str(tmp).replace(\"'\", \"\\\"\")):\n",
    "                    if self.verbose: print('is_list_of_dicts_with_value_list')\n",
    "                    tmp = [{\"entity\":v} for el in tmp for v in el.values() if not isinstance(v, list)]\n",
    "                    tmp2 = [{\"entity\":v[0]} for el in tmp for v in el.values() if isinstance(v, list)]\n",
    "                    #Â print('returning this: ', {'model_output ':str(tmp2)}  )\n",
    "                    return {'model_output':str(tmp2)}\n",
    "                #Â print('returning this: ', {'model_output ':str(tmp)}  )\n",
    "\n",
    "                return {'model_output':str(tmp)}\n",
    "            \n",
    "            if self.verbose: print('QUI HO il QUARTO QUESTO:', model_output)\n",
    "\n",
    "            if is_list_of_dicts_with_value_list(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_with_value_list'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_with_value_list')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [{\"entity\":v} for el in tmp for v in el.values() if not isinstance(v, list)]\n",
    "                tmp2 = [{\"entity\":v[0]} for el in tmp for v in el.values() if isinstance(v, list)]\n",
    "                return {'model_output':str(tmp)}\n",
    "\n",
    "            if is_list_of_dict_numeric_values(model_output):\n",
    "                self.counter_dict['is_list_of_dict_numeric_values'] += 1\n",
    "                if self.verbose: print('is_list_of_dict_int_values')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [str({\"entity\":str(v)}) for el in tmp for v in el.values()]\n",
    "                model_output = str(tmp)\n",
    "            \n",
    "            if is_list_of_dicts_none_values(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_none_values'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_none_values')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [str({\"entity\":v}) for el in tmp for v in el.values() if v is not None]\n",
    "                model_output = str(tmp)\n",
    "                    \n",
    "            if is_list_of_empty_dict(model_output):\n",
    "                self.counter_dict['is_list_of_empty_dict'] += 1\n",
    "                if self.verbose: print('is_list_of_empty_dict')\n",
    "                return {'model_output':'[{\"entity\":\"\"}]'}\n",
    "            \n",
    "            if is_list_with_one_empty_dict(model_output):\n",
    "                self.counter_dict['is_list_with_one_empty_dict'] += 1\n",
    "                if self.verbose: print('is_list_with_one_empty_dict')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [el for el in tmp if el != {}]\n",
    "                model_output = tmp\n",
    "                return {'model_output':str(model_output)}\n",
    "            \n",
    "            if is_list_of_dicts_of_lists(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_of_lists'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_of_lists')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [{\"entity\":v} for el in tmp for v in el.values() if not isinstance(v, list)]\n",
    "                # tmp.extend([{\"entity\":el.values()[0]} for el in tmp if isinstance(el.values(), list)])\n",
    "                # print('returning this: ', {'model_output ':str(tmp)}  )\n",
    "                return {'model_output':str(tmp)}  \n",
    "                \n",
    "            if self.verbose: print('CLEANED: ', model_output)\n",
    "            cleaning_done, cleaned_model_output = only_dicts_with_key_entity(model_output, wrong_keys_to_entity=wrong_keys_to_entity)\n",
    "            if cleaning_done:\n",
    "                model_output = cleaned_model_output\n",
    "            \n",
    "            if is_list_of_dicts(model_output):\n",
    "                self.counter_dict['is_list_of_dicts'] += 1\n",
    "                if self.verbose: print('PRE  CLEANED: ', model_output)\n",
    "                if is_list_of_dicts_with_multiple_keys_included_entity(model_output):\n",
    "                    self.counter_dict['is_list_of_dicts_with_multiple_keys_included_entity'] += 1\n",
    "                    if self.verbose: print('is_list_of_dicts_with_multiple_keys_included_entity')\n",
    "                    tmp = json.loads(model_output)\n",
    "                    out = []\n",
    "                    for item in tmp:\n",
    "                        if len(item) > 1 and 'entity' in item.keys():\n",
    "                            out.append({\"entity\":item.get('entity')})\n",
    "                    return {'model_output':str(out)}\n",
    "                tmp = json.loads(model_output)\n",
    "                return {'model_output':str(tmp)}\n",
    "            \n",
    "            else: \n",
    "                self.counter_dict['uknown'] += 1\n",
    "                # print('NOT CLEANED: ', model_output, '\\n\\n')\n",
    "                return {'model_output':'[{\"entity\":\"\"}]'}\n",
    "        \n",
    "            \n",
    "    def _exceptions_handler(self, model_output: str, error) -> str:\n",
    "        # if hasattr(error, 'msg'):\n",
    "        #     if error.msg.startswith('Expecting property name enclosed in double quotes'):\n",
    "        #         model_output = model_output.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\": \\'\", \": \\\"\")\n",
    "        \n",
    "        try:\n",
    "            json.loads(model_output)\n",
    "        except Exception as error:\n",
    "            if isinstance(error, json.decoder.JSONDecodeError):\n",
    "                #if error.msg == \"Expecting ',' delimiter\":\n",
    "                key_part, value_part = model_output.split(': ', 1)\n",
    "                first_occurrence = value_part.find('\"')\n",
    "                last_occurrence = value_part.rfind('\"')\n",
    "                model_output = key_part + ': \"' + value_part[first_occurrence+1:last_occurrence].replace(\"'\", r'\\'') + '\"' + '}'\n",
    "        return model_output\n",
    "    # .replace(\"\\'\", \" \")\n",
    "    \n",
    "    def _substitute_apexes(self, model_output: str) -> str:\n",
    "        model_output = model_output.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\": \\'\", \": \\\"\")\n",
    "        return model_output\n",
    "    \n",
    "    \n",
    "    def _extract_text_between_curl_brackets(self, model_output: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the text between the curl brackets of the model output, as enities are usually outputted in this format: {\"entity\": \"value\"}\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the example from the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        text_between_curl_brackets = re.findall(r'\\{(.+?)\\}', model_output)\n",
    "        cleaned_output = ['{'+ el +'}' for el in text_between_curl_brackets]\n",
    "        cleaned_output = '[' + ', '.join(cleaned_output) + ']'\n",
    "        return cleaned_output\n",
    "    \n",
    "\n",
    "    def _clean_text_between_curl_brackets(self, text_between_curl_brackets: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean the text between the curl brackets of the model output, as entities are usually outputted in this format: {\"key\": \"value\"}\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the example from the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        text_between_curl_brackets = re.sub(r'\",(.+?)}', r'\"}', text_between_curl_brackets)\n",
    "        text_between_curl_brackets = re.sub(r'{},', r'', text_between_curl_brackets)\n",
    "        text_between_curl_brackets = re.sub(r',{}', r'', text_between_curl_brackets)\n",
    "        # print('CLEANED: ', text_between_curl_brackets)\n",
    "        # text_between_curl_brackets = re.sub(r'\\{\"entity\":\\[\\]\\},', r'', text_between_curl_brackets)\n",
    "        # text_between_curl_brackets = re.sub(r',{\\'entity\\':[]}', r'', text_between_curl_brackets)\n",
    "        return text_between_curl_brackets\n",
    "    \n",
    "    def apply_cleaning(self, data, wrong_keys_to_entity) -> None:\n",
    "        \"\"\"\n",
    "        Apply the cleaning to the model output and return the cleaned response in a new cloumn called 'model_output\n",
    "\n",
    "        Args:\n",
    "        data (list): the dataset containing the model output\n",
    "        wrong_keys_to_entity (bool): if True, the function also extracts the dictionaries with keys different from 'entity', converting the keys into 'entity'. If not, all keys that are not 'entity' are dropped\n",
    "        \"\"\"\n",
    "        data = data.filter(lambda example: example[\"entities\"] is not None)\n",
    "        data = data.map(lambda x: self._clean_ground_truth(x), remove_columns=['ground_truth'])\n",
    "        data = data.map(lambda x: self._clean_model_output(x, wrong_keys_to_entity)) \n",
    "        self.data = data\n",
    "        return data\n",
    "    \n",
    "    def get_examples_based_on_metric(self, metric, upper_threshold=1, lower_threshold=0):\n",
    "        \"\"\"\n",
    "        Select the examples based on the metric and the threshold.\n",
    "        Args:\n",
    "        metric (str): the metric to consider\n",
    "        threshold (float): the threshold to consider\n",
    "        return:\n",
    "        list: the list of examples that satisfy the condition\n",
    "        \"\"\"\n",
    "        out = [example for example in self.data if example[metric] <= upper_threshold and example[metric] >= lower_threshold]\n",
    "        return(Dataset.from_pandas(pd.DataFrame(out)))\n",
    "\n",
    "    def create_allucinations_columns(self, data, verbose:bool=False):\n",
    "        \"\"\"\n",
    "        Create two columns that contain the invented entities/hallucinations, which are spans of text that have been extracted but are not in the sentence.\n",
    "        The first column ('light_allucinations') contains the invented entities that are similar to at least one entity in the sentence, while the second column ('heavy_allucinations') contains the invented entities that are not similar to the entities in the sentence.\n",
    "       \n",
    "        Args:\n",
    "        data (list): the dataset containing the model output\n",
    "        verbose (bool): default False. If True, print verbose\n",
    "        \"\"\"\n",
    "        light_allucinations, heavy_allucinations = [], []\n",
    "        for el in data:\n",
    "            light_invented_entities = []\n",
    "            heavy_invented_entities = []\n",
    "            for extracted_entity in el['model_output_parsed']['entities']:\n",
    "                if extracted_entity not in el['sentence']:\n",
    "                    if verbose: print(f\"'{extracted_entity}' not in sentence...\")\n",
    "                    if len(extracted_entity.split())==1:\n",
    "                        possible_entities = el['sentence'].split()\n",
    "                    else:\n",
    "                        n_words_in_entity= len(extracted_entity.split())\n",
    "                        possible_entities = [' '.join(el['sentence'].split()[i:i+n_words_in_entity]) for i in range(len(el['sentence'].split())-(n_words_in_entity-1))]\n",
    "                    if verbose: print(f'looking through {possible_entities}...')\n",
    "                    similarities = [fuzz.ratio(extracted_entity, possible_similar_entity) > 80\n",
    "                                    for possible_similar_entity in possible_entities]\n",
    "                    if any(similarities):\n",
    "                        if verbose: print('SIMILARITY FOUND', extracted_entity, '||||', el['sentence'].split()[similarities.index(True):similarities.index(True)+len(extracted_entity.split())])\n",
    "                        light_invented_entities.append({'extracted_entity':extracted_entity, 'original_entity':el['sentence'].split()[similarities.index(True)], 'original_sentence':el['sentence']})  \n",
    "                    else:\n",
    "                        heavy_invented_entities.append({'extracted_entity':extracted_entity, 'original_sentence':el['sentence']})\n",
    "                    if verbose: print('\\n')\n",
    "\n",
    "            light_allucinations.append(light_invented_entities)\n",
    "            heavy_allucinations.append(heavy_invented_entities)\n",
    "        data = data.add_column('light_allucinations', light_allucinations)\n",
    "        data = data.add_column('heavy_allucinations', heavy_allucinations)\n",
    "        return data\n",
    "\n",
    "    def remove_allucinations_from_computation(self, data_with_allucination_col):\n",
    "        \"\"\"\n",
    "        Remove the invented entities/hallucinations, which are spans of text that have been extracted but are not in the sentence.\n",
    "        This function creates a new column called 'model_output_parsed' that contains the model output without the invented entities.\"\"\"\n",
    "        \n",
    "        def helper(example):\n",
    "            if example['heavy_allucinations'] == []:\n",
    "                return example\n",
    "            else:\n",
    "                for el in example['heavy_allucinations']:\n",
    "                    print('REMOVING: ', el['extracted_entity'])\n",
    "                    example['model_output_parsed']['entities'].remove(el['extracted_entity'])\n",
    "                return example\n",
    "        data_with_allucination_col = data_with_allucination_col.map(helper)\n",
    "        return data_with_allucination_col\n",
    "    \n",
    "    def count_repetitions_in_extraction(self, data):\n",
    "        \"\"\"\n",
    "        Count the number of times an entity is extracted multiple times in the model output.\n",
    "        Args:\n",
    "        data (list): the dataset containing the model output. It must be parsed already.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for el in data:\n",
    "            entities = [entity for entity in el['model_output_parsed']['entities']]\n",
    "            if len(entities) != len(set(entities)):\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'entities', 'original_text', 'original_id', 'prompt', 'inference_prompt', 'ground_truth', 'model_responses', 'model_output'],\n",
      "    num_rows: 681\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation':      TP  FP  FN\n",
       " 0     5   0   9\n",
       " 1     4   0   0\n",
       " 2     9   0   5\n",
       " 3     5   0   1\n",
       " 4     2   4   0\n",
       " ..   ..  ..  ..\n",
       " 676   4   1   0\n",
       " 677   4   0   1\n",
       " 678   5   1   0\n",
       " 679  12   4   1\n",
       " 680   4   1   4\n",
       " \n",
       " [681 rows x 3 columns],\n",
       " 'precision': 0.6738070616043729,\n",
       " 'recall': 0.7008297480024586,\n",
       " 'f1': 0.6870527980718536}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from utils.evaluator import Evaluator\n",
    "from utils.output_cleaner import OutputCleaner\n",
    "file =  data_path\n",
    "eval_data = Dataset.from_csv(file) \n",
    "\n",
    "output_cleaner = OutputCleaner(verbose=False)\n",
    "similar_is_equal = True\n",
    "similar_is_equal_threshold = 100\n",
    "cleaned_data = output_cleaner.apply_cleaning(eval_data, wrong_keys_to_entity=False) #.select(range(12,13))\n",
    "\n",
    "evaluator = Evaluator(data=cleaned_data, offset=False, output_cleaner=cleaned_data)\n",
    "print(evaluator.data)\n",
    "evaluator.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, similarity_types=['case', 'subset', 'superset'])\n",
    "tmp = evaluator.add_TP_FP_TN_FN_to_data()\n",
    "\n",
    "tmp = tmp.map(lambda x: {'model_output_parsed':evaluator._parse_json(x['model_output'])})\n",
    "tmp = tmp.map(lambda x: {'ground_truth_parsed':evaluator._parse_json(x['ground_truth'])})\n",
    "evaluator.evaluation_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vediamo quante volte succede che il modello inventa entitÃ  che non son onel testo originario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 166 invented entities over 4589 extracted entities -> 3.6%\n"
     ]
    }
   ],
   "source": [
    "output_analist = OutputAnalist(tmp)\n",
    "\n",
    "allucinations = []\n",
    "for el in output_analist.data:\n",
    "    invented_entities = []\n",
    "    for extracted_entity in el['model_output_parsed']['entities']:\n",
    "        if extracted_entity not in el['sentence']:\n",
    "            #print('EXTRACTED ENTITY NOT IN sentence: ', extracted_entity, '||||', el['sentence'])\n",
    "            invented_entities.append(extracted_entity)\n",
    "    allucinations.append(invented_entities)\n",
    "len1 = len([el for sublist in allucinations for el in sublist])\n",
    "len2 = len([el for sublist in output_analist.data['model_output_parsed'] for el in sublist['entities']])\n",
    "print(f\"There are {len1} invented entities over {len2} extracted entities -> {round(len1/len2*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quante di queste allucinazioni sono perÃ² molto simili a qualcosa che c'Ã¨ nel testo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 90 heavy allucinations over 166 allucinations -> 54.2%\n",
      "71 sentences are impacted by allucination out of 681 -> 13.2%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp1 = output_analist.create_allucinations_columns(tmp, verbose = False)\n",
    "\n",
    "len1 = len([el for sublist in allucinations for el in sublist])\n",
    "len2 = len([el for sublist in tmp1['heavy_allucinations'] for el in sublist])\n",
    "print(f\"There are {len2} heavy allucinations over {len1} allucinations -> {round(len2/len1*100,1)}%\")\n",
    "print(f\"{len([sublist for sublist in tmp1['heavy_allucinations'] if len(sublist)>0])} sentences are impacted by allucination out of 681 -> {round(len2/681*100,1)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### quando il modello Ã¨ allucinato le performances peggiorano? Non solo quello Ã¨ sbagliato, ma magari anche le altre fanno casino..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO allucinations -> f1:0.695427538103849 recall: 0.7049581005586593, precision 0.6861512319456244\n",
      "   allucinations -> f1:0.6286057692307692 recall: 0.6705128205128205, precision 0.5916289592760181\n"
     ]
    }
   ],
   "source": [
    "data_allucinated = tmp1.filter(lambda x: len(x['heavy_allucinations'])>0)\n",
    "evaluator_allucinations = Evaluator(data=data_allucinated, offset=False, output_cleaner=output_cleaner)\n",
    "evaluator_allucinations.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, similarity_types=['case', 'subset', 'superset'])\n",
    "evaluator_NO_allucinations = Evaluator(data=tmp1.filter(lambda x: len(x['heavy_allucinations'])==0), offset=False, output_cleaner=output_cleaner)\n",
    "evaluator_NO_allucinations.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, similarity_types=['case', 'subset', 'superset'])\n",
    "print(f\"NO allucinations -> f1:{evaluator_NO_allucinations.evaluation_table['f1']} recall: {evaluator_NO_allucinations.evaluation_table['recall']}, precision {evaluator_NO_allucinations.evaluation_table['precision']}\")\n",
    "print(f\"   allucinations -> f1:{evaluator_allucinations.evaluation_table['f1']} recall: {evaluator_allucinations.evaluation_table['recall']}, precision {evaluator_allucinations.evaluation_table['precision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cerchiamo di capire quale sia essere l'impatto delle allucinazioni, cioÃ¨ cosa succede se le togliamo dal computo e laciamo il resto invariato. Per esempio, se le estratte sono 'Pietro' 'ferrazzi' e la frase originale Ã¨ 'Pietro sta programmando', normalmente conteggio 'ferrazzi' come FP. Qui voglio vedere se escludendolo dal conteggio le performance sono comunque peggiori. In altre parole, voglio vedere se un'allucinazione ha l'effetto di modifcare anche quello che succede intorno ad essa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo Ã¨ il confronto tra avere allucinazioni e dopo averle tolte considerando soltanto le frasi per cui sono state generate allucinazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allucinations removed -> f1:0.6591661151555261 recall: 0.6604774535809018, precision 0.6578599735799208\n",
      "allucinations         -> f1:0.6286057692307692 recall: 0.6705128205128205, precision 0.5916289592760181\n"
     ]
    }
   ],
   "source": [
    "output_analist = OutputAnalist(tmp)\n",
    "data_allucinated_removed = output_analist.remove_allucinations_from_computation(data_allucinated)\n",
    "\n",
    "evaluator_marginal_allucinations_removed = Evaluator(data_allucinated_removed, offset=False, output_cleaner=None)\n",
    "evaluator_marginal_allucinations_removed.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, similarity_types=['case', 'subset', 'superset'], already_parsed_inputs=True)\n",
    "print(f\"allucinations removed -> f1:{evaluator_marginal_allucinations_removed.evaluation_table['f1']} recall: {evaluator_marginal_allucinations_removed.evaluation_table['recall']}, precision {evaluator_marginal_allucinations_removed.evaluation_table['precision']}\")\n",
    "\n",
    "evaluator_marginal_allucinations = Evaluator(data_allucinated, offset=False, output_cleaner=None)\n",
    "evaluator_marginal_allucinations.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, similarity_types=['case', 'subset', 'superset'], already_parsed_inputs=False)\n",
    "print(f\"allucinations         -> f1:{evaluator_marginal_allucinations.evaluation_table['f1']} recall: {evaluator_marginal_allucinations.evaluation_table['recall']}, precision {evaluator_marginal_allucinations.evaluation_table['precision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sembra che le allucinazioni rendano le performances leggermente peggiori, anche una volta che le marginallizzi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "un allucinazione Ã¨ un FP. \n",
    "H0: p( TN | allucinazione ) < p( TN | ! allucinazione)  [cioÃ¨, il fatto che ci siano delle allucinazioni Ã¨ correlato alla miglior comprensione del contesto da parte del modello, che 'esagera' a generare positivi, ma non sbaglia piÃ¹ i negativi]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "da qui si vede che in realtÃ  non cambia molto -> le allucinazioni non sembrano impattare in maniera importante il resto del testo generato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "summary_all_removed = evaluator_marginal_allucinations_removed.evaluation_table['evaluation'].sum() / evaluator_marginal_allucinations_removed.data.num_rows\n",
    "summary_NO_all = evaluator_NO_allucinations.evaluation_table['evaluation'].sum() / evaluator_NO_allucinations.data.num_rows\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "labels = ['TP', 'FP', 'FN']\n",
    "width = 0.35\n",
    "x = range(len(labels))\n",
    "rects1 = ax.bar(x, summary_all_removed, width, label='allucinations removed')\n",
    "rects2 = ax.bar([i + width for i in x], summary_NO_all, width, label='NO all')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('TP, FP, FN (per sentence) for allucinated and non allucinated sentences')\n",
    "ax.set_xticks([i + width/2 for i in x])\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia.',\n",
       " 'entities': \"[{'id': '1614', 'offsets': array([23, 35]), 'role': '', 'semantic_type_id': '', 'text': 'hypertension', 'type': 'EVENT'}\\n {'id': '1629', 'offsets': array([40, 52]), 'role': '', 'semantic_type_id': '', 'text': 'dyslipidemia', 'type': 'EVENT'}\\n {'id': '1644', 'offsets': array([53, 62]), 'role': '', 'semantic_type_id': '', 'text': 'diagnosed', 'type': 'EVENT'}\\n {'id': '1659', 'offsets': array([110, 118]), 'role': '', 'semantic_type_id': '', 'text': 'mellitus', 'type': 'EVENT'}\\n {'id': '1674', 'offsets': array([149, 157]), 'role': '', 'semantic_type_id': '', 'text': 'referred', 'type': 'EVENT'}\\n {'id': '1689', 'offsets': array([186, 197]), 'role': '', 'semantic_type_id': '', 'text': 'hypokalemia', 'type': 'EVENT'}\\n {'id': '1996', 'offsets': array([ 91, 118]), 'role': '', 'semantic_type_id': 'C0743128', 'text': 'new-onset diabetes mellitus', 'type': 'CLINENTITY'}\\n {'id': '2076', 'offsets': array([ 0, 17]), 'role': 'PATIENT', 'semantic_type_id': '', 'text': 'A 46-year-old man', 'type': 'ACTOR'}\\n {'id': '2090', 'offsets': array([63, 71]), 'role': '', 'semantic_type_id': '', 'text': '4-months', 'type': 'TIMEX3'}\\n {'id': '2099', 'offsets': array([128, 135]), 'role': '', 'semantic_type_id': '', 'text': '1-month', 'type': 'TIMEX3'}]\",\n",
       " 'original_text': 'A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia. Hormonal study and dynamic biochemical tests performed indicated ECS. Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases. Somatostatin receptor scintigraphy has shown an increased uptake in the parotid gland and mild expression in liver metastasis. The patient underwent right parotidectomy, and histopathologic examination confirmed ACC. Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide. Despite chemotherapy onset, a rapid disease progression and clinical course deterioration was observed.\\r\\n',\n",
       " 'original_id': 'EN101783',\n",
       " 'prompt': '<s>[INST] Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. <<A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia.>>> [/INST][{\"entity\": \"hypertension\"}, {\"entity\": \"dyslipidemia\"}, {\"entity\": \"diagnosed\"}, {\"entity\": \"mellitus\"}, {\"entity\": \"referred\"}, {\"entity\": \"hypokalemia\"}, {\"entity\": \"new-onset diabetes mellitus\"}, {\"entity\": \"A 46-year-old man\"}, {\"entity\": \"4-months\"}, {\"entity\": \"1-month\"}] </s>',\n",
       " 'inference_prompt': '<s>[INST] Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}]. <<A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia.>>> [/INST]',\n",
       " 'ground_truth': '[{\"entity\": \"hypertension\"}, {\"entity\": \"dyslipidemia\"}, {\"entity\": \"diagnosed\"}, {\"entity\": \"mellitus\"}, {\"entity\": \"referred\"}, {\"entity\": \"hypokalemia\"}, {\"entity\": \"new-onset diabetes mellitus\"}, {\"entity\": \"A 46-year-old man\"}, {\"entity\": \"4-months\"}, {\"entity\": \"1-month\"}] ',\n",
       " 'model_responses': ' [{\"entity\": \"hypertension\"}, {\"entity\": \"dyslipidemia\"}, {\"entity\": \"diagnosed\"}, {\"entity\": \"hypokalemia\"}, {\"entity\": \"hypokalemia\"}, {\"entity\": \"hypokalemia\"}, {\"entity\": \"1-month\"}]  [{\"entity\": \"hypokalemia\"}, {\"entity\": \"hypokalemia\"}, {\"entity\": \"hypokalemia\"}, {\"entity\": \"A 46-year-old',\n",
       " 'model_output': \"[{'entity': 'hypertension'}, {'entity': 'dyslipidemia'}, {'entity': 'diagnosed'}, {'entity': 'hypokalemia'}, {'entity': 'hypokalemia'}, {'entity': 'hypokalemia'}, {'entity': '1-month'}, {'entity': 'hypokalemia'}, {'entity': 'hypokalemia'}, {'entity': 'hypokalemia'}]\",\n",
       " 'TP': 5,\n",
       " 'FP': 0,\n",
       " 'FN': 9,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.35714285714285715,\n",
       " 'f1': 0.5263157894736842,\n",
       " 'model_output_parsed': {'entities': ['hypertension',\n",
       "   'dyslipidemia',\n",
       "   'diagnosed',\n",
       "   'hypokalemia',\n",
       "   '1-month']},\n",
       " 'ground_truth_parsed': {'entities': ['hypertension',\n",
       "   'dyslipidemia',\n",
       "   'diagnosed',\n",
       "   'mellitus',\n",
       "   'referred',\n",
       "   'hypokalemia',\n",
       "   'new-onset diabetes mellitus',\n",
       "   'A 46-year-old man',\n",
       "   '4-months',\n",
       "   '1-month']},\n",
       " 'light_allucinations': [],\n",
       " 'heavy_allucinations': []}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
