{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is the best model for Llama 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietroferrazzi/Desktop/dottorato/mistral_finetuning/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/pietroferrazzi/Desktop/dottorato/mistral_finetuning/.venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset        \n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "class Evaluator():\n",
    "\n",
    "    def __init__(self,  data: Dataset, offset:bool, output_cleaner) -> None:\n",
    "        self.offset = offset\n",
    "        self.data = data\n",
    "        self.cleaner = output_cleaner\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _drop_duplicates(self, model_response: list) -> str:\n",
    "        \"\"\"\n",
    "        Drop the duplicates from a list. This is useful when the model output contains the same entity multiple times.\n",
    "\n",
    "        Args:\n",
    "        model_response (str): the model response with no duplicates\n",
    "        \"\"\"\n",
    "        # print('DROPPING DUPLICATES: ', model_response)\n",
    "        try :\n",
    "            return list({v['entity']:v for v in model_response}.values())\n",
    "        except Exception as error:\n",
    "            model_response = self._remove_space_from_dict_keys(model_response)\n",
    "            # print('ERROR: ', model_response)\n",
    "            return list({v['entity']:v for v in model_response}.values())\n",
    "        \n",
    "\n",
    "    def _change_apexes(self, model_output: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the text between the curl brackets of the model output, and change the apexes from double single \\'.\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the example from the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        text_between_curl_brackets = re.findall(r'\\{(.+?)\\}', model_output)\n",
    "        cleaned_output = '['\n",
    "        for el in text_between_curl_brackets:\n",
    "            key_part, value_part = el.split(': ', 1)\n",
    "            first_occurrence = value_part.find('\"')\n",
    "            last_occurrence = value_part.rfind('\"')\n",
    "            tmp = '{' + key_part + ': \"' + value_part[first_occurrence+1:last_occurrence].replace('\"', \"'\") + '\"' + '}'\n",
    "            cleaned_output += tmp + ', '\n",
    "        cleaned_output = cleaned_output[:-2] + ']'\n",
    "        return cleaned_output\n",
    "    \n",
    "    def _assess_model_output(self, model_response: str) -> (bool, str):\n",
    "        \"\"\"\n",
    "        Check if the model output is in the right format. If not, return False.\n",
    "        \n",
    "        Args:\n",
    "        model_output (str): the postprocessed model output after beeing passed to _postprocess_model_output()\n",
    "\n",
    "        return:\n",
    "        bool: True if the format is correct, False otherwise\n",
    "        str: the model output in the adjusted format\n",
    "        \"\"\"\n",
    "        good_format = True\n",
    "        # if self.cleaner.verbose: print('prima sostituz:  ', model_response)\n",
    "        model_response = model_response.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\": \\'\", \": \\\"\")\n",
    "        model_response = re.sub(r'(?<=[a-zA-Z])\"(?=[a-zA-Z])', \"'\", model_response)\n",
    "        # if self.cleaner.verbose: print('dopo sostituz: ', model_response)\n",
    "        try :\n",
    "            out = json.loads(model_response)\n",
    "            if isinstance(out, dict):\n",
    "                model_response = '[' + model_response + ']'\n",
    "        except Exception as error:\n",
    "            if hasattr(error, 'msg'):\n",
    "                if error.msg.startswith('Expecting property name enclosed in double quotes'):\n",
    "                    model_response = model_response.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\": \\'\", \": \\\"\")\n",
    "                    try:\n",
    "                        out = json.loads(model_response)\n",
    "                        if isinstance(out, dict):\n",
    "                            model_response = '[' + model_response + ']'\n",
    "                            good_format = True\n",
    "                    except Exception as error2:\n",
    "                        if isinstance(error2, json.decoder.JSONDecodeError):\n",
    "                            if error2.msg == \"Expecting ',' delimiter\":\n",
    "                                model_response = self._change_apexes(model_response)\n",
    "                                good_format = True\n",
    "            else:\n",
    "                #print('MODEL RESPNSE: ', model_response)\n",
    "                good_format = False\n",
    "        if not good_format:\n",
    "            model_response = re.findall(r'\\[\\{(.+?)\\}\\]', model_response)\n",
    "            if len(model_response) != 0:\n",
    "                model_response = '[{' + model_response[0] + '}]'\n",
    "                good_format = True\n",
    "                try :\n",
    "                    json.loads(model_response)\n",
    "                except Exception as error:\n",
    "                    good_format = False\n",
    "            else:\n",
    "                good_format = False\n",
    "        return good_format, model_response\n",
    "\n",
    "    def _parse_json(self, model_response: str, drop_duplicates: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Parse the model output to extract the entities and their offsets if present.\n",
    "        \n",
    "        Args:\n",
    "        model_response (str): the model response \n",
    "        drop_duplicates (bool): if True, drop the duplicates in the model response\n",
    "        \"\"\"\n",
    "        # print('MODEL RESPONSE 1: ', model_response)\n",
    "        model_response = model_response.replace(\"\\n\", \" \")\n",
    "\n",
    "        good_format, model_response = self._assess_model_output(model_response)\n",
    "\n",
    "        # print('MODEL RESPONSE 2: ', model_response)        \n",
    "        # print('MODEL RESPONSE 3: ', model_response)\n",
    "        if model_response == []:\n",
    "            model_response = '[{\"entity\":\"\"}]'\n",
    "        if self.offset and good_format:\n",
    "            output = json.loads(model_response)\n",
    "            if drop_duplicates:\n",
    "                output = self._drop_duplicates(output)\n",
    "            entities = [entity[\"entity\"] for entity in output]\n",
    "            offsets = [entity[\"offset\"] for entity in output]\n",
    "            return {\"entities\": entities, \"offsets\": offsets}\n",
    "        if (not self.offset) and good_format:\n",
    "\n",
    "            # print('MODEL RESPONSE 4: ', model_response)\n",
    "            # print('ORA STO PARSANDO: ', model_response)\n",
    "            output = json.loads(model_response)\n",
    "            # print('OUTPUT: ', type(output))\n",
    "            if drop_duplicates:\n",
    "                output = self._drop_duplicates(output)\n",
    "            entities = [entity[\"entity\"] for entity in output]\n",
    "            # print('ENTITIES: ', entities)\n",
    "            return {\"entities\": entities}\n",
    "        if not good_format:\n",
    "            return {\"entities\": []}\n",
    "    \n",
    "    def _count_common_words(self, string1: str, string2: str) -> int:\n",
    "        \"\"\"\n",
    "        Count the number of common words between two entities without considering repetition.\n",
    "\n",
    "        Args:\n",
    "        string1 (str): an entity in the model response\n",
    "        string2 (str): an entity in the ground truth\n",
    "\n",
    "        return:\n",
    "        int: the number of common words\n",
    "        \"\"\"\n",
    "        model_words = set(string1.lower().split())\n",
    "        ground_truth_words = set(string2.lower().split())\n",
    "        common_words = model_words.intersection(ground_truth_words)\n",
    "        return len(common_words)\n",
    "        \n",
    "    def _entity_similar_to_ground_truth_entity_LowerUppercase(self, entity_in_model_response: str, entity_in_ground_truth: str) -> (bool, str):\n",
    "        \"\"\"\n",
    "        Check if two entities are similar, i.e. if the difference is just a fact of being upper or lower case.\n",
    "\n",
    "        Args:\n",
    "        entity_in_model_response (str): an entity in the model response\n",
    "        entity_in_ground_truth (str): an entity in the ground truth\n",
    "        threshold (int): the threshold to consider the entities similar. The default value is 80. 0 is completely different, 100 is the same.\n",
    "\n",
    "        return:\n",
    "        bool: True if the entities are similar, False otherwise\n",
    "        str: the entity in the ground truth if the entities are similar, the entity in the model response otherwise\n",
    "\n",
    "        \"\"\"\n",
    "        FP_words = 0\n",
    "        FN_words = 0\n",
    "        TP_words = 0\n",
    "        if entity_in_model_response.lower() == entity_in_ground_truth.lower():\n",
    "            # print('SIMILI CASE: ', entity_in_model_response, ' e ', entity_in_ground_truth)\n",
    "            TP_words = len(entity_in_ground_truth.split())\n",
    "            return True, entity_in_ground_truth, FP_words, FN_words, TP_words\n",
    "        return False, entity_in_model_response, FP_words, FN_words, TP_words\n",
    "    \n",
    "    def _entity_similar_to_ground_truth_entity_StopWords(self, entity_in_model_response: str, entity_in_ground_truth: str) -> (bool, str):\n",
    "        \"\"\"\n",
    "        Check if two entities are similar, i.e. if the difference is just a stop words (e.g., \"the\" or \"a\"). Everything is performend in lower case.\n",
    "        This is useful when the model output is not exactly the same as the ground truth.\n",
    "\n",
    "        Args:\n",
    "        entity_in_model_response (str): an entity in the model response\n",
    "        entity_in_ground_truth (str): an entity in the ground truth\n",
    "        threshold (int): the threshold to consider the entities similar. The default value is 80. 0 is completely different, 100 is the same.\n",
    "\n",
    "        return:\n",
    "        bool: True if the entities are similar, False otherwise\n",
    "        str: the entity in the ground truth if the entities are similar, the entity in the model response otherwise\n",
    "        \"\"\"\n",
    "        def __preprocess_string__(string):\n",
    "            # Remove common articles and noise words\n",
    "            noise_words = [\"a\", \"an\", \"the\", \"of\"]\n",
    "            words = string.split()\n",
    "            filtered_words = [word for word in words if word.lower() not in noise_words]\n",
    "            return ' '.join(filtered_words)\n",
    "        \n",
    "        FP_words = 0\n",
    "        FN_words = 0\n",
    "        TP_words = 0\n",
    "        normalized_string = __preprocess_string__(entity_in_model_response)\n",
    "        normalized_entity_ground_truth = __preprocess_string__(entity_in_ground_truth)\n",
    "        if normalized_string == normalized_entity_ground_truth:\n",
    "            n_words_ground_truth = len(entity_in_ground_truth.split())\n",
    "            n_words_model_response = len(entity_in_model_response.split())\n",
    "            FP_words = max(0, n_words_model_response - n_words_ground_truth)\n",
    "            FN_words = max(0, n_words_ground_truth - n_words_model_response)\n",
    "            TP_words = self._count_common_words(entity_in_model_response, entity_in_ground_truth)\n",
    "            #print('SIMILI NORMALIZED: ', entity_in_model_response, ' e ', entity_in_ground_truth, ' -> FP_words:', FP_words,' FN_words:', FN_words,'TP_words:', TP_words)\n",
    "            return True, entity_in_ground_truth, FP_words, FN_words, TP_words\n",
    "        return False, entity_in_model_response, FP_words, FN_words, TP_words\n",
    "\n",
    "    def _entity_similar_to_ground_truth_entity_Subset(self, entity_in_model_response: str, entity_in_ground_truth: str, ground_truth:list) -> (bool, str, int, int):\n",
    "        \"\"\"\n",
    "        Check if two entities are similar in terms of being a subset of the one in list. E.g., entity='am happy' ground truth='I am happy'.\n",
    "        This is useful when the model output is not exactly the same as the ground truth.\n",
    "\n",
    "        Args:\n",
    "        entity_in_model_response (str): an entity in the model response\n",
    "        entity_in_ground_truth (str): an entity in the ground truth\n",
    "\n",
    "        return:\n",
    "        bool: True if the entities are similar, False otherwise\n",
    "        str: the entity in the ground truth if the entities are similar, the entity in the model response otherwise\n",
    "        FP_words: number of identified false positive words, i.e. number of words identified as entity that are not in the ground truth\n",
    "        FN_words: number of identified false positive words, always 0 in this case\n",
    "        TP_words: number of identified true positive words\n",
    "        \"\"\"\n",
    "        FP_words = 0\n",
    "        FN_words = 0\n",
    "        TP_words = 0\n",
    "        if entity_in_model_response.lower() != entity_in_ground_truth.lower():\n",
    "            if entity_in_model_response.lower() in entity_in_ground_truth.lower():\n",
    "                # words_not_in_gt_entity = entity_in_ground_truth.lower().replace(entity_in_model_response.lower(), '')\n",
    "                # w_are_in_other_entity = any([words_not_in_gt_entity in gt_entity for gt_entity in ground_truth])\n",
    "                # if w_are_in_other_entity:\n",
    "                #     TP_words = self._count_common_words(entity_in_model_response, entity_in_ground_truth)\n",
    "                # else:\n",
    "                FN_words = entity_in_ground_truth.strip().count(\" \") - entity_in_model_response.strip().count(\" \")\n",
    "                TP_words = self._count_common_words(entity_in_model_response, entity_in_ground_truth)\n",
    "                # print('SIMILI Subset: ', entity_in_model_response, ' e ', entity_in_ground_truth, ' -> FP_words:', FP_words,' FN_words:', FN_words,'TP_words:', TP_words)\n",
    "                return True, entity_in_ground_truth, FP_words, FN_words, TP_words\n",
    "        return False, entity_in_model_response, FP_words, FN_words, TP_words\n",
    "\n",
    "    def _entity_similar_to_ground_truth_entity_Superset(self, entity_in_model_response: str, entity_in_ground_truth: str) -> (bool, str, int, int):\n",
    "        \"\"\"\n",
    "        Check if two entities are similar in terms of being a super of the one in list. E.g., entity='I am very happy' ground truth='I am happy'.\n",
    "        This is useful when the model output is not exactly the same as the ground truth.\n",
    "\n",
    "        Args:\n",
    "        entity_in_model_response (str): an entity in the model response\n",
    "        entity_in_ground_truth (str): an entity in the ground truth\n",
    "        threshold (int): the threshold to consider the entities similar. The default value is 80. 0 is completely different, 100 is the same.\n",
    "\n",
    "        return:\n",
    "        bool: True if the entities are similar, False otherwise\n",
    "        str: the entity in the ground truth if the entities are similar, the entity in the model response otherwise\n",
    "        FP_words: number of identified false positive words, i.e. number of words identified as entity that are not in the ground truth\n",
    "        FN_words: number of identified false positive words, always 0 in this case\n",
    "        \"\"\"\n",
    "        FP_words = 0\n",
    "        FN_words = 0\n",
    "        TP_words = 0\n",
    "        if entity_in_model_response.lower() != entity_in_ground_truth.lower():\n",
    "            if entity_in_ground_truth.lower() in entity_in_model_response.lower():\n",
    "                FP_words = entity_in_model_response.strip().count(\" \") - entity_in_ground_truth.strip().count(\" \")\n",
    "                TP_words = self._count_common_words(entity_in_model_response, entity_in_ground_truth)\n",
    "                # print('SIMILI Superset: ', entity_in_model_response, ' e ', entity_in_ground_truth, ' -> FP_words:', FP_words,' FN_words:', FN_words,'TP_words:', TP_words)\n",
    "                return True, entity_in_ground_truth, FP_words, FN_words, TP_words\n",
    "        return False, entity_in_model_response, FP_words, FN_words, TP_words\n",
    "\n",
    "\n",
    "    def _entity_similar_to_ground_truth_entity_Leveshtein(self, entity_in_model_response: str, entity_in_ground_truth: str, threshold: int) -> (bool, str):\n",
    "        \"\"\"\n",
    "        Check if two entities are similar in terms of Leveshtein distance. This is useful when the model output is not exactly the same as the ground truth.\n",
    "\n",
    "        Args:\n",
    "        entity_in_model_response (str): an entity in the model response\n",
    "        entity_in_ground_truth (str): an entity in the ground truth\n",
    "        threshold (int): the threshold to consider the entities similar. The default value is 80. 0 is completely different, 100 is the same.\n",
    "\n",
    "        return:\n",
    "        bool: True if the entities are similar, False otherwise\n",
    "        str: the entity in the ground truth if the entities are similar, the entity in the model response otherwise\n",
    "        \"\"\"\n",
    "        similarity = fuzz.ratio(entity_in_model_response.lower(), entity_in_ground_truth.lower())\n",
    "        if similarity >= threshold:\n",
    "            # print('SIMILI LEVESHTEIN: ', entity_in_model_response, ' e ', entity_in_ground_truth)\n",
    "            return True, entity_in_ground_truth\n",
    "        return False, entity_in_model_response\n",
    "    \n",
    "\n",
    "    def _entity_similar_to_ground_truth_entity(self, entity_in_model_response: str, entity_in_ground_truth: str, leveshtein_threshold: int, similarity_types:list=['case', 'stop_words', 'subset', 'superset', 'leveshtein'], ground_truth:list=[]) -> (bool, str):\n",
    "        \"\"\"\n",
    "        Check if two entities are similar. This is useful when the model output is not exactly the same as the ground truth.\n",
    "\n",
    "        Args:\n",
    "        entity_in_model_response (str): an entity in the model response\n",
    "        entity_in_ground_truth (str): an entity in the ground truth\n",
    "        leveshtein_threshold (int): the threshold to consider the entities similar. The default value is 80. 0 is completely different, 100 is the same.\n",
    "        similarity_types (list): the list of similarity types to consider. The default value is ['case', 'stop_words', 'subset', 'superset', 'leveshtein']\n",
    "\n",
    "        return:\n",
    "        bool: True if the entities are similar, False otherwise\n",
    "        str: the entity in the ground truth if the entities are similar, the entity in the model response otherwise\n",
    "        \"\"\"\n",
    "        FP_words = 0\n",
    "        FN_words = 0\n",
    "        TP_words = 0\n",
    "\n",
    "        if entity_in_model_response == entity_in_ground_truth:\n",
    "            TP_words = len(entity_in_model_response.split())\n",
    "            return True, entity_in_ground_truth, FP_words, FN_words, TP_words\n",
    "        \n",
    "        if 'case' in similarity_types:\n",
    "            similar, entity_to_output, FP_words, FN_words, TP_words = self._entity_similar_to_ground_truth_entity_LowerUppercase(entity_in_model_response, entity_in_ground_truth)\n",
    "            #print('SIMILI CASE: ', similar, entity_to_output, FP_words, FN_words, TP_words)\n",
    "            if similar:\n",
    "                return similar, entity_to_output, FP_words, FN_words, TP_words\n",
    "        if 'stop_words' in similarity_types:\n",
    "            similar, entity_to_output, FP_words, FN_words, TP_words = self._entity_similar_to_ground_truth_entity_StopWords(entity_in_model_response, entity_in_ground_truth)\n",
    "            #print('SIMILI STOP WORDS: ', similar, entity_to_output, FP_words, FN_words, TP_words)\n",
    "            if similar:\n",
    "                return similar, entity_to_output, FP_words, FN_words, TP_words\n",
    "        if 'subset' in similarity_types:\n",
    "            similar, entity_to_output, FP_words, FN_words, TP_words = self._entity_similar_to_ground_truth_entity_Subset(entity_in_model_response, entity_in_ground_truth, entity_in_ground_truth)\n",
    "            #print('SIMILI SUBSET: ', similar, entity_to_output, FP_words, FN_words, TP_words)\n",
    "            if similar:\n",
    "                return similar, entity_to_output, FP_words, FN_words, TP_words\n",
    "        if 'superset' in similarity_types:\n",
    "            similar, entity_to_output, FP_words, FN_words, TP_words = self._entity_similar_to_ground_truth_entity_Superset(entity_in_model_response, entity_in_ground_truth)  \n",
    "            #print('SIMILI SUPERSET: ', similar, entity_to_output, FP_words, FN_words, TP_words)\n",
    "            if similar:\n",
    "                return similar, entity_to_output, FP_words, FN_words, TP_words\n",
    "        if 'leveshtein' in similarity_types:\n",
    "            similar, entity_to_output = self._entity_similar_to_ground_truth_entity_Leveshtein(entity_in_model_response, entity_in_ground_truth, leveshtein_threshold)\n",
    "            #print('SIMILI LEVESTAIN: ', similar, entity_to_output)\n",
    "            if similar:\n",
    "                FP_words, FN_words, TP_words = 0, 0, 0 # non calcolo FP, FN, TP per leveshtein\n",
    "                return similar, entity_to_output, FP_words, FN_words, TP_words\n",
    "\n",
    "        return False, entity_in_model_response, FP_words, FN_words, TP_words\n",
    "    \n",
    "\n",
    "    # def _entity_similar_to_ground_truth_entity_deprecated(self, entity_in_model_response: str, entity_in_ground_truth: str, threshold: int) -> (bool, str):\n",
    "    #     \"\"\"\n",
    "    #     Check if two entities are similar. This is useful when the model output is not exactly the same as the ground truth.\n",
    "\n",
    "    #     Args:\n",
    "    #     entity_in_model_response (str): an entity in the model response\n",
    "    #     entity_in_ground_truth (str): an entity in the ground truth\n",
    "    #     threshold (int): the threshold to consider the entities similar. The default value is 80. 0 is completely different, 100 is the same.\n",
    "\n",
    "    #     return:\n",
    "    #     bool: True if the entities are similar, False otherwise\n",
    "    #     str: the entity in the ground truth if the entities are similar, the entity in the model response otherwise\n",
    "    #     \"\"\"\n",
    "    #     def __preprocess_string__(string):\n",
    "    #         # Remove common articles and noise words\n",
    "    #         noise_words = [\"a\", \"an\", \"the\", \"of\"]\n",
    "    #         words = string.split()\n",
    "    #         filtered_words = [word for word in words if word.lower() not in noise_words]\n",
    "    #         return ' '.join(filtered_words)\n",
    "\n",
    "    #     if entity_in_model_response == entity_in_ground_truth:\n",
    "    #         return True, entity_in_ground_truth\n",
    "        \n",
    "    #     normalized_string = __preprocess_string__(entity_in_model_response)\n",
    "    #     normalized_entity_ground_truth = __preprocess_string__(entity_in_ground_truth)\n",
    "    #     if normalized_string == normalized_entity_ground_truth:\n",
    "    #         print('SIMILI NORMALIZED 1: ', entity_in_model_response, ' e ', entity_in_ground_truth)\n",
    "    #         return True, entity_in_ground_truth\n",
    "        \n",
    "    #     similarity = fuzz.ratio(entity_in_model_response.lower(), entity_in_ground_truth.lower())\n",
    "    #     if similarity >= threshold:\n",
    "    #         print('SIMILI LEVESTAIN 2: ', entity_in_model_response, ' e ', entity_in_ground_truth)\n",
    "    #         return True, entity_in_ground_truth\n",
    "    #     return False, entity_in_model_response\n",
    "    \n",
    "        \n",
    "    def entity_in_ground_truth_list(self, entity_in_model_response: str, ground_truth: list, model_response_list: list, leveshtein_threshold: int, similarity_types:'list[str]') -> (str, int, int):\n",
    "        \"\"\"\n",
    "        Check if an entity is in the ground truth\n",
    "\n",
    "        Args:\n",
    "        entity_in_model_response (str): an entity in the model response\n",
    "        ground_truth (list): the ground truth\n",
    "        model_response_list (list): the list off all entities already in the answer\n",
    "        threshold (int): the threshold to consider the entities similar. The default value is 80. 0 is completely different, 100 is the same.\n",
    "        similarity_types: the list of similarity types to consider. Must contain elements in ['case', 'stop_words', 'subset', 'superset', 'leveshtein']\n",
    "\n",
    "        return:\n",
    "        bool: True if the entity is in the ground truth, False otherwise\n",
    "        str: the entity in the ground truth if the entity is in the ground truth, the entity in the model response otherwise\n",
    "        \"\"\"\n",
    "        strings = []\n",
    "        FPs = []\n",
    "        FNs = []\n",
    "        TPs = []\n",
    "        #print('\\n ENTITY IN MODEL RESPONSE: ', entity_in_model_response)\n",
    "        # if entity_in_model_response in ground_truth:\n",
    "        #     print('TROVATO: ', entity_in_model_response)\n",
    "        #     FPs.append(0)\n",
    "        #     FNs.append(0)\n",
    "        #     TPs.append(len(entity_in_model_response.split()))\n",
    "        for entity_in_ground_truth in ground_truth:\n",
    "            is_in, string, FP, FN, TP = self._entity_similar_to_ground_truth_entity(entity_in_model_response, entity_in_ground_truth, leveshtein_threshold, similarity_types, ground_truth)\n",
    "            if is_in:\n",
    "                strings.append(string)\n",
    "                FPs.append(FP)\n",
    "                FNs.append(FN)\n",
    "                TPs.append(TP)\n",
    "                continue\n",
    "        #if entity_in_model_response in ground_truth and entity_in_model_response\n",
    "        #print('STRINGS: ', strings)\n",
    "        if len(strings) > 0:\n",
    "            if entity_in_model_response in strings: # se ho estratto la stessa, ritorno se stessa\n",
    "                TP = len(entity_in_model_response.split())\n",
    "                #print('returning: ', entity_in_model_response, ' con TP: ', TP)\n",
    "                return entity_in_model_response, 0, 0, TP\n",
    "            else: #\n",
    "                #print('sto analizzando: \"', entity_in_model_response, '\" e ho trovato: ', strings)\n",
    "\n",
    "                #print('returning: ', entity_in_model_response, ' con TP: ', TP, ' FP: ', FP, ' FN: ', FN)\n",
    "                return strings[-1], FPs[-1], FNs[-1], TPs[-1]\n",
    "        else:\n",
    "            FP = len(entity_in_model_response.split())\n",
    "            FN = 0\n",
    "            TP = 0\n",
    "           \n",
    "        #print('returning: ', entity_in_model_response, ' con TP: ', TP, ' FP: ', FP, ' FN: ', FN)\n",
    "        return entity_in_model_response, FP, FN, TP\n",
    "    \n",
    "\n",
    "\n",
    "    def _extract_TP_FP_FN(self, model_response: str, ground_truth: str, similar_is_equal:bool=True, similar_is_equal_threshold: int=100, similarity_types:'list[str]'=['case', 'stop_words', 'subset', 'superset', 'leveshtein'], words_level:bool=True, already_parsed_inputs:bool=False) -> [int, int, int]:\n",
    "        \"\"\"\n",
    "        Compute the F1 score, the precision and the recall between the model output and the ground truth\n",
    "\n",
    "        Args:\n",
    "        model_response (str): the model output as it is returned by the model\n",
    "        ground_truth (str): the ground truth in json format.\n",
    "        similar_is_equal (bool): if True, the function will consider similar entities as equal. The default value is False.\n",
    "        similar_is_equal_threshold (int): the threshold to consider the entities similar. The default value is 80. 0 is completely different, 100 is the same.\n",
    "        words_level (bool): if True, the function will consider as base elements the words. If False, the function will consider as base elements the entity. \n",
    "        E.g., if True, the original sentence is \"Yesterday morning I was so very happy and sad\", the ground truth is [\"yesterday morning\", \"so very happy\"] the model output is [\"morning\", \"happy and\"], the function will consider FP=2 (\"and\"); TP=1 (\"morning\", \"happy\"); FN=2 (\"Yesterday\", \"so\"). \n",
    "        If False, the function will consider FP=1 (\"happy and\"); TP=0; FN=2 (\"Yesterday morning\", \"so very happy\").\n",
    "        similarity_types: the list of similarity types to consider. Must contain elements in ['case', 'stop_words', 'subset', 'superset', 'leveshtein']\n",
    "\n",
    "        \"\"\"\n",
    "        # print('MODEL RESPONSE: ', model_response)\n",
    "        # print('GROUND TRUTH: ', ground_truth)\n",
    "        if not already_parsed_inputs:\n",
    "            # print('ORIGINAL model_response: ', model_response)\n",
    "            model_response = self._parse_json(model_response)\n",
    "            # print('GROUND TRUTH: ', ground_truth)\n",
    "            ground_truth = self._parse_json(ground_truth.replace('\\n', ''))\n",
    "        model_response = model_response[\"entities\"]\n",
    "        ground_truth = ground_truth[\"entities\"]\n",
    "        \n",
    "        \n",
    "        # print('PARSED ORIGINAL model_response: ', model_response)\n",
    "        # print('PARSED GROUND TRUTH: ', ground_truth)\n",
    "        if not similar_is_equal:\n",
    "            similarity_types = []\n",
    "\n",
    "        if words_level:\n",
    "            FP_sum = 0\n",
    "            FN_sum = 0\n",
    "            TP_sum = 0\n",
    "            identified_entities_list = []\n",
    "            for i, response_entity in enumerate(model_response):\n",
    "                entity_identified, FP, FN, TP= self.entity_in_ground_truth_list(response_entity, ground_truth, model_response, similar_is_equal_threshold, similarity_types)\n",
    "                FP_sum += FP\n",
    "                FN_sum += FN\n",
    "                TP_sum += TP\n",
    "                identified_entities_list.append(entity_identified)\n",
    "            #print('IDENTIFIED ENTITIES: ', identified_entities_list)\n",
    "            #print('GROUND TRUTH: ', ground_truth)\n",
    "            FN_entities = set(ground_truth).difference(set(identified_entities_list))\n",
    "            FN_entities = [entity.split() for entity in FN_entities]\n",
    "            FN_entities = [item for row in FN_entities for item in row]\n",
    "            #print('FALSE NEGATIVES: ', FN_entities)\n",
    "            FN_sum += len(FN_entities)\n",
    "            #print('PARSED GROUND TRUTH: ', ground_truth, 'TP_sum:', TP_sum, 'FP_sum:', FP_sum, 'FN_sum:', FN_sum, '\\n\\n')\n",
    "            return [TP_sum, FP_sum, FN_sum]\n",
    "           \n",
    "        elif not words_level:\n",
    "            for i, response_entity in enumerate(model_response):\n",
    "                model_response[i], _, _, _= self.entity_in_ground_truth_list(response_entity, ground_truth, model_response, similar_is_equal_threshold, similarity_types)\n",
    "            #print('PARSED GROUND TRUTH: ', ground_truth)\n",
    "            #print('NEW model_response to calculate TP, FP, FN: ', model_response, '\\n\\n')\n",
    "\n",
    "            TP = len(set(model_response).intersection(set(ground_truth)))\n",
    "            FP = len(set(model_response).difference(set(ground_truth)))\n",
    "            FN = len(set(ground_truth).difference(set(model_response)))\n",
    "            # F1 = 2 * TP / (2 * TP + FN + FP)\n",
    "            return [TP, FP, FN]\n",
    "    \n",
    "    def generate_evaluation_table(self, similar_is_equal_threshold: int, words_level:bool, similarity_types:'list[str]', already_parsed_inputs:bool=False, add_TP_FP_TN_FN_to_data:bool=False) -> dict:\n",
    "        \"\"\"\n",
    "        Generate the evaluation table for the model output and the ground truth.\n",
    "\n",
    "        Args:\n",
    "        similar_is_equal_threshold (int): the threshold to consider the entities similar by the Leveshtein distance. The default value is 80. 0 is completely\n",
    "        different, 100 is the same. \n",
    "        words_level (bool): if True, the function will consider as base elements the words. If False, the function will consider as base elements the entity. \n",
    "        E.g., if True, the original sentence is \"Yesterday morning I was so very happy and sad\", the ground truth is [\"yesterday morning\", \"so very happy\"] the model output is [\"morning\", \"happy and\"], the function will consider FP=2 (\"and\"); TP=1 (\"morning\", \"happy\"); FN=2 (\"Yesterday\", \"so\"). \n",
    "        If False, the function will consider FP=1 (\"happy and\"); TP=0; FN=2 (\"Yesterday morning\", \"so very happy\").\n",
    "        similarity_types: the list of similarity types to consider. Must contain elements in ['case', 'stop_words', 'subset', 'superset', 'leveshtein']\n",
    "\n",
    "        return:\n",
    "        dict: the evaluation table\n",
    "        \"\"\"\n",
    "        self.evaluation_table = None\n",
    "        metrics_list = []\n",
    "        if not already_parsed_inputs:\n",
    "            for i, res in enumerate(self.data['model_output']):\n",
    "            #  if self.cleaner.verbose: print('res:', res)\n",
    "                metrics_list.append(self._extract_TP_FP_FN(res, self.data['ground_truth'][i], True, similar_is_equal_threshold, similarity_types, words_level, already_parsed_inputs))\n",
    "        else:\n",
    "            for i, res in enumerate(self.data['model_output_parsed']):\n",
    "                metrics_list.append(self._extract_TP_FP_FN(res, self.data['ground_truth_parsed'][i], True, similar_is_equal_threshold, similarity_types, words_level, already_parsed_inputs))\n",
    "        metrics_dataframe = pd.DataFrame(metrics_list, columns=['TP', 'FP', 'FN'])\n",
    "        summary = metrics_dataframe.sum()\n",
    "        precision = summary['TP'] / (summary['TP'] + summary['FP'])\n",
    "        recall = summary['TP'] / (summary['TP'] + summary['FN'])\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        self.evaluation_table = {'evaluation': metrics_dataframe, 'precision':precision, 'recall':recall, 'f1':f1}\n",
    "        if add_TP_FP_TN_FN_to_data: self.add_TP_FP_TN_FN_to_data(already_parsed_inputs=already_parsed_inputs)\n",
    "\n",
    "        return {'evaluation': metrics_dataframe, 'precision':precision, 'recall':recall, 'f1':f1}\n",
    "\n",
    "    def add_TP_FP_TN_FN_to_data(self, already_parsed_inputs:bool=False):\n",
    "        \"\"\"\n",
    "        Add the True Positives, False Positives, False Negatives, Precision, Recall and F1 score to the data. Precision, recall and f1 are approximated to avoid division by zero.\n",
    "        \"\"\"\n",
    "        metrics_list = []\n",
    "        if not already_parsed_inputs:\n",
    "            for i, res in enumerate(self.data['model_output']):\n",
    "                metrics_list.append(self._extract_TP_FP_FN(res, self.data['ground_truth'][i], True, 100, ['case', 'subset', 'superset'], already_parsed_inputs=already_parsed_inputs))\n",
    "        else:\n",
    "            for i, res in enumerate(self.data['model_output_parsed']):\n",
    "                metrics_list.append(self._extract_TP_FP_FN(res, self.data['ground_truth_parsed'][i], True, 100, ['case', 'subset', 'superset'], already_parsed_inputs=already_parsed_inputs))\n",
    "        if 'TP' in self.data.column_names:\n",
    "            self.data = self.data.remove_columns(['TP', 'FP', 'FN', 'precision', 'recall', 'f1'])\n",
    "        self.data = self.data.add_column('TP', [el[0] for el in metrics_list])\n",
    "        self.data = self.data.add_column('FP', [el[1] for el in metrics_list])\n",
    "        self.data = self.data.add_column('FN', [el[2] for el in metrics_list])\n",
    "        self.data = self.data.add_column('precision', [el[0] / (el[0] + el[1] + 1e-16) for el in metrics_list])\n",
    "        self.data = self.data.add_column('recall', [el[0] / (el[0] + el[2] + 1e-16) for el in metrics_list])\n",
    "        def _f1(example):\n",
    "            example['f1'] = 2 * (example['precision'] * example['recall']) / (example['precision'] + example['recall'] + 1e-16)\n",
    "            return example\n",
    "        self.data = self.data.map(lambda x: _f1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/llama/7B_NoQuant_FT/maxNewTokensFactor8_nShotsInference0_llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_16_32_0.01_2_0.0002.csv\"\n",
    "apadpetrs_checkpoint = \"ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_16_32_0.01_2_0.0002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "class OutputAnalist():\n",
    "    def __init__(self, data, verbose=False) -> None:\n",
    "        self.verbose = verbose\n",
    "        self.data = data\n",
    "        self.counter_dict = {\n",
    "            'perfect_output':0,\n",
    "            'is_empty_list':0,\n",
    "            'is_list_of_lists':0,\n",
    "            'is_list_of_dicts':0,\n",
    "            'is_list_of_lists_and_dict':0,\n",
    "            'is_list_of_strings':0,\n",
    "            'is_list_of_empty_dict':0,\n",
    "            'is_list_with_one_empty_dict':0,\n",
    "            'is_list_of_dicts_with_empty_lists':0,\n",
    "            'is_list_of_dicts_with_one_key_multiple_values':0,\n",
    "            'is_list_of_dicts_with_multiple_keys_included_entity':0,\n",
    "            'is_list_of_dict_numeric_values':0,\n",
    "            'is_list_of_dicts_none_values':0,\n",
    "            'is_list_of_dicts_and_strings':0,\n",
    "            'is_list_of_dicts_and_lists_of_strings':0,\n",
    "            'is_list_of_dicts_with_value_list':0,\n",
    "            'is_string':0,\n",
    "            'is_list_of_strings_representing_dicts':0,\n",
    "            'is_list_of_dicts_of_lists':0,\n",
    "            'is_numeric':0,\n",
    "            'are_entities_extracted_as_dict_keys_instead_of_values':0,\n",
    "            'uknown':0\n",
    "        }\n",
    "  \n",
    "    def _remove_space_from_dict_keys(self, model_ouput_list: list) -> list:\n",
    "        \"\"\"\n",
    "        Remove the spaces from the keys of a dictionary. E.g., [{\"entity \": \"value\"}] -> [{\"entity\": \"value\"}]\n",
    "\n",
    "        Args:\n",
    "        model_ouput_list (dict): the list of dictionaries to be cleaned\n",
    "\n",
    "        return:\n",
    "        list: the cleaned list of dicts\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for dict in model_ouput_list:\n",
    "            # print('DICT: ', dict)\n",
    "            out.append({k.replace(' ', ''): v for k, v in dict.items()})\n",
    "        return out\n",
    "    \n",
    "    def _drop_duplicates(self, model_response: list) -> str:\n",
    "        \"\"\"\n",
    "        Drop the duplicates from a list. This is useful when the model output contains the same entity multiple times.\n",
    "\n",
    "        Args:\n",
    "        model_response (str): the model response with no duplicates\n",
    "        \"\"\"\n",
    "        # print('DROPPING DUPLICATES: ', model_response)\n",
    "        try :\n",
    "            return list({v['entity']:v for v in model_response}.values())\n",
    "        except Exception as error:\n",
    "            model_response = self._remove_space_from_dict_keys(model_response)\n",
    "            # print('ERROR: ', model_response)\n",
    "            return list({v['entity']:v for v in model_response}.values())\n",
    "        \n",
    "    def _assess_model_output(self, model_response: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the model output is in the right format. If not, return False.\n",
    "        \n",
    "        Args:\n",
    "        model_output (str): the postprocessed model output after beeing passed to _postprocess_model_output()\n",
    "\n",
    "        return:\n",
    "        bool: True if the format is correct, False otherwise\n",
    "        \"\"\"\n",
    "        good_format = True\n",
    "        try :\n",
    "            res = json.loads(model_response)\n",
    "            # print( res)\n",
    "        except:\n",
    "            good_format = False\n",
    "        return good_format\n",
    "\n",
    "            \n",
    "    def _remove_json_special_chars(self, string):\n",
    "        \"\"\"\n",
    "        Remove the special characters from a string. This is useful when the model output contains special characters that are not allowed in the json format.\n",
    "        \"\"\"\n",
    "        # print('sto pulendo: ', string)\n",
    "        chars = ['\\xa0', '\\x80', '\\x93', '\\U00100000', '\\r\\n', '\\U00100000I', '\\\\u001d', '\\\\\"']\n",
    "        for char in chars:\n",
    "            string = string.replace(char, ' ')\n",
    "        char_no_space = ['\\xad']\n",
    "        for char in char_no_space:\n",
    "            string = string.replace(char, '')\n",
    "        string = string.replace('\\\\u0010', '^')\n",
    "        return string\n",
    "    \n",
    "    \n",
    "    def _clean_ground_truth(self, example: dict) -> dict:\n",
    "        ground_truth = example['ground_truth']\n",
    "        # print('inner ground truth: ', ground_truth)\n",
    "        ground_truth = self._remove_json_special_chars(ground_truth)\n",
    "        ground_truth = ground_truth.replace('</s>', '').replace('<|im_e', '').replace('<|end_of_text|>', '').replace('<|endoftext|>', '')\n",
    "        return({'ground_truth': ground_truth})\n",
    "\n",
    "    def _clean_model_output(self, example: dict,  wrong_keys_to_entity:bool, latest_version:bool=True) -> dict:\n",
    "        \"\"\"\n",
    "        Postprocess the model output to return a json like formatted string that can be used to compute the F1 score.\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the model output as it is returned by the model. The processing of the output is done in the function\n",
    "        wrong_keys_to_entity (bool): if True, the function also extracts the dictionaries with keys different from 'entity', converting the keys into 'entity'. If not, all keys that are not 'entity' are dropped\n",
    "\n",
    "        return:\n",
    "        dict: the model response\n",
    "\n",
    "        \"\"\"\n",
    "       \n",
    "        def is_empty_list(string:str)  -> bool:\n",
    "            if string=='[]':\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_lists(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, list) for item in tmp):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_lists_and_dict(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                found_dict = False\n",
    "                found_list = False\n",
    "                for element in tmp:\n",
    "                    if isinstance(element, list):\n",
    "                        found_list = True\n",
    "                    elif isinstance(element, dict):\n",
    "                        found_dict = True\n",
    "                    if found_list and found_dict:\n",
    "                        return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_strings(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, str) for item in tmp):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        def is_list_of_empty_dict(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                #print('TMP: ', tmp)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    if all(str(item) == \"{}\" for item in tmp):\n",
    "                        return True\n",
    "            return False\n",
    "\n",
    "        def is_list_with_one_empty_dict(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list):\n",
    "                    for item in tmp:\n",
    "                        if item == {}:\n",
    "                            return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_with_empty_lists(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        for v in item.values():\n",
    "                            if v == []:\n",
    "                                return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_with_one_key_multiple_values(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        if len(item) == 1 and len(item.values()) > 1:\n",
    "                            return True\n",
    "            return False\n",
    "\n",
    "        def is_list_of_dicts_with_multiple_keys_included_entity(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        if len(item) > 1 and 'entity' in item.keys():\n",
    "                            return True\n",
    "            return False\n",
    "\n",
    "        def is_list_of_dict_numeric_values(string:str)  -> bool:\n",
    "            #print('STRING: ', string)\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                #print('TMP: ', tmp)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        if len(item.values()) > 0:\n",
    "                            val = list(item.values())[0] \n",
    "                            if isinstance(val, int) or isinstance(val, float):\n",
    "                                return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_none_values(string:str) -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        if len(item.values()) > 0:\n",
    "                            val = list(item.values())[0] \n",
    "                            if val is None:\n",
    "                                return True\n",
    "            return False\n",
    "\n",
    "        def is_list_of_dicts_and_strings(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                #print('ASSESSED')\n",
    "                tmp = json.loads(string)\n",
    "                found_dict = False\n",
    "                found_string = False\n",
    "                for element in tmp:\n",
    "                    if isinstance(element, str):\n",
    "                        found_string = True\n",
    "                    elif isinstance(element, dict):\n",
    "                        found_dict = True\n",
    "                    if found_string and found_dict:\n",
    "                        return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_and_lists_of_strings(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                # print('TMP: ', tmp)\n",
    "                if isinstance(tmp, list):\n",
    "                    if all(isinstance(item, dict) for item in tmp):\n",
    "                        return False\n",
    "                    for item in tmp:\n",
    "                        # print('ITEM: ', item)\n",
    "                        if isinstance(item, dict):\n",
    "                            \n",
    "                            if len(item.values()) == 0:\n",
    "                               return False\n",
    "                            if item.get('entity') is None:\n",
    "                                return False\n",
    "                        elif isinstance(item, list):\n",
    "                            if len(item) != 1:\n",
    "                                return False\n",
    "                            if not isinstance(item[0], str):\n",
    "                                return False\n",
    "                        else:\n",
    "                            return False\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_with_value_list(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        for v in item.values():\n",
    "                            if isinstance(v, list):\n",
    "                                return True\n",
    "            return False\n",
    "        \n",
    "        def is_string(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, str):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_strings_representing_dicts(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                # print('TMP: ', tmp)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, str) for item in tmp):\n",
    "                    tmp_list = []\n",
    "                    for item in tmp:\n",
    "                        # print('ITEM: ', item)\n",
    "                        if self._assess_model_output(item):\n",
    "                          tmp_list.append(json.loads(item))\n",
    "                    if all(isinstance(item, dict) for item in tmp_list):\n",
    "                        return True\n",
    "            return False\n",
    "        \n",
    "        def is_list_of_dicts_of_lists(string:str)  -> bool:\n",
    "            # print('STRING: ', string)\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                # print('TMP: ', tmp)\n",
    "                if isinstance(tmp, list) and all(isinstance(item, dict) for item in tmp):\n",
    "                    for item in tmp:\n",
    "                        # print('item: ',item)\n",
    "                        tmp2 = list(item.values())[0]\n",
    "                        if len(tmp2) > 0:\n",
    "                            if isinstance(list(item.values())[0], list):\n",
    "                                return True\n",
    "            return False\n",
    "        \n",
    "        def is_numeric(string:str)  -> bool:\n",
    "            if self._assess_model_output(string):\n",
    "                tmp = json.loads(string)\n",
    "                if isinstance(tmp, (int, float)):\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def are_entities_extracted_as_dict_keys_instead_of_values(string:str, example:dict) -> bool:\n",
    "            if is_list_of_dicts(string):\n",
    "                tmp = json.loads(string)\n",
    "                keys = [key for item in tmp for key in item.keys()]\n",
    "                if 'entity' not in keys:\n",
    "                    if all(entity in example['sentence'] for entity in keys):\n",
    "                        return True\n",
    "            return False\n",
    "        \n",
    "        \n",
    "        \n",
    "        def convert_wrong_keys_into_entity(string:str) -> List[str]:\n",
    "            if is_list_of_dicts(string):\n",
    "                tmp = json.loads(string)\n",
    "                tmp = [str({\"entity\":v}) for el in tmp for v in el.values()]\n",
    "                return tmp\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "\n",
    "        def only_dicts_with_key_entity(string:str, wrong_keys_to_entity:bool) -> Tuple[bool, str]:\n",
    "            \"\"\"\n",
    "            Extract only the dictionaries with the key 'entity' in the list of dictionaries in the string\n",
    "            \n",
    "            Args:\n",
    "            string (str): the string to be cleaned\n",
    "            wrong_keys_to_entity (bool): if True, the function also extracts the dictionaries with keys different from 'entity', converting the keys into 'entity'\n",
    "            \"\"\"\n",
    "            els_between_curly = re.findall(r'\\{(.+?)\\}', string)\n",
    "            clean = [el for el in els_between_curly if el.startswith('\"entity\"') or el.startswith(\"'entity'\")]\n",
    "            clean = ['{' + el + '}' for el in clean]\n",
    "            dirty = []\n",
    "            if wrong_keys_to_entity:\n",
    "                dirty = [el for el in els_between_curly if (not el.startswith('\"entity\"')) and (not el.startswith(\"'entity'\"))]\n",
    "                dirty = ['{' + el + '}' for el in dirty]\n",
    "                dirty = '[' + ', '.join(dirty) + ']'\n",
    "                cleaned_dirty = convert_wrong_keys_into_entity(dirty)\n",
    "                out = '[' + ', '.join(clean) + ', '.join(cleaned_dirty) +  ']'\n",
    "            else:\n",
    "                out = '[' + ', '.join(clean) + ']'\n",
    "            # out = out.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\" \\'\", \" \\\"\")\n",
    "            operations_performed = False\n",
    "            if len(clean) != len(els_between_curly):\n",
    "                operations_performed = True\n",
    "            if is_empty_list(out):\n",
    "                return operations_performed, '[{\"entity\":\"\"}]'\n",
    "            return operations_performed, str(out)\n",
    "        \n",
    "        if self.verbose: print('EXAMPLE:  ', example['model_responses'])\n",
    "        model_output = example['model_responses']\n",
    "        if self.verbose: print('ORIGINAL MODEL OUTPUT:', model_output)\n",
    "        # print('ORIGINAL MODEL OUTPUT:', model_output)\n",
    "        if self.verbose: print('GROUND TRUTH: ', example['ground_truth'])\n",
    "        # model_output = self._exceptions_handler(model_output)\n",
    "\n",
    "        if is_list_of_dicts(model_output):\n",
    "            self.counter_dict['perfect_output'] += 1\n",
    "            if self.verbose: print('is_list_of_dicts')\n",
    "            tmp = json.loads(model_output)\n",
    "            return {'model_output':str(tmp)}\n",
    "    \n",
    "        if model_output is None or is_empty_list(model_output):\n",
    "            return {'model_output':'[{\"entity\":\"\"}]'}\n",
    "        \n",
    "        # model_output = self._special_cases_handler(model_output)\n",
    "        model_output = self._remove_json_special_chars(model_output)\n",
    "        if self.verbose:print('PULITO: ', model_output)\n",
    "\n",
    "                \n",
    "        if are_entities_extracted_as_dict_keys_instead_of_values(model_output, example):\n",
    "            self.counter_dict['are_entities_extracted_as_dict_keys_instead_of_values'] += 1\n",
    "            if self.verbose: print('ENTITIES EXTRACTED AS DICT KEYS INSTEAD OF VALUES')\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp = [{\"entity\":k} for el in tmp for k in el.keys() ]\n",
    "            tmp = str(tmp)\n",
    "            return {'model_output':tmp}\n",
    "        \n",
    "        if is_list_of_dicts_and_lists_of_strings(model_output):\n",
    "            self.counter_dict['is_list_of_dicts_and_lists_of_strings'] += 1\n",
    "            if self.verbose: print('is_list_of_dicts_and_lists_of_strings')\n",
    "            tmp = json.loads(model_output)\n",
    "            out = []\n",
    "            for item in tmp:\n",
    "                if self.verbose: print('ITEM: ', item)\n",
    "                if isinstance(item, dict):\n",
    "                    out.append(item)\n",
    "                elif isinstance(item, list):\n",
    "                    out.append({\"entity\":item[0]})\n",
    "            return {'model_output':str(out)}\n",
    "\n",
    "        if is_numeric(model_output):\n",
    "            self.counter_dict['is_numeric'] += 1\n",
    "            # print('IS NUMERIC')\n",
    "            return {'model_output':'[{\"entity\":\"\"}]'}\n",
    "\n",
    "        # print('QUI HO QUESTO: ', model_output)\n",
    "        if is_list_of_strings_representing_dicts(model_output):\n",
    "            self.counter_dict['is_list_of_strings_representing_dicts'] += 1\n",
    "            if self.verbose: print('is_list_of_strings_representing_dicts 1')                \n",
    "            tmp = json.loads(model_output)\n",
    "            tmp_list = []\n",
    "            for item in tmp:\n",
    "                if self._assess_model_output(item):\n",
    "                  tmp_list.append(json.loads(item))\n",
    "            if self.verbose: print('TEMPOOOO 2 ',tmp)\n",
    "            return {'model_output':str(tmp_list)}\n",
    "        \n",
    "        if is_list_of_dicts_with_one_key_multiple_values(model_output):\n",
    "            self.counter_dict['is_list_of_dicts_with_one_key_multiple_values'] += 1\n",
    "            if self.verbose: print('is_list_of_dicts_with_one_key_multiple_values')\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp = [{\"entity\":v[0]} for el in tmp for v in el.values()]\n",
    "            return {'model_output':str(tmp)}\n",
    "       \n",
    "        if is_list_of_dicts_with_multiple_keys_included_entity(model_output):\n",
    "            self.counter_dict['is_list_of_dicts_with_multiple_keys_included_entity'] += 1\n",
    "            if self.verbose: print('is_list_of_dicts_with_multiple_keys_included_entity')\n",
    "            tmp = json.loads(model_output)\n",
    "            out = []\n",
    "            for item in tmp:\n",
    "                if item.get('entity') is not None:\n",
    "                    out.append({\"entity\":item.get('entity')})\n",
    "            return {'model_output':str(out)}\n",
    "        \n",
    "        \n",
    "        if is_list_of_lists_and_dict(model_output):\n",
    "            self.counter_dict['is_list_of_lists_and_dict'] += 1\n",
    "            if self.verbose: print('is_list_of_lists_and_dict')\n",
    "            tmp = json.loads(model_output)\n",
    "            for el in tmp:\n",
    "                if isinstance(el, list):\n",
    "                    tmp = str(el)\n",
    "                    # print('is_list_of_lists_and_dict')\n",
    "                    return {'model_output':tmp}\n",
    "                \n",
    "        if is_list_of_lists(model_output):\n",
    "            self.counter_dict['is_list_of_lists'] += 1\n",
    "            if self.verbose: print('is_list_of_lists')\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp2 = str(tmp[0]).replace(\"'\", \"\\\"\")\n",
    "            if is_list_of_dicts_and_strings(tmp2):\n",
    "                tmp = tmp[0]\n",
    "                out = [item for item in tmp if isinstance(item, dict)]\n",
    "                return {'model_output':str(out)} \n",
    "            tmp = str(tmp[0])\n",
    "            return {'model_output':tmp}\n",
    "        \n",
    "\n",
    "        if is_list_of_strings(model_output):\n",
    "            self.counter_dict['is_list_of_strings'] += 1\n",
    "            if self.verbose: print('is_list_of_strings')\n",
    "            tmp = json.loads(model_output)\n",
    "            tmp = [{\"entity\":el} for el in tmp]\n",
    "            tmp = str(tmp)\n",
    "            # print('is_list_of_strings')\n",
    "            if self.verbose: print('TEMPOOOO ',tmp)\n",
    "            return {'model_output': tmp}\n",
    "        \n",
    "        if is_string(model_output):\n",
    "            self.counter_dict['is_string'] += 1\n",
    "            # model_output = model_output.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\" \\'\", \" \\\"\")\n",
    "            if self.verbose: print('PULO: ', model_output)\n",
    "            tmp = json.loads(model_output)\n",
    "            if all(el in tmp for el in ['{', 'entity', '}']):\n",
    "                return {'model_output':tmp}\n",
    "            tmp = [{\"entity\":tmp}]\n",
    "            tmp = str(tmp)\n",
    "            #print('is_string')\n",
    "            return {'model_output':tmp}\n",
    "\n",
    "        \n",
    "        if latest_version:\n",
    "            model_output = self._extract_text_between_curl_brackets(model_output)\n",
    "            model_output = self._clean_text_between_curl_brackets(model_output)\n",
    "\n",
    "            # print('QUI HO il SECONDO QUESTO: ', model_output)\n",
    "\n",
    "            if is_list_of_strings_representing_dicts(model_output):\n",
    "                self.counter_dict['is_list_of_strings_representing_dicts'] += 1\n",
    "                if self.verbose: print('is_list_of_strings_representing_dicts 2')                \n",
    "                tmp = json.loads(model_output)\n",
    "                tmp_list = []\n",
    "                for item in tmp:\n",
    "                    if self._assess_model_output(item):\n",
    "                        tmp_list.append(json.loads(item))\n",
    "                return {'model_output':str(tmp_list)}\n",
    "            \n",
    "            if is_list_of_dicts_with_one_key_multiple_values(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_with_one_key_multiple_values'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_with_one_key_multiple_values')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [{\"entity\":v[0]} for el in tmp for v in el.values()]\n",
    "                return {'model_output':str(tmp)}\n",
    "            \n",
    "            if is_list_of_dicts_and_lists_of_strings(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_and_lists_of_strings'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_and_lists_of_strings')\n",
    "                tmp = json.loads(model_output)\n",
    "                out = []\n",
    "                for item in tmp:\n",
    "                    # print('ITEM: ', item)\n",
    "                    if isinstance(item, dict):\n",
    "                        out.append(item)\n",
    "                    elif isinstance(item, list):\n",
    "                        out.append({\"entity\":item[0]})\n",
    "                return {'model_output':str(out)}\n",
    "            \n",
    "            if self.verbose: print('QUI HO il TEERZO QUESTO: ', model_output)\n",
    "\n",
    "            if is_list_of_dicts_with_empty_lists(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_with_empty_lists'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_with_empty_lists')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [{\"entity\":v} for el in tmp for v in el.values() if v != []]\n",
    "                # print('TMP: ', tmp)\n",
    "                if is_list_of_dicts_with_value_list(str(tmp).replace(\"'\", \"\\\"\")):\n",
    "                    if self.verbose: print('is_list_of_dicts_with_value_list')\n",
    "                    tmp = [{\"entity\":v} for el in tmp for v in el.values() if not isinstance(v, list)]\n",
    "                    tmp2 = [{\"entity\":v[0]} for el in tmp for v in el.values() if isinstance(v, list)]\n",
    "                    # print('returning this: ', {'model_output ':str(tmp2)}  )\n",
    "                    return {'model_output':str(tmp2)}\n",
    "                # print('returning this: ', {'model_output ':str(tmp)}  )\n",
    "\n",
    "                return {'model_output':str(tmp)}\n",
    "            \n",
    "            if self.verbose: print('QUI HO il QUARTO QUESTO:', model_output)\n",
    "\n",
    "            if is_list_of_dicts_with_value_list(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_with_value_list'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_with_value_list')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [{\"entity\":v} for el in tmp for v in el.values() if not isinstance(v, list)]\n",
    "                tmp2 = [{\"entity\":v[0]} for el in tmp for v in el.values() if isinstance(v, list)]\n",
    "                return {'model_output':str(tmp)}\n",
    "\n",
    "            if is_list_of_dict_numeric_values(model_output):\n",
    "                self.counter_dict['is_list_of_dict_numeric_values'] += 1\n",
    "                if self.verbose: print('is_list_of_dict_int_values')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [str({\"entity\":str(v)}) for el in tmp for v in el.values()]\n",
    "                model_output = str(tmp)\n",
    "            \n",
    "            if is_list_of_dicts_none_values(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_none_values'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_none_values')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [str({\"entity\":v}) for el in tmp for v in el.values() if v is not None]\n",
    "                model_output = str(tmp)\n",
    "                    \n",
    "            if is_list_of_empty_dict(model_output):\n",
    "                self.counter_dict['is_list_of_empty_dict'] += 1\n",
    "                if self.verbose: print('is_list_of_empty_dict')\n",
    "                return {'model_output':'[{\"entity\":\"\"}]'}\n",
    "            \n",
    "            if is_list_with_one_empty_dict(model_output):\n",
    "                self.counter_dict['is_list_with_one_empty_dict'] += 1\n",
    "                if self.verbose: print('is_list_with_one_empty_dict')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [el for el in tmp if el != {}]\n",
    "                model_output = tmp\n",
    "                return {'model_output':str(model_output)}\n",
    "            \n",
    "            if is_list_of_dicts_of_lists(model_output):\n",
    "                self.counter_dict['is_list_of_dicts_of_lists'] += 1\n",
    "                if self.verbose: print('is_list_of_dicts_of_lists')\n",
    "                tmp = json.loads(model_output)\n",
    "                tmp = [{\"entity\":v} for el in tmp for v in el.values() if not isinstance(v, list)]\n",
    "                # tmp.extend([{\"entity\":el.values()[0]} for el in tmp if isinstance(el.values(), list)])\n",
    "                # print('returning this: ', {'model_output ':str(tmp)}  )\n",
    "                return {'model_output':str(tmp)}  \n",
    "                \n",
    "            if self.verbose: print('CLEANED: ', model_output)\n",
    "            cleaning_done, cleaned_model_output = only_dicts_with_key_entity(model_output, wrong_keys_to_entity=wrong_keys_to_entity)\n",
    "            if cleaning_done:\n",
    "                model_output = cleaned_model_output\n",
    "            \n",
    "            if is_list_of_dicts(model_output):\n",
    "                self.counter_dict['is_list_of_dicts'] += 1\n",
    "                if self.verbose: print('PRE  CLEANED: ', model_output)\n",
    "                if is_list_of_dicts_with_multiple_keys_included_entity(model_output):\n",
    "                    self.counter_dict['is_list_of_dicts_with_multiple_keys_included_entity'] += 1\n",
    "                    if self.verbose: print('is_list_of_dicts_with_multiple_keys_included_entity')\n",
    "                    tmp = json.loads(model_output)\n",
    "                    out = []\n",
    "                    for item in tmp:\n",
    "                        if len(item) > 1 and 'entity' in item.keys():\n",
    "                            out.append({\"entity\":item.get('entity')})\n",
    "                    return {'model_output':str(out)}\n",
    "                tmp = json.loads(model_output)\n",
    "                return {'model_output':str(tmp)}\n",
    "            \n",
    "            else: \n",
    "                self.counter_dict['uknown'] += 1\n",
    "                # print('NOT CLEANED: ', model_output, '\\n\\n')\n",
    "                return {'model_output':'[{\"entity\":\"\"}]'}\n",
    "        \n",
    "            \n",
    "    def _exceptions_handler(self, model_output: str, error) -> str:\n",
    "        # if hasattr(error, 'msg'):\n",
    "        #     if error.msg.startswith('Expecting property name enclosed in double quotes'):\n",
    "        #         model_output = model_output.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\": \\'\", \": \\\"\")\n",
    "        \n",
    "        try:\n",
    "            json.loads(model_output)\n",
    "        except Exception as error:\n",
    "            if isinstance(error, json.decoder.JSONDecodeError):\n",
    "                #if error.msg == \"Expecting ',' delimiter\":\n",
    "                key_part, value_part = model_output.split(': ', 1)\n",
    "                first_occurrence = value_part.find('\"')\n",
    "                last_occurrence = value_part.rfind('\"')\n",
    "                model_output = key_part + ': \"' + value_part[first_occurrence+1:last_occurrence].replace(\"'\", r'\\'') + '\"' + '}'\n",
    "        return model_output\n",
    "    # .replace(\"\\'\", \" \")\n",
    "    \n",
    "    def _substitute_apexes(self, model_output: str) -> str:\n",
    "        model_output = model_output.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\": \\'\", \": \\\"\")\n",
    "        return model_output\n",
    "    \n",
    "    \n",
    "    def _extract_text_between_curl_brackets(self, model_output: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the text between the curl brackets of the model output, as enities are usually outputted in this format: {\"entity\": \"value\"}\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the example from the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        text_between_curl_brackets = re.findall(r'\\{(.+?)\\}', model_output)\n",
    "        cleaned_output = ['{'+ el +'}' for el in text_between_curl_brackets]\n",
    "        cleaned_output = '[' + ', '.join(cleaned_output) + ']'\n",
    "        return cleaned_output\n",
    "    \n",
    "\n",
    "    def _clean_text_between_curl_brackets(self, text_between_curl_brackets: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean the text between the curl brackets of the model output, as entities are usually outputted in this format: {\"key\": \"value\"}\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the example from the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        text_between_curl_brackets = re.sub(r'\",(.+?)}', r'\"}', text_between_curl_brackets)\n",
    "        text_between_curl_brackets = re.sub(r'{},', r'', text_between_curl_brackets)\n",
    "        text_between_curl_brackets = re.sub(r',{}', r'', text_between_curl_brackets)\n",
    "        # print('CLEANED: ', text_between_curl_brackets)\n",
    "        # text_between_curl_brackets = re.sub(r'\\{\"entity\":\\[\\]\\},', r'', text_between_curl_brackets)\n",
    "        # text_between_curl_brackets = re.sub(r',{\\'entity\\':[]}', r'', text_between_curl_brackets)\n",
    "        return text_between_curl_brackets\n",
    "    \n",
    "    def apply_cleaning(self, data, wrong_keys_to_entity) -> None:\n",
    "        \"\"\"\n",
    "        Apply the cleaning to the model output and return the cleaned response in a new cloumn called 'model_output\n",
    "\n",
    "        Args:\n",
    "        data (list): the dataset containing the model output\n",
    "        wrong_keys_to_entity (bool): if True, the function also extracts the dictionaries with keys different from 'entity', converting the keys into 'entity'. If not, all keys that are not 'entity' are dropped\n",
    "        \"\"\"\n",
    "        data = data.filter(lambda example: example[\"entities\"] is not None)\n",
    "        data = data.map(lambda x: self._clean_ground_truth(x), remove_columns=['ground_truth'])\n",
    "        data = data.map(lambda x: self._clean_model_output(x, wrong_keys_to_entity)) \n",
    "        self.data = data\n",
    "        return data\n",
    "    \n",
    "    def get_examples_based_on_metric(self, metric, upper_threshold=1, lower_threshold=0):\n",
    "        \"\"\"\n",
    "        Select the examples based on the metric and the threshold.\n",
    "        Args:\n",
    "        metric (str): the metric to consider\n",
    "        threshold (float): the threshold to consider\n",
    "        return:\n",
    "        list: the list of examples that satisfy the condition\n",
    "        \"\"\"\n",
    "        out = [example for example in self.data if example[metric] <= upper_threshold and example[metric] >= lower_threshold]\n",
    "        return(Dataset.from_pandas(pd.DataFrame(out)))\n",
    "\n",
    "    def create_allucinations_columns(self, data, verbose:bool=False):\n",
    "        \"\"\"\n",
    "        Create two columns that contain the invented entities/hallucinations, which are spans of text that have been extracted but are not in the sentence.\n",
    "        The first column ('light_allucinations') contains the invented entities that are similar to at least one entity in the sentence, while the second column ('heavy_allucinations') contains the invented entities that are not similar to the entities in the sentence.\n",
    "       \n",
    "        Args:\n",
    "        data (list): the dataset containing the model output\n",
    "        verbose (bool): default False. If True, print verbose\n",
    "        \"\"\"\n",
    "        light_allucinations, heavy_allucinations = [], []\n",
    "        for el in data:\n",
    "            light_invented_entities = []\n",
    "            heavy_invented_entities = []\n",
    "            for extracted_entity in el['model_output_parsed']['entities']:\n",
    "                if extracted_entity.lower() not in el['sentence'].lower():\n",
    "                    if verbose: print(f\"'{extracted_entity}' not in sentence...\")\n",
    "                    if len(extracted_entity.split())==1:\n",
    "                        possible_entities = el['sentence'].split()\n",
    "                    else:\n",
    "                        n_words_in_entity= len(extracted_entity.split())\n",
    "                        possible_entities = [' '.join(el['sentence'].split()[i:i+n_words_in_entity]) for i in range(len(el['sentence'].split())-(n_words_in_entity-1))]\n",
    "                    if verbose: print(f'looking through {possible_entities}...')\n",
    "                    similarities = [fuzz.ratio(extracted_entity, possible_similar_entity) > 80\n",
    "                                    for possible_similar_entity in possible_entities]\n",
    "                    if any(similarities):\n",
    "                        if verbose: print('SIMILARITY FOUND', extracted_entity, '||||', el['sentence'].split()[similarities.index(True):similarities.index(True)+len(extracted_entity.split())])\n",
    "                        light_invented_entities.append({'extracted_entity':extracted_entity, 'original_entity':el['sentence'].split()[similarities.index(True)], 'original_sentence':el['sentence']})  \n",
    "                    else:\n",
    "                        heavy_invented_entities.append({'extracted_entity':extracted_entity, 'original_sentence':el['sentence']})\n",
    "                    if verbose: print('\\n')\n",
    "\n",
    "            light_allucinations.append(light_invented_entities)\n",
    "            heavy_allucinations.append(heavy_invented_entities)\n",
    "        data = data.add_column('light_allucinations', light_allucinations)\n",
    "        data = data.add_column('heavy_allucinations', heavy_allucinations)\n",
    "        return data\n",
    "\n",
    "    def remove_allucinations_from_computation(self, data_with_allucination_col, only_heavy:bool=True):\n",
    "        \"\"\"\n",
    "        Remove the invented entities/hallucinations, which are spans of text that have been extracted but are not in the sentence.\n",
    "        This function creates a new column called 'model_output_parsed' that contains the model output without the invented entities.\n",
    "        \"\"\"\n",
    "        if 'heavy_allucinations' not in data_with_allucination_col.column_names:\n",
    "            data_with_allucination_col = self.create_allucinations_columns(data_with_allucination_col)\n",
    "        \n",
    "        def helper(example):\n",
    "            if only_heavy:\n",
    "                if (example['heavy_allucinations'] == []):\n",
    "                    return example\n",
    "                else:\n",
    "                    for el in example['heavy_allucinations']:\n",
    "                        # print('REMOVING: ', el['extracted_entity'])\n",
    "                        example['model_output_parsed']['entities'].remove(el['extracted_entity'])\n",
    "                    return example\n",
    "            else:\n",
    "                if (example['light_allucinations'] == []) and (example['heavy_allucinations'] == []):\n",
    "                    return example\n",
    "                else:\n",
    "                    for el in example['light_allucinations']:\n",
    "                        # print('REMOVING: ', el['extracted_entity'])\n",
    "                        example['model_output_parsed']['entities'].remove(el['extracted_entity'])\n",
    "                    return example\n",
    "        data_with_allucination_col = data_with_allucination_col.map(helper)\n",
    "        return data_with_allucination_col\n",
    "    \n",
    "    def _assess_model_output(self, model_response: str) -> (bool, str):\n",
    "        \"\"\"\n",
    "        Check if the model output is in the right format. If not, return False.\n",
    "        \n",
    "        Args:\n",
    "        model_output (str): the postprocessed model output after beeing passed to _postprocess_model_output()\n",
    "\n",
    "        return:\n",
    "        bool: True if the format is correct, False otherwise\n",
    "        str: the model output in the adjusted format\n",
    "        \"\"\"\n",
    "        good_format = True\n",
    "        # if self.cleaner.verbose: print('prima sostituz:  ', model_response)\n",
    "        model_response = model_response.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\": \\'\", \": \\\"\")\n",
    "        model_response = re.sub(r'(?<=[a-zA-Z])\"(?=[a-zA-Z])', \"'\", model_response)\n",
    "        # if self.cleaner.verbose: print('dopo sostituz: ', model_response)\n",
    "        try :\n",
    "            out = json.loads(model_response)\n",
    "            if isinstance(out, dict):\n",
    "                model_response = '[' + model_response + ']'\n",
    "        except Exception as error:\n",
    "            print('ERROR: ', error)\n",
    "            if hasattr(error, 'msg'):\n",
    "                if error.msg.startswith('Expecting property name enclosed in double quotes'):\n",
    "                    model_response = model_response.replace(\"{\\'\", \"{\\\"\").replace(\"\\'}\", \"\\\"}\").replace(\"\\'ent\", \"\\\"ent\").replace(\"ty\\'\", \"ty\\\"\").replace(\": \\'\", \": \\\"\")\n",
    "                    try:\n",
    "                        out = json.loads(model_response)\n",
    "                        if isinstance(out, dict):\n",
    "                            model_response = '[' + model_response + ']'\n",
    "                            good_format = True\n",
    "                    except Exception as error2:\n",
    "                        if isinstance(error2, json.decoder.JSONDecodeError):\n",
    "                            if error2.msg == \"Expecting ',' delimiter\":\n",
    "                                model_response = self._change_apexes(model_response)\n",
    "                                good_format = True\n",
    "            else:\n",
    "                #print('MODEL RESPNSE: ', model_response)\n",
    "                good_format = False\n",
    "        if not good_format:\n",
    "            model_response = re.findall(r'\\[\\{(.+?)\\}\\]', model_response)\n",
    "            if len(model_response) != 0:\n",
    "                model_response = '[{' + model_response[0] + '}]'\n",
    "                good_format = True\n",
    "                try :\n",
    "                    json.loads(model_response)\n",
    "                except Exception as error:\n",
    "                    good_format = False\n",
    "            else:\n",
    "                good_format = False\n",
    "        return good_format, model_response\n",
    "\n",
    "    def _parse_json(self, model_response: str, drop_duplicates: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Parse the model output to extract the entities and their offsets if present.\n",
    "        \n",
    "        Args:\n",
    "        model_response (str): the model response \n",
    "        drop_duplicates (bool): if True, drop the duplicates in the model response\n",
    "        \"\"\"\n",
    "        # print('MODEL RESPONSE 1: ', model_response)\n",
    "        model_response = model_response.replace(\"\\n\", \" \")\n",
    "\n",
    "        good_format, model_response = self._assess_model_output(model_response)\n",
    "\n",
    "        # print('MODEL RESPONSE 2: ', model_response)        \n",
    "        # print('MODEL RESPONSE 3: ', model_response)\n",
    "        if model_response == []:\n",
    "            model_response = '[{\"entity\":\"\"}]'\n",
    "        if good_format:\n",
    "            # print('MODEL RESPONSE 4: ', model_response)\n",
    "            # print('ORA STO PARSANDO: ', model_response)\n",
    "            output = json.loads(model_response)\n",
    "            # print('OUTPUT: ', type(output))\n",
    "            if drop_duplicates:\n",
    "                output = self._drop_duplicates(output)\n",
    "            entities = [entity[\"entity\"] for entity in output]\n",
    "            # print('ENTITIES: ', entities)\n",
    "            return {\"entities\": entities}\n",
    "        if not good_format:\n",
    "            return {\"entities\": []}\n",
    "    \n",
    "    def count_repetitions_in_extraction(self):\n",
    "        \"\"\"\n",
    "        Count the number of times an entity is extracted multiple times in the model output.\n",
    "        Args:\n",
    "        data (list): the dataset containing the model output. \n",
    "        \"\"\"\n",
    "        duplications = 0\n",
    "        total_extracted = 0\n",
    "        extracted_deduplicated = 0\n",
    "        for el in self.data['model_output']:\n",
    "            ent_list = self._parse_json(el, drop_duplicates=False)['entities']\n",
    "            n_duplicated = len(ent_list) - len(set(ent_list)) \n",
    "            duplications += n_duplicated\n",
    "            total_extracted += len(ent_list)\n",
    "            extracted_deduplicated += len(set(ent_list))\n",
    "        return {\"n_duplicated_ent\":duplications, \"n_extracted_ent\":total_extracted, \"n_extracted_deduplicated_ent\":extracted_deduplicated}\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 681/681 [00:00<00:00, 12095.47 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation':      TP  FP  FN\n",
       " 0     5   0   9\n",
       " 1     4   0   0\n",
       " 2     9   0   5\n",
       " 3     5   0   1\n",
       " 4     2   4   0\n",
       " ..   ..  ..  ..\n",
       " 676   4   1   0\n",
       " 677   4   0   1\n",
       " 678   5   1   0\n",
       " 679  12   4   1\n",
       " 680   4   1   4\n",
       " \n",
       " [681 rows x 3 columns],\n",
       " 'precision': 0.6738070616043729,\n",
       " 'recall': 0.7008297480024586,\n",
       " 'f1': 0.6870527980718536}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "#from utils.evaluator import Evaluator\n",
    "from utils.output_cleaner import OutputCleaner\n",
    "file =  data_path\n",
    "eval_data = Dataset.from_csv(file) \n",
    "\n",
    "output_cleaner = OutputCleaner(verbose=False)\n",
    "similar_is_equal = True\n",
    "similar_is_equal_threshold = 100\n",
    "cleaned_data = output_cleaner.apply_cleaning(eval_data, wrong_keys_to_entity=False) #.select(range(12,13))\n",
    "\n",
    "evaluator = Evaluator(data=cleaned_data, offset=False, output_cleaner=cleaned_data)\n",
    "evaluator.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, similarity_types=['case', 'subset', 'superset'])\n",
    "\n",
    "data_parsed = evaluator.data.map(lambda x: {'model_output_parsed':evaluator._parse_json(x['model_output']),\n",
    "                                            'ground_truth_parsed':evaluator._parse_json(x['ground_truth'])})\n",
    "evaluator.evaluation_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vediamo quante volte succede che il modello inventa entità che non son onel testo originario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 117 invented entities over 4589 extracted entities -> 2.5%\n"
     ]
    }
   ],
   "source": [
    "output_analist = OutputAnalist(data_parsed)\n",
    "\n",
    "allucinations = []\n",
    "for el in output_analist.data:\n",
    "    invented_entities = []\n",
    "    for extracted_entity in el['model_output_parsed']['entities']:\n",
    "        if extracted_entity.lower() not in el['sentence'].lower():\n",
    "            #print('EXTRACTED ENTITY NOT IN sentence: ', extracted_entity, '||||', el['sentence'])\n",
    "            invented_entities.append(extracted_entity)\n",
    "    allucinations.append(invented_entities)\n",
    "len1 = len([el for sublist in allucinations for el in sublist])\n",
    "len2 = len([el for sublist in output_analist.data['model_output_parsed'] for el in sublist['entities']])\n",
    "print(f\"There are {len1} invented entities over {len2} extracted entities -> {round(len1/len2*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quante di queste allucinazioni sono però molto simili a qualcosa che c'è nel testo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 76 heavy allucinations over 117 allucinations -> 65.0%\n",
      "61 sentences are impacted by allucination out of 681 -> 11.2%\n"
     ]
    }
   ],
   "source": [
    "data_with_all_col = output_analist.create_allucinations_columns(data_parsed, verbose = False)\n",
    "\n",
    "len1 = len([el for sublist in allucinations for el in sublist])\n",
    "len2 = len([el for sublist in data_with_all_col['heavy_allucinations'] for el in sublist])\n",
    "print(f\"There are {len2} heavy allucinations over {len1} allucinations -> {round(len2/len1*100,1)}%\")\n",
    "print(f\"{len([sublist for sublist in data_with_all_col['heavy_allucinations'] if len(sublist)>0])} sentences are impacted by allucination out of 681 -> {round(len2/681*100,1)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### quando il modello è allucinato le performances peggiorano? Non solo quello è sbagliato, ma magari anche le altre fanno casino..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO allucinations -> f1:0.695427538103849 recall: 0.7049581005586593, precision 0.6861512319456244\n",
      "   allucinations -> f1:0.6286057692307692 recall: 0.6705128205128205, precision 0.5916289592760181\n"
     ]
    }
   ],
   "source": [
    "data_allucinated = data_with_all_col.filter(lambda x: len(x['heavy_allucinations'])>0)\n",
    "evaluator_allucinations = Evaluator(data=data_allucinated, offset=False, output_cleaner=output_cleaner)\n",
    "evaluator_allucinations.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, similarity_types=['case', 'subset', 'superset'])\n",
    "evaluator_sentences_with_no_allucinations = Evaluator(data=data_with_all_col.filter(lambda x: len(x['heavy_allucinations'])==0), offset=False, output_cleaner=output_cleaner)\n",
    "evaluator_sentences_with_no_allucinations.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, similarity_types=['case', 'subset', 'superset'])\n",
    "print(f\"NO allucinations -> f1:{evaluator_sentences_with_no_allucinations.evaluation_table['f1']} recall: {evaluator_sentences_with_no_allucinations.evaluation_table['recall']}, precision {evaluator_sentences_with_no_allucinations.evaluation_table['precision']}\")\n",
    "print(f\"   allucinations -> f1:{evaluator_allucinations.evaluation_table['f1']} recall: {evaluator_allucinations.evaluation_table['recall']}, precision {evaluator_allucinations.evaluation_table['precision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cerchiamo di capire quale sia essere l'impatto delle allucinazioni, cioè cosa succede se le togliamo dal computo e laciamo il resto invariato. Per esempio, se le estratte sono 'Pietro' 'ferrazzi' e la frase originale è 'Pietro sta programmando', normalmente conteggio 'ferrazzi' come FP. Qui voglio vedere se escludendolo dal conteggio le performance sono comunque peggiori. In altre parole, voglio vedere se un'allucinazione ha l'effetto di modifcare anche quello che succede intorno ad essa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo è il confronto tra avere allucinazioni e dopo averle tolte considerando soltanto le frasi per cui sono state generate allucinazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allucinations removed -> f1:0.6591661151555261 recall: 0.6604774535809018, precision 0.6578599735799208\n",
      "allucinations         -> f1:0.6286057692307692 recall: 0.6705128205128205, precision 0.5916289592760181\n"
     ]
    }
   ],
   "source": [
    "output_analist = OutputAnalist(tmp)\n",
    "data_allucinated_removed = output_analist.remove_allucinations_from_computation(data_allucinated)\n",
    "\n",
    "evaluator_marginal_allucinations_removed = Evaluator(data_allucinated_removed, offset=False, output_cleaner=None)\n",
    "evaluator_marginal_allucinations_removed.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, similarity_types=['case', 'subset', 'superset'], already_parsed_inputs=True)\n",
    "print(f\"allucinations removed -> f1:{evaluator_marginal_allucinations_removed.evaluation_table['f1']} recall: {evaluator_marginal_allucinations_removed.evaluation_table['recall']}, precision {evaluator_marginal_allucinations_removed.evaluation_table['precision']}\")\n",
    "\n",
    "evaluator_marginal_allucinations = Evaluator(data_allucinated, offset=False, output_cleaner=None)\n",
    "evaluator_marginal_allucinations.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, similarity_types=['case', 'subset', 'superset'], already_parsed_inputs=False)\n",
    "print(f\"allucinations         -> f1:{evaluator_marginal_allucinations.evaluation_table['f1']} recall: {evaluator_marginal_allucinations.evaluation_table['recall']}, precision {evaluator_marginal_allucinations.evaluation_table['precision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sembra che le allucinazioni rendano le performances leggermente peggiori, anche una volta che le marginallizzi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "un allucinazione è un FP. \n",
    "H0: p( TN | allucinazione ) < p( TN | ! allucinazione)  [cioè, il fatto che ci siano delle allucinazioni è correlato alla miglior comprensione del contesto da parte del modello, che 'esagera' a generare positivi, ma non sbaglia più i negativi]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "da qui si vede che in realtà non cambia molto -> le allucinazioni non sembrano impattare in maniera importante il resto del testo generato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "summary_all_removed = evaluator_marginal_allucinations_removed.evaluation_table['evaluation'].sum() / evaluator_marginal_allucinations_removed.data.num_rows\n",
    "summary_NO_all = evaluator_sentences_with_no_allucinations.evaluation_table['evaluation'].sum() / evaluator_sentences_with_no_allucinations.data.num_rows\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "labels = ['TP', 'FP', 'FN']\n",
    "width = 0.35\n",
    "x = range(len(labels))\n",
    "rects1 = ax.bar(x, summary_all_removed, width, label='allucinations removed')\n",
    "rects2 = ax.bar([i + width for i in x], summary_NO_all, width, label='NO all')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('TP, FP, FN (per sentence) for allucinated and non allucinated sentences')\n",
    "ax.set_xticks([i + width/2 for i in x])\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vediamo cosa succede se tolgo dalle entità estratte quelle allucinate, e valuto le performances come se il modello quelle non le avesse estratte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light allucinations removed -> f1:0.6911910009880672\n",
      "heavy allucinations removed -> f1:0.6885841981668055 \n",
      "no allucination removed     -> f1:0.6870527980718536\n"
     ]
    }
   ],
   "source": [
    "output_analist = OutputAnalist(data_parsed)\n",
    "data_allucinated_removed_only_heavy = output_analist.remove_allucinations_from_computation(data_parsed, only_heavy=False)\n",
    "data_allucinated_removed_light = output_analist.remove_allucinations_from_computation(data_parsed, only_heavy=True)\n",
    "\n",
    "\n",
    "evaluator_light = Evaluator(data=data_allucinated_removed_light, offset=False, output_cleaner=None)\n",
    "evaluator_light.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                    words_level=True, \n",
    "                                    similarity_types=['case', 'subset', 'superset'],\n",
    "                                    already_parsed_inputs=True,\n",
    "                                    add_TP_FP_TN_FN_to_data=True)\n",
    "\n",
    "evaluator_heavy =  Evaluator(data_allucinated_removed_only_heavy, offset=False, output_cleaner=None)\n",
    "evaluator_heavy.generate_evaluation_table(similar_is_equal_threshold=100,\n",
    "                                                                   words_level=True, \n",
    "                                                                   similarity_types=['case', 'subset', 'superset'], \n",
    "                                                                   already_parsed_inputs=True,\n",
    "                                                                   add_TP_FP_TN_FN_to_data=True\n",
    "                                                                   )\n",
    "print(f\"light allucinations removed -> f1:{evaluator_light.evaluation_table['f1']}\")\n",
    "print(f\"heavy allucinations removed -> f1:{evaluator_heavy.evaluation_table['f1']} \")\n",
    "print(f\"no allucination removed     -> f1:{ evaluator.evaluation_table['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### guardiamo quante volte è successo che entità duplicate (perfect match) sono state estratte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_duplicated_ent: 2536, n_extracted_ent: 7125, n_extracted_deduplicated_ent: 4589\n",
      " the percentage of duplicated entities over the total of extracted is 35.6%\n",
      " the percentage of duplicated entities over the total of valid entities is 64.4%\n"
     ]
    }
   ],
   "source": [
    "res = output_analist.count_repetitions_in_extraction()\n",
    "print(f\"n_duplicated_ent: {res['n_duplicated_ent']}, n_extracted_ent: {res['n_extracted_ent']}, n_extracted_deduplicated_ent: {res['n_extracted_deduplicated_ent']}\\n the percentage of duplicated entities over the total of extracted is {round(res['n_duplicated_ent']/res['n_extracted_ent']*100,1)}%\\n the percentage of duplicated entities over the total of valid entities is {round(res['n_extracted_deduplicated_ent']/res['n_extracted_ent']*100,1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOINT ANALYSIS with seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.11k/1.11k [00:00<00:00, 6.98MB/s]\n",
      "Downloading data: 100%|██████████| 333k/333k [00:00<00:00, 743kB/s]\n",
      "Generating test split: 100%|██████████| 681/681 [00:00<00:00, 63478.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "checkpoint = \"ferrazzipietro/bestResults_LS_Llama-2-7b-hf_adapters_en.layer1_NoQuant_16_32_0.05_2_0.0002_3EpochsLast_eval\"\n",
    "data_seq2seq = load_dataset(checkpoint, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "summary_gen = output_analist.data.to_pandas()[['TP', 'FP', 'FN']].sum() / output_analist.data.num_rows\n",
    "summary_seq2seq = data_seq2seq.to_pandas()[['TP', 'FP', 'FN']].sum() / data_seq2seq.num_rows\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "labels = ['TP', 'FP', 'FN']\n",
    "width = 0.35\n",
    "x = range(len(labels))\n",
    "rects1 = ax.bar(x, summary_gen, width, label='gen')\n",
    "rects2 = ax.bar([i + width for i in x], summary_seq2seq, width, label='s2s')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('TP, FP, FN (per sentence) for gen and seq2seq')\n",
    "ax.set_xticks([i + width/2 for i in x])\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- togliere le allucinazioni fa fare le performance dell'altro s2s? Se sì, allora le allucinazioni sono IL problema, altrimenti è il modello\n",
    "- grafico lunghezza in token delle entità estratte e relativo f1\n",
    "- rapport tra allucinazioni e FP. \n",
    "- le allucinazioni da dove vengono fuori? Sono sinonimi, acronomi di entità vere? le all. sono risolvibili o no?\n",
    "- capita che si siano allcuniazioni intra parola (es. \"covid-20\" invece di \"covid-19\")?\n",
    "\n",
    "- test significatività f1 vs f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
