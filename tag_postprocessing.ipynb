{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietroferrazzi/Desktop/dottorato/mistral_finetuning/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate_ft_adapters_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_ft_adapters_list\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "from utils.generate_ft_adapters_list import generate_ft_adapters_list\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import postprocessing_params_llama as postprocessing\n",
    "from log import tag_llama7B_NoQuant as models_params\n",
    "adapters_list = generate_ft_adapters_list(\"tag_llama7B_NoQuant\", simplest_prompt=models_params.simplest_prompt)\n",
    "print(adapters_list)\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "\n",
    "max_new_tokens_factor_list = postprocessing.max_new_tokens_factor_list\n",
    "n_shots_inference_list = postprocessing.n_shots_inference_list\n",
    "layer = models_params.TRAIN_LAYER\n",
    "language = layer.split('.')[0]\n",
    "\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n",
    "tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, add_eos_token=False,\n",
    "                                         token=LLAMA_TOKEN)\n",
    "preprocessor = DataPreprocessor(model_checkpoint=models_params.BASE_MODEL_CHECKPOINT, \n",
    "                                tokenizer = tokenizer)\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset,\n",
    "                                                 models_params.instruction_on_response_format)\n",
    "_, val_data, _ = preprocessor.split_layer_into_train_val_test_(dataset, layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for max_new_tokens_factor in max_new_tokens_factor_list:\n",
    "    for n_shots_inference in n_shots_inference_list:\n",
    "        for adapters in tqdm(adapters_list, desc=\"adapters_list\"):\n",
    "            if adapters.endswith(\"0.0008\"):\n",
    "                continue\n",
    "            print(\"PROCESSING:\", adapters, \"n_shots_inference:\", n_shots_inference, \"max_new_tokens_factor:\", max_new_tokens_factor)\n",
    "            if not models_params.quantization:\n",
    "                print(\"NO QUANTIZATION\")\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    models_params.BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "                    return_dict=True,  \n",
    "                    torch_dtype=postprocessing.torch_dtype,\n",
    "                    device_map= \"auto\",\n",
    "                    token=LLAMA_TOKEN)    \n",
    "            else:\n",
    "                print(\"QUANTIZATION\")\n",
    "                load_in_8bit = not models_params.load_in_4bit[0]\n",
    "                load_in_4bit = models_params.load_in_4bit[0]\n",
    "                load_in_8bit = not load_in_4bit\n",
    "                bnb_4bit_use_double_quant = models_params.bnb_4bit_use_double_quant\n",
    "                bnb_4bit_quant_type = models_params.bnb_4bit_quant_type[0]\n",
    "                bnb_4bit_compute_dtype = models_params.bnb_4bit_compute_dtype[0]\n",
    "                llm_int8_threshold = models_params.llm_int8_threshold[0]\n",
    "                # llm_int8_has_fp16_weight = models_params.llm_int8_has_fp16_weight AVOID IT AT INFERENCE TIME!\n",
    "                # llm_int8_skip_modules = models_params.llm_int8_skip_modules AVOID IT AT INFERENCE TIME!\n",
    "\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                            load_in_4bit=load_in_4bit,\n",
    "                            load_in_8bit=load_in_8bit,\n",
    "                            bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    "                            bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "                            bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "                            llm_int8_threshold=llm_int8_threshold ,\n",
    "                            # llm_int8_has_fp16_weight =True #,AVOID IT AT INFERENCE TIME!\n",
    "                            # llm_int8_skip_modules=llm_int8_skip_modules AVOID IT AT INFERENCE TIME!\n",
    "                            )\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    models_params.BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "                    quantization_config = bnb_config,\n",
    "                    return_dict=True,  \n",
    "                    device_map= \"auto\",\n",
    "                    token=LLAMA_TOKEN)\n",
    "            merged_model = PeftModel.from_pretrained(base_model, \n",
    "                                                     adapters, \n",
    "                                                     token=HF_TOKEN, \n",
    "                                                     device_map='auto',\n",
    "                                                     is_trainable = False)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, \n",
    "                                                      add_eos_token=False,\n",
    "                                                      token=LLAMA_TOKEN)\n",
    "            tokenizer.pad_token = tokenizer.eos_token# \"<pad>\" #tokenizer.eos_token\n",
    "            tokenizer.padding_side = \"left\"\n",
    "#            tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, add_eos_token=True, token=LLAMA_TOKEN)\n",
    "#            tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "#            merged_model.resize_token_embeddings(len(tokenizer))\n",
    "#            print('tokenizer.pad_token_id:', tokenizer.pad_token_id)\n",
    "#            merged_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "            postprocessor = TestDataProcessor(test_data=val_data, \n",
    "                                              preprocessor=preprocessor, \n",
    "                                              n_shots_inference=n_shots_inference, \n",
    "                                              language=language, \n",
    "                                              tokenizer=tokenizer)\n",
    "            postprocessor.add_inference_prompt_column(simplest_prompt=False)\n",
    "\n",
    "            # tmp = []\n",
    "            # for example in postprocessor.test_data:\n",
    "            #     tmp.append(example)\n",
    "            # import pandas as pd\n",
    "            # tmp = pd.DataFrame(tmp)\n",
    "            # tmp = tmp.iloc[tmp['inference_prompt'].str.len().argsort()]\n",
    "            # postprocessor.test_data = Dataset.from_pandas(tmp)\n",
    "\n",
    "            postprocessor.add_ground_truth_column()\n",
    "            #try:\n",
    "            postprocessor.add_responses_column(model=merged_model, \n",
    "                                            tokenizer=tokenizer, \n",
    "                                            batch_size=postprocessing.batch_size, \n",
    "                                            max_new_tokens_factor=max_new_tokens_factor)\n",
    "            postprocessor.test_data.to_csv(f\"{postprocessing.save_directory}maxNewTokensFactor{max_new_tokens_factor}_nShotsInference{n_shots_inference}_{adapters.split('/')[1]}.csv\", index=False)\n",
    "            # except RuntimeError as e:\n",
    "                # print(\"ERROR IN PROCESSING: \", e, adapters)\n",
    "                # print(e.message)\n",
    "            del merged_model\n",
    "            if models_params.quantization: del base_model\n",
    "            del tokenizer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
