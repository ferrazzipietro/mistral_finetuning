{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_TAG_3EPOCHS_16_32_0.01_2_0.0002', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_TAG_3EPOCHS_16_64_0.01_2_0.0002', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_TAG_3EPOCHS_32_32_0.01_2_0.0002', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_TAG_3EPOCHS_32_64_0.01_2_0.0002']\n",
      "MODEL TYPE: llama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55908aa61d046eb92e44a7a5358521e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3620ee610ebd4ef9b962e69fa8877a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d420257fa88d44cd847ba8c97c9a8133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa716401b2774e599ff341445c158fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessorTag\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "from utils.generate_ft_adapters_list import generate_ft_adapters_list\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import postprocessing_params_llama as postprocessing\n",
    "from log import tag_llama7B_NoQuant as models_params\n",
    "adapters_list = generate_ft_adapters_list(\"tag_llama7B_NoQuant\", simplest_prompt=models_params.simplest_prompt)\n",
    "print(adapters_list)\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "\n",
    "max_new_tokens_factor_list = postprocessing.max_new_tokens_factor_list\n",
    "n_shots_inference_list = postprocessing.n_shots_inference_list\n",
    "layer = models_params.TRAIN_LAYER\n",
    "language = layer.split('.')[0]\n",
    "\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n",
    "tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, add_eos_token=False,\n",
    "                                         token=LLAMA_TOKEN)\n",
    "\n",
    "\n",
    "preprocessor = DataPreprocessorTag(models_params.BASE_MODEL_CHECKPOINT, \n",
    "                                     tokenizer, token_llama=HF_TOKEN, \n",
    "                                     data =dataset, \n",
    "                                     tagging_string=models_params.tagging_string)\n",
    "preprocessor.apply(instruction_on_response_format=models_params.instruction_on_response_format)\n",
    "dataset = preprocessor.data.map(lambda samples: tokenizer(samples[models_params.dataset_text_field]), batched=True)\n",
    "train_data, val_data, test_data = preprocessor.split_layer_into_train_val_test_(dataset, models_params.TRAIN_LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332bec3bc5e84664a1d5ed877ada7bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36da4269f0748f8b0c30396485d732c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda36ecd6379414d8998aac0c66db201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/49.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafed4bd37da4653bc58ddf6940e9a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164cfd4ab6f148dc8f70e40d5827668e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating responses:   0%|          | 0/8 [00:00<?, ?it/s]/home/pferrazzi/mistral_finetuning/.venv/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "generating responses: 100%|██████████| 8/8 [02:40<00:00, 20.09s/it]\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True\n",
    "            )\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    models_params.BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "    quantization_config = bnb_config,\n",
    "    return_dict=True,  \n",
    "    device_map= \"auto\",\n",
    "    token=LLAMA_TOKEN,\n",
    "    cache_dir='/data/disk1/share/pferrazzi/.cache')\n",
    "adapters = adapters_list[0]\n",
    "merged_model = PeftModel.from_pretrained(base_model, adapters, token=HF_TOKEN, \n",
    "                                         device_map='auto',\n",
    "                                         is_trainable = False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, \n",
    "                                          add_eos_token=False,\n",
    "                                          token=LLAMA_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token# \"<pad>\" #tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "postprocessor = TestDataProcessor(test_data=val_data.select(range(8)), \n",
    "                                  preprocessor=preprocessor, \n",
    "                                  n_shots_inference=0, \n",
    "                                  language=language, \n",
    "                                  tokenizer=tokenizer)\n",
    "postprocessor.add_inference_prompt_column(simplest_prompt=False)\n",
    "\n",
    "\n",
    "postprocessor.add_ground_truth_column()\n",
    "            #try:\n",
    "postprocessor.add_responses_column(model=merged_model, \n",
    "                                tokenizer=tokenizer, \n",
    "                                batch_size=4, \n",
    "                                max_new_tokens_factor=6)\n",
    "# postprocessor.test_data.to_csv(f\"{postprocessing.save_directory}maxNewTokensFactor{6}_nShotsInference{0}_{adapters.split('/')[1]}.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.',\n",
       " 'entities': [{'id': '1764',\n",
       "   'offsets': [24, 32],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'findings',\n",
       "   'type': 'EVENT'},\n",
       "  {'id': '1779',\n",
       "   'offsets': [33, 40],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'pointed',\n",
       "   'type': 'EVENT'},\n",
       "  {'id': '2010',\n",
       "   'offsets': [57, 89],\n",
       "   'role': '',\n",
       "   'semantic_type_id': 'C1306459',\n",
       "   'text': 'primary right parotid malignancy',\n",
       "   'type': 'CLINENTITY'},\n",
       "  {'id': '2017',\n",
       "   'offsets': [95, 111],\n",
       "   'role': '',\n",
       "   'semantic_type_id': 'C0494165',\n",
       "   'text': 'liver metastases',\n",
       "   'type': 'CLINENTITY'},\n",
       "  {'id': '2052',\n",
       "   'offsets': [57, 89],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'primary right parotid malignancy',\n",
       "   'type': 'BODYPART'},\n",
       "  {'id': '2058',\n",
       "   'offsets': [95, 111],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'liver metastases',\n",
       "   'type': 'BODYPART'}],\n",
       " 'original_text': 'A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia. Hormonal study and dynamic biochemical tests performed indicated ECS. Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases. Somatostatin receptor scintigraphy has shown an increased uptake in the parotid gland and mild expression in liver metastasis. The patient underwent right parotidectomy, and histopathologic examination confirmed ACC. Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide. Despite chemotherapy onset, a rapid disease progression and clinical course deterioration was observed.\\r\\n',\n",
       " 'original_id': 'EN101783',\n",
       " 'ground_truth': 'Imaging and cytological <tag>findings</tag> <tag>pointed</tag> toward a likely <tag>primary right parotid malignancy</tag> with <tag>liver metastases</tag>.</s>',\n",
       " 'prompt': '<s>[INST] Extract the entities contained in the text, inserting the tag <tag> before and the the tag </tag> after each entity. <<Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.>>> [/INST]Imaging and cytological <tag>findings</tag> <tag>pointed</tag> toward a likely <tag>primary right parotid malignancy</tag> with <tag>liver metastases</tag>.</s>',\n",
       " 'input_ids': [1,\n",
       "  1,\n",
       "  518,\n",
       "  25580,\n",
       "  29962,\n",
       "  7338,\n",
       "  1461,\n",
       "  278,\n",
       "  16212,\n",
       "  11122,\n",
       "  297,\n",
       "  278,\n",
       "  1426,\n",
       "  29892,\n",
       "  23800,\n",
       "  278,\n",
       "  4055,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  1434,\n",
       "  322,\n",
       "  278,\n",
       "  278,\n",
       "  4055,\n",
       "  1533,\n",
       "  4039,\n",
       "  29958,\n",
       "  1156,\n",
       "  1269,\n",
       "  7855,\n",
       "  29889,\n",
       "  3532,\n",
       "  1888,\n",
       "  6751,\n",
       "  322,\n",
       "  274,\n",
       "  3637,\n",
       "  5996,\n",
       "  1284,\n",
       "  886,\n",
       "  11520,\n",
       "  11183,\n",
       "  263,\n",
       "  5517,\n",
       "  7601,\n",
       "  1492,\n",
       "  610,\n",
       "  327,\n",
       "  333,\n",
       "  286,\n",
       "  2520,\n",
       "  6906,\n",
       "  411,\n",
       "  619,\n",
       "  369,\n",
       "  1539,\n",
       "  579,\n",
       "  2129,\n",
       "  29889,\n",
       "  6778,\n",
       "  29958,\n",
       "  518,\n",
       "  29914,\n",
       "  25580,\n",
       "  29962,\n",
       "  1888,\n",
       "  6751,\n",
       "  322,\n",
       "  274,\n",
       "  3637,\n",
       "  5996,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  2886,\n",
       "  886,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  3149,\n",
       "  287,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  11183,\n",
       "  263,\n",
       "  5517,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  16072,\n",
       "  1492,\n",
       "  610,\n",
       "  327,\n",
       "  333,\n",
       "  286,\n",
       "  2520,\n",
       "  6906,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  411,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  14381,\n",
       "  1539,\n",
       "  579,\n",
       "  2129,\n",
       "  829,\n",
       "  4039,\n",
       "  15513,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'inference_prompt': '<s>[INST] Extract the entities contained in the text, inserting the tag <tag> before and the the tag </tag> after each entity. <<Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.>>> [/INST]',\n",
       " 'model_responses': 'Imaging and cytological <tag>findings</tag> <tag>pointed</tag> toward a likely <tag>primary right parotid malignancy with liver metastases</tag>.\\n</p>\\nImaging and cytological <tag>findings</tag> <tag>pointed</tag> toward a likely <tag>primary right parotid m'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff0e1c9a26c44aab9d7179d957a5bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26616a75b054cf6bed39e03dfc6d3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating responses: 100%|██████████| 4/4 [02:41<00:00, 40.31s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "postprocessor2 = TestDataProcessor(test_data=val_data.select(range(4)), \n",
    "                                  preprocessor=preprocessor, \n",
    "                                  n_shots_inference=0, \n",
    "                                  language=language, \n",
    "                                  tokenizer=tokenizer)\n",
    "postprocessor2.add_inference_prompt_column(simplest_prompt=False)\n",
    "\n",
    "\n",
    "postprocessor2.add_ground_truth_column()\n",
    "            #try:\n",
    "postprocessor2.add_responses_column(model=merged_model, \n",
    "                                tokenizer=tokenizer, \n",
    "                                batch_size=2, \n",
    "                                max_new_tokens_factor=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
