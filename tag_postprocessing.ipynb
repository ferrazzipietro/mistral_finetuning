{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_TAG_3EPOCHS_16_32_0.01_2_0.0002', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_TAG_3EPOCHS_16_64_0.01_2_0.0002', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_TAG_3EPOCHS_32_32_0.01_2_0.0002', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_TAG_3EPOCHS_32_64_0.01_2_0.0002']\n",
      "MODEL TYPE: llama\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55908aa61d046eb92e44a7a5358521e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3620ee610ebd4ef9b962e69fa8877a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d420257fa88d44cd847ba8c97c9a8133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa716401b2774e599ff341445c158fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessorTag\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "from utils.generate_ft_adapters_list import generate_ft_adapters_list\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import postprocessing_params_llama as postprocessing\n",
    "from log import tag_llama7B_NoQuant as models_params\n",
    "adapters_list = generate_ft_adapters_list(\"tag_llama7B_NoQuant\", simplest_prompt=models_params.simplest_prompt)\n",
    "print(adapters_list)\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "\n",
    "max_new_tokens_factor_list = postprocessing.max_new_tokens_factor_list\n",
    "n_shots_inference_list = postprocessing.n_shots_inference_list\n",
    "layer = models_params.TRAIN_LAYER\n",
    "language = layer.split('.')[0]\n",
    "\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n",
    "tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, add_eos_token=False,\n",
    "                                         token=LLAMA_TOKEN)\n",
    "\n",
    "\n",
    "preprocessor = DataPreprocessorTag(models_params.BASE_MODEL_CHECKPOINT, \n",
    "                                     tokenizer, token_llama=HF_TOKEN, \n",
    "                                     data =dataset, \n",
    "                                     tagging_string=models_params.tagging_string)\n",
    "preprocessor.apply(instruction_on_response_format=models_params.instruction_on_response_format)\n",
    "dataset = preprocessor.data.map(lambda samples: tokenizer(samples[models_params.dataset_text_field]), batched=True)\n",
    "train_data, val_data, test_data = preprocessor.split_layer_into_train_val_test_(dataset, models_params.TRAIN_LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia.',\n",
       " 'entities': [{'id': '1614',\n",
       "   'offsets': [23, 35],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'hypertension',\n",
       "   'type': 'EVENT'},\n",
       "  {'id': '1629',\n",
       "   'offsets': [40, 52],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'dyslipidemia',\n",
       "   'type': 'EVENT'},\n",
       "  {'id': '1644',\n",
       "   'offsets': [53, 62],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'diagnosed',\n",
       "   'type': 'EVENT'},\n",
       "  {'id': '1659',\n",
       "   'offsets': [110, 118],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'mellitus',\n",
       "   'type': 'EVENT'},\n",
       "  {'id': '1674',\n",
       "   'offsets': [149, 157],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'referred',\n",
       "   'type': 'EVENT'},\n",
       "  {'id': '1689',\n",
       "   'offsets': [186, 197],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'hypokalemia',\n",
       "   'type': 'EVENT'},\n",
       "  {'id': '1996',\n",
       "   'offsets': [91, 118],\n",
       "   'role': '',\n",
       "   'semantic_type_id': 'C0743128',\n",
       "   'text': 'new-onset diabetes mellitus',\n",
       "   'type': 'CLINENTITY'},\n",
       "  {'id': '2076',\n",
       "   'offsets': [0, 17],\n",
       "   'role': 'PATIENT',\n",
       "   'semantic_type_id': '',\n",
       "   'text': 'A 46-year-old man',\n",
       "   'type': 'ACTOR'},\n",
       "  {'id': '2090',\n",
       "   'offsets': [63, 71],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': '4-months',\n",
       "   'type': 'TIMEX3'},\n",
       "  {'id': '2099',\n",
       "   'offsets': [128, 135],\n",
       "   'role': '',\n",
       "   'semantic_type_id': '',\n",
       "   'text': '1-month',\n",
       "   'type': 'TIMEX3'}],\n",
       " 'original_text': 'A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia. Hormonal study and dynamic biochemical tests performed indicated ECS. Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases. Somatostatin receptor scintigraphy has shown an increased uptake in the parotid gland and mild expression in liver metastasis. The patient underwent right parotidectomy, and histopathologic examination confirmed ACC. Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide. Despite chemotherapy onset, a rapid disease progression and clinical course deterioration was observed.\\r\\n',\n",
       " 'original_id': 'EN101783',\n",
       " 'ground_truth': '<tag>A 46-year-old man</tag> with <tag>hypertension</tag> and <tag>dyslipidemia</tag> <tag>diagnosed</tag> <tag>4-months</tag> before, as well as <tag>new-onset diabetes mellitus</tag> unveiled <tag>1-month</tag> earlier, was <tag>referred</tag> to emergency department for <tag>hypokalemia</tag>.',\n",
       " 'prompt': '<s>[INST] Extract the entities contained in the text, inserting the tag <tag> before and the the tag </tag> after each entity. <<A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia.>>> [/INST]<tag>A 46-year-old man</tag> with <tag>hypertension</tag> and <tag>dyslipidemia</tag> <tag>diagnosed</tag> <tag>4-months</tag> before, as well as <tag>new-onset diabetes mellitus</tag> unveiled <tag>1-month</tag> earlier, was <tag>referred</tag> to emergency department for <tag>hypokalemia</tag>.</s>',\n",
       " 'input_ids': [1,\n",
       "  1,\n",
       "  518,\n",
       "  25580,\n",
       "  29962,\n",
       "  7338,\n",
       "  1461,\n",
       "  278,\n",
       "  16212,\n",
       "  11122,\n",
       "  297,\n",
       "  278,\n",
       "  1426,\n",
       "  29892,\n",
       "  23800,\n",
       "  278,\n",
       "  4055,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  1434,\n",
       "  322,\n",
       "  278,\n",
       "  278,\n",
       "  4055,\n",
       "  1533,\n",
       "  4039,\n",
       "  29958,\n",
       "  1156,\n",
       "  1269,\n",
       "  7855,\n",
       "  29889,\n",
       "  3532,\n",
       "  29909,\n",
       "  29871,\n",
       "  29946,\n",
       "  29953,\n",
       "  29899,\n",
       "  6360,\n",
       "  29899,\n",
       "  1025,\n",
       "  767,\n",
       "  411,\n",
       "  7498,\n",
       "  10700,\n",
       "  2673,\n",
       "  322,\n",
       "  270,\n",
       "  952,\n",
       "  3466,\n",
       "  680,\n",
       "  29885,\n",
       "  423,\n",
       "  24876,\n",
       "  2662,\n",
       "  29871,\n",
       "  29946,\n",
       "  29899,\n",
       "  10874,\n",
       "  29879,\n",
       "  1434,\n",
       "  29892,\n",
       "  408,\n",
       "  1532,\n",
       "  408,\n",
       "  716,\n",
       "  29899,\n",
       "  787,\n",
       "  300,\n",
       "  652,\n",
       "  370,\n",
       "  10778,\n",
       "  286,\n",
       "  514,\n",
       "  277,\n",
       "  375,\n",
       "  443,\n",
       "  345,\n",
       "  2356,\n",
       "  29871,\n",
       "  29896,\n",
       "  29899,\n",
       "  10874,\n",
       "  8859,\n",
       "  29892,\n",
       "  471,\n",
       "  12992,\n",
       "  304,\n",
       "  11176,\n",
       "  14703,\n",
       "  14311,\n",
       "  363,\n",
       "  10163,\n",
       "  554,\n",
       "  12698,\n",
       "  423,\n",
       "  29889,\n",
       "  6778,\n",
       "  29958,\n",
       "  518,\n",
       "  29914,\n",
       "  25580,\n",
       "  29962,\n",
       "  29966,\n",
       "  4039,\n",
       "  29958,\n",
       "  29909,\n",
       "  29871,\n",
       "  29946,\n",
       "  29953,\n",
       "  29899,\n",
       "  6360,\n",
       "  29899,\n",
       "  1025,\n",
       "  767,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  411,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  5819,\n",
       "  10700,\n",
       "  2673,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  322,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  29881,\n",
       "  952,\n",
       "  3466,\n",
       "  680,\n",
       "  29885,\n",
       "  423,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  6051,\n",
       "  4211,\n",
       "  2662,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  29946,\n",
       "  29899,\n",
       "  10874,\n",
       "  29879,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  1434,\n",
       "  29892,\n",
       "  408,\n",
       "  1532,\n",
       "  408,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  1482,\n",
       "  29899,\n",
       "  787,\n",
       "  300,\n",
       "  652,\n",
       "  370,\n",
       "  10778,\n",
       "  286,\n",
       "  514,\n",
       "  277,\n",
       "  375,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  443,\n",
       "  345,\n",
       "  2356,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  29896,\n",
       "  29899,\n",
       "  10874,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  8859,\n",
       "  29892,\n",
       "  471,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  276,\n",
       "  14373,\n",
       "  829,\n",
       "  4039,\n",
       "  29958,\n",
       "  304,\n",
       "  11176,\n",
       "  14703,\n",
       "  14311,\n",
       "  363,\n",
       "  529,\n",
       "  4039,\n",
       "  29958,\n",
       "  29882,\n",
       "  1478,\n",
       "  554,\n",
       "  12698,\n",
       "  423,\n",
       "  829,\n",
       "  4039,\n",
       "  15513,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True\n",
    "            )\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    models_params.BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "    quantization_config = bnb_config,\n",
    "    return_dict=True,  \n",
    "    device_map= \"auto\",\n",
    "    token=LLAMA_TOKEN)\n",
    "adapters = adapters_list[0]\n",
    "merged_model = PeftModel.from_pretrained(base_model, adapters, token=HF_TOKEN, \n",
    "                                         device_map='auto',\n",
    "                                         is_trainable = False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, \n",
    "                                          add_eos_token=False,\n",
    "                                          token=LLAMA_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token# \"<pad>\" #tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "postprocessor = TestDataProcessor(test_data=val_data, \n",
    "                                  preprocessor=preprocessor, \n",
    "                                  n_shots_inference=0, \n",
    "                                  language=language, \n",
    "                                  tokenizer=tokenizer)\n",
    "postprocessor.add_inference_prompt_column(simplest_prompt=False)\n",
    "\n",
    "\n",
    "postprocessor.add_ground_truth_column()\n",
    "            #try:\n",
    "postprocessor.add_responses_column(model=merged_model, \n",
    "                                tokenizer=tokenizer, \n",
    "                                batch_size=postprocessing.batch_size, \n",
    "                                max_new_tokens_factor=6)\n",
    "# postprocessor.test_data.to_csv(f\"{postprocessing.save_directory}maxNewTokensFactor{6}_nShotsInference{0}_{adapters.split('/')[1]}.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for max_new_tokens_factor in max_new_tokens_factor_list:\n",
    "    for n_shots_inference in n_shots_inference_list:\n",
    "        for adapters in tqdm(adapters_list, desc=\"adapters_list\"):\n",
    "            if adapters.endswith(\"0.0008\"):\n",
    "                continue\n",
    "            print(\"PROCESSING:\", adapters, \"n_shots_inference:\", n_shots_inference, \"max_new_tokens_factor:\", max_new_tokens_factor)\n",
    "            if not models_params.quantization:\n",
    "                print(\"NO QUANTIZATION\")\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    models_params.BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "                    return_dict=True,  \n",
    "                    torch_dtype=postprocessing.torch_dtype,\n",
    "                    device_map= \"auto\",\n",
    "                    token=LLAMA_TOKEN)    \n",
    "            else:\n",
    "                print(\"QUANTIZATION\")\n",
    "                load_in_8bit = not models_params.load_in_4bit[0]\n",
    "                load_in_4bit = models_params.load_in_4bit[0]\n",
    "                load_in_8bit = not load_in_4bit\n",
    "                bnb_4bit_use_double_quant = models_params.bnb_4bit_use_double_quant\n",
    "                bnb_4bit_quant_type = models_params.bnb_4bit_quant_type[0]\n",
    "                bnb_4bit_compute_dtype = models_params.bnb_4bit_compute_dtype[0]\n",
    "                llm_int8_threshold = models_params.llm_int8_threshold[0]\n",
    "                # llm_int8_has_fp16_weight = models_params.llm_int8_has_fp16_weight AVOID IT AT INFERENCE TIME!\n",
    "                # llm_int8_skip_modules = models_params.llm_int8_skip_modules AVOID IT AT INFERENCE TIME!\n",
    "\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                            load_in_4bit=load_in_4bit,\n",
    "                            load_in_8bit=load_in_8bit,\n",
    "                            bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    "                            bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "                            bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "                            llm_int8_threshold=llm_int8_threshold ,\n",
    "                            # llm_int8_has_fp16_weight =True #,AVOID IT AT INFERENCE TIME!\n",
    "                            # llm_int8_skip_modules=llm_int8_skip_modules AVOID IT AT INFERENCE TIME!\n",
    "                            )\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    models_params.BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "                    quantization_config = bnb_config,\n",
    "                    return_dict=True,  \n",
    "                    device_map= \"auto\",\n",
    "                    token=LLAMA_TOKEN)\n",
    "            merged_model = PeftModel.from_pretrained(base_model, \n",
    "                                                     adapters, \n",
    "                                                     token=HF_TOKEN, \n",
    "                                                     device_map='auto',\n",
    "                                                     is_trainable = False)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, \n",
    "                                                      add_eos_token=False,\n",
    "                                                      token=LLAMA_TOKEN)\n",
    "            tokenizer.pad_token = tokenizer.eos_token# \"<pad>\" #tokenizer.eos_token\n",
    "            tokenizer.padding_side = \"left\"\n",
    "#            tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, add_eos_token=True, token=LLAMA_TOKEN)\n",
    "#            tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "#            merged_model.resize_token_embeddings(len(tokenizer))\n",
    "#            print('tokenizer.pad_token_id:', tokenizer.pad_token_id)\n",
    "#            merged_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "            postprocessor = TestDataProcessor(test_data=val_data, \n",
    "                                              preprocessor=preprocessor, \n",
    "                                              n_shots_inference=n_shots_inference, \n",
    "                                              language=language, \n",
    "                                              tokenizer=tokenizer)\n",
    "            postprocessor.add_inference_prompt_column(simplest_prompt=False)\n",
    "\n",
    "            # tmp = []\n",
    "            # for example in postprocessor.test_data:\n",
    "            #     tmp.append(example)\n",
    "            # import pandas as pd\n",
    "            # tmp = pd.DataFrame(tmp)\n",
    "            # tmp = tmp.iloc[tmp['inference_prompt'].str.len().argsort()]\n",
    "            # postprocessor.test_data = Dataset.from_pandas(tmp)\n",
    "\n",
    "            postprocessor.add_ground_truth_column()\n",
    "            #try:\n",
    "            postprocessor.add_responses_column(model=merged_model, \n",
    "                                            tokenizer=tokenizer, \n",
    "                                            batch_size=postprocessing.batch_size, \n",
    "                                            max_new_tokens_factor=max_new_tokens_factor)\n",
    "            postprocessor.test_data.to_csv(f\"{postprocessing.save_directory}maxNewTokensFactor{max_new_tokens_factor}_nShotsInference{n_shots_inference}_{adapters.split('/')[1]}.csv\", index=False)\n",
    "            # except RuntimeError as e:\n",
    "                # print(\"ERROR IN PROCESSING: \", e, adapters)\n",
    "                # print(e.message)\n",
    "            del merged_model\n",
    "            if models_params.quantization: del base_model\n",
    "            del tokenizer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
