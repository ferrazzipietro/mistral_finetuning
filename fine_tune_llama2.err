2024-02-27 08:41:31.121176: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-27 08:41:31.210675: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-27 08:41:35.378652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
wandb: Currently logged in as: ferrazzipietro. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/ferrazzi/.netrc
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /extra/ferrazzi/llm/mistral_finetuning/wandb/run-20240227_084150-wgggo7l4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run en.layer12024-02-27 08:41:50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ferrazzipietro/Llama-2-13b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002_simplePromptFormat
wandb: üöÄ View run at https://wandb.ai/ferrazzipietro/Llama-2-13b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002_simplePromptFormat/runs/wgggo7l4
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:01<00:03,  1.79s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:03<00:01,  1.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.51s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.58s/it]
Map:   0%|          | 0/1520 [00:00<?, ? examples/s]Map:  14%|‚ñà‚ñç        | 216/1520 [00:00<00:00, 2130.25 examples/s]Map:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 802/1520 [00:00<00:00, 4307.87 examples/s]Map:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1318/1520 [00:00<00:00, 3682.10 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 3397.90 examples/s]
Map:   0%|          | 0/1520 [00:00<?, ? examples/s]Map:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1000/1520 [00:00<00:00, 3804.43 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 3862.31 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 3443.09 examples/s]
Map:   0%|          | 0/170 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [00:00<00:00, 1954.13 examples/s]
Map:   0%|          | 0/669 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 669/669 [00:00<00:00, 3602.09 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 669/669 [00:00<00:00, 3219.59 examples/s]
Map:   0%|          | 0/681 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:00<00:00, 4102.67 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:00<00:00, 3646.24 examples/s]
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/63 [00:00<?, ?it/s]/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  2%|‚ñè         | 1/63 [00:16<16:33, 16.03s/it]  3%|‚ñé         | 2/63 [00:25<12:22, 12.17s/it]  5%|‚ñç         | 3/63 [00:33<10:24, 10.41s/it]  6%|‚ñã         | 4/63 [00:41<09:10,  9.33s/it]  8%|‚ñä         | 5/63 [00:47<07:59,  8.27s/it] 10%|‚ñâ         | 6/63 [01:01<09:31, 10.02s/it] 11%|‚ñà         | 7/63 [01:10<09:09,  9.82s/it] 13%|‚ñà‚ñé        | 8/63 [01:19<08:37,  9.41s/it] 14%|‚ñà‚ñç        | 9/63 [01:26<07:58,  8.86s/it] 16%|‚ñà‚ñå        | 10/63 [01:33<07:10,  8.12s/it]                                                16%|‚ñà‚ñå        | 10/63 [01:33<07:10,  8.12s/it] 17%|‚ñà‚ñã        | 11/63 [01:49<09:02, 10.44s/it] 19%|‚ñà‚ñâ        | 12/63 [01:58<08:40, 10.21s/it] 21%|‚ñà‚ñà        | 13/63 [02:07<08:03,  9.67s/it] 22%|‚ñà‚ñà‚ñè       | 14/63 [02:14<07:24,  9.06s/it] 24%|‚ñà‚ñà‚ñç       | 15/63 [02:21<06:38,  8.29s/it] 25%|‚ñà‚ñà‚ñå       | 16/63 [02:34<07:44,  9.88s/it] 27%|‚ñà‚ñà‚ñã       | 17/63 [02:44<07:29,  9.76s/it] 29%|‚ñà‚ñà‚ñä       | 18/63 [02:52<07:01,  9.36s/it] 30%|‚ñà‚ñà‚ñà       | 19/63 [03:00<06:28,  8.82s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [03:06<05:49,  8.12s/it]                                                32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [03:06<05:49,  8.12s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [03:15<05:53,  8.43s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [03:30<07:02, 10.31s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [03:39<06:37,  9.95s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [03:48<06:09,  9.46s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [03:55<05:38,  8.92s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [04:02<05:02,  8.19s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [04:15<05:54,  9.84s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [04:25<05:42,  9.80s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [04:34<05:20,  9.43s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [04:41<04:54,  8.91s/it]                                                48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [04:41<04:54,  8.91s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [04:48<04:22,  8.19s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [05:04<05:24, 10.46s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [05:13<05:06, 10.22s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [05:22<04:40,  9.68s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [05:29<04:12,  9.01s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [05:36<03:42,  8.23s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [05:50<04:19,  9.98s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [05:59<04:07,  9.90s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [06:08<03:48,  9.50s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [06:16<03:25,  8.94s/it]                                                63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [06:16<03:25,  8.94s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [06:22<03:00,  8.21s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [06:31<02:57,  8.46s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [06:47<03:31, 10.59s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [06:56<03:14, 10.26s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [07:05<02:54,  9.72s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [07:13<02:35,  9.15s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [07:19<02:14,  8.41s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [07:33<02:30, 10.03s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [07:42<02:18,  9.87s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [07:51<02:02,  9.44s/it]                                                79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [07:51<02:02,  9.44s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 51/63 [07:58<01:46,  8.84s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 52/63 [08:05<01:28,  8.08s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 53/63 [08:18<01:36,  9.65s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 54/63 [08:27<01:26,  9.59s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 55/63 [08:36<01:13,  9.18s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/63 [08:43<01:00,  8.70s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 57/63 [08:50<00:48,  8.04s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [09:04<00:48,  9.79s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 59/63 [09:13<00:39,  9.76s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [09:22<00:28,  9.36s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [09:22<00:28,  9.36s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 61/63 [09:29<00:17,  8.78s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 62/63 [09:36<00:08,  8.07s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [09:48<00:00,  9.34s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [09:48<00:00,  9.34s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [09:48<00:00,  9.34s/it]
adapter_model.safetensors:   0%|          | 0.00/153M [00:00<?, ?B/s]adapter_model.safetensors:   0%|          | 16.4k/153M [00:00<16:21, 156kB/s]adapter_model.safetensors:   1%|          | 770k/153M [00:00<00:39, 3.86MB/s]adapter_model.safetensors:   1%|          | 1.47M/153M [00:00<00:42, 3.57MB/s]adapter_model.safetensors:   2%|‚ñè         | 3.08M/153M [00:00<00:21, 6.92MB/s]adapter_model.safetensors:   3%|‚ñé         | 4.18M/153M [00:00<00:22, 6.74MB/s]adapter_model.safetensors:   4%|‚ñé         | 5.46M/153M [00:00<00:23, 6.42MB/s]adapter_model.safetensors:   5%|‚ñç         | 7.26M/153M [00:01<00:17, 8.38MB/s]adapter_model.safetensors:   6%|‚ñå         | 8.99M/153M [00:01<00:15, 9.51MB/s]adapter_model.safetensors:   7%|‚ñã         | 10.8M/153M [00:01<00:14, 10.1MB/s]adapter_model.safetensors:   8%|‚ñä         | 12.8M/153M [00:01<00:13, 10.6MB/s]adapter_model.safetensors:  10%|‚ñâ         | 14.7M/153M [00:01<00:12, 10.9MB/s]adapter_model.safetensors:  10%|‚ñà         | 16.0M/153M [00:02<00:20, 6.82MB/s]adapter_model.safetensors:  12%|‚ñà‚ñè        | 18.6M/153M [00:02<00:16, 8.09MB/s]adapter_model.safetensors:  13%|‚ñà‚ñé        | 19.6M/153M [00:02<00:19, 7.01MB/s]adapter_model.safetensors:  13%|‚ñà‚ñé        | 20.5M/153M [00:02<00:22, 6.00MB/s]adapter_model.safetensors:  14%|‚ñà‚ñç        | 21.5M/153M [00:03<00:22, 5.82MB/s]adapter_model.safetensors:  15%|‚ñà‚ñå        | 23.4M/153M [00:03<00:17, 7.27MB/s]adapter_model.safetensors:  16%|‚ñà‚ñã        | 25.3M/153M [00:03<00:15, 8.43MB/s]adapter_model.safetensors:  18%|‚ñà‚ñä        | 27.2M/153M [00:03<00:13, 9.33MB/s]adapter_model.safetensors:  19%|‚ñà‚ñâ        | 29.1M/153M [00:03<00:12, 9.99MB/s]adapter_model.safetensors:  20%|‚ñà‚ñà        | 31.0M/153M [00:03<00:11, 10.5MB/s]adapter_model.safetensors:  21%|‚ñà‚ñà        | 32.1M/153M [00:04<00:19, 6.18MB/s]adapter_model.safetensors:  23%|‚ñà‚ñà‚ñé       | 35.6M/153M [00:04<00:15, 7.73MB/s]adapter_model.safetensors:  24%|‚ñà‚ñà‚ñç       | 36.6M/153M [00:04<00:16, 7.28MB/s]adapter_model.safetensors:  25%|‚ñà‚ñà‚ñå       | 38.6M/153M [00:04<00:13, 8.37MB/s]adapter_model.safetensors:  26%|‚ñà‚ñà‚ñã       | 40.5M/153M [00:05<00:12, 9.14MB/s]adapter_model.safetensors:  28%|‚ñà‚ñà‚ñä       | 42.5M/153M [00:05<00:11, 9.81MB/s]adapter_model.safetensors:  29%|‚ñà‚ñà‚ñâ       | 44.4M/153M [00:05<00:10, 10.3MB/s]adapter_model.safetensors:  30%|‚ñà‚ñà‚ñà       | 46.3M/153M [00:05<00:09, 10.7MB/s]adapter_model.safetensors:  31%|‚ñà‚ñà‚ñà‚ñè      | 48.0M/153M [00:06<00:14, 7.06MB/s]adapter_model.safetensors:  34%|‚ñà‚ñà‚ñà‚ñé      | 51.6M/153M [00:06<00:10, 10.2MB/s]adapter_model.safetensors:  35%|‚ñà‚ñà‚ñà‚ñç      | 53.5M/153M [00:06<00:09, 10.5MB/s]adapter_model.safetensors:  36%|‚ñà‚ñà‚ñà‚ñå      | 55.4M/153M [00:06<00:09, 10.8MB/s]adapter_model.safetensors:  37%|‚ñà‚ñà‚ñà‚ñã      | 57.3M/153M [00:06<00:08, 11.0MB/s]adapter_model.safetensors:  39%|‚ñà‚ñà‚ñà‚ñä      | 59.2M/153M [00:06<00:08, 11.3MB/s]adapter_model.safetensors:  40%|‚ñà‚ñà‚ñà‚ñâ      | 61.2M/153M [00:07<00:08, 11.4MB/s]adapter_model.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà      | 63.1M/153M [00:07<00:07, 11.5MB/s]adapter_model.safetensors:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 64.3M/153M [00:07<00:12, 7.06MB/s]adapter_model.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 67.6M/153M [00:07<00:08, 10.0MB/s]adapter_model.safetensors:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 69.5M/153M [00:07<00:08, 10.4MB/s]adapter_model.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 71.4M/153M [00:08<00:07, 10.7MB/s]adapter_model.safetensors:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 73.3M/153M [00:08<00:07, 11.0MB/s]adapter_model.safetensors:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 75.2M/153M [00:08<00:06, 11.2MB/s]adapter_model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 77.2M/153M [00:08<00:06, 11.4MB/s]adapter_model.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 79.1M/153M [00:08<00:06, 11.5MB/s]adapter_model.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 80.3M/153M [00:09<00:09, 7.44MB/s]adapter_model.safetensors:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 83.6M/153M [00:09<00:08, 8.40MB/s]adapter_model.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 84.6M/153M [00:09<00:08, 7.82MB/s]adapter_model.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 86.6M/153M [00:09<00:07, 8.71MB/s]adapter_model.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 88.5M/153M [00:09<00:06, 9.48MB/s]adapter_model.safetensors:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 90.4M/153M [00:10<00:06, 10.1MB/s]adapter_model.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 92.3M/153M [00:10<00:05, 11.7MB/s]adapter_model.safetensors:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 93.6M/153M [00:10<00:05, 10.8MB/s]adapter_model.safetensors:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 95.3M/153M [00:10<00:05, 10.7MB/s]adapter_model.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 96.4M/153M [00:10<00:08, 6.58MB/s]adapter_model.safetensors:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 99.7M/153M [00:11<00:05, 9.58MB/s]adapter_model.safetensors:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 102M/153M [00:11<00:05, 10.1MB/s] adapter_model.safetensors:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 104M/153M [00:11<00:04, 10.5MB/s]adapter_model.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 105M/153M [00:11<00:04, 10.9MB/s]adapter_model.safetensors:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 107M/153M [00:11<00:04, 11.1MB/s]adapter_model.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 109M/153M [00:11<00:03, 11.3MB/s]adapter_model.safetensors:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 111M/153M [00:12<00:03, 11.4MB/s]adapter_model.safetensors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 112M/153M [00:12<00:08, 4.76MB/s]adapter_model.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 116M/153M [00:13<00:05, 7.19MB/s]adapter_model.safetensors:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 118M/153M [00:13<00:04, 8.01MB/s]adapter_model.safetensors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 119M/153M [00:13<00:03, 8.63MB/s]adapter_model.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 121M/153M [00:13<00:03, 9.56MB/s]adapter_model.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 123M/153M [00:13<00:02, 10.1MB/s]adapter_model.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 125M/153M [00:13<00:02, 10.5MB/s]adapter_model.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 127M/153M [00:14<00:02, 10.9MB/s]adapter_model.safetensors:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 128M/153M [00:14<00:03, 7.23MB/s]adapter_model.safetensors:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 132M/153M [00:14<00:02, 10.0MB/s]adapter_model.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 133M/153M [00:14<00:01, 10.5MB/s]adapter_model.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 135M/153M [00:14<00:01, 10.8MB/s]adapter_model.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 137M/153M [00:15<00:01, 11.1MB/s]adapter_model.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 139M/153M [00:15<00:01, 11.2MB/s]adapter_model.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 141M/153M [00:15<00:01, 11.4MB/s]adapter_model.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 143M/153M [00:15<00:00, 11.5MB/s]adapter_model.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 144M/153M [00:16<00:01, 7.03MB/s]adapter_model.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 148M/153M [00:16<00:00, 9.99MB/s]adapter_model.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 149M/153M [00:16<00:00, 10.4MB/s]adapter_model.safetensors:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 151M/153M [00:16<00:00, 10.7MB/s]adapter_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 153M/153M [00:16<00:00, 11.0MB/s]adapter_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 153M/153M [00:16<00:00, 9.03MB/s]
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.030 MB of 0.050 MB uploaded (0.003 MB deduped)wandb: - 0.030 MB of 0.050 MB uploaded (0.003 MB deduped)wandb: \ 0.050 MB of 0.050 MB uploaded (0.003 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 6.2%             
wandb: 
wandb: Run history:
wandb:                    train/epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà
wandb:              train/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà
wandb:                train/grad_norm ‚ñà‚ñà‚ñá‚ñá‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb:            train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                     train/loss ‚ñà‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:               train/total_flos ‚ñÅ
wandb:               train/train_loss ‚ñÅ‚ñÅ
wandb:            train/train_runtime ‚ñÅ‚ñÅ
wandb: train/train_samples_per_second ‚ñÅ‚ñÅ
wandb:   train/train_steps_per_second ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    train/epoch 3.0
wandb:              train/global_step 63
wandb:                train/grad_norm 0.3099
wandb:            train/learning_rate 0.0002
wandb:                     train/loss 0.45
wandb:               train/total_flos 2.127443536223232e+16
wandb:               train/train_loss 0.73737
wandb:            train/train_runtime 588.39
wandb: train/train_samples_per_second 3.411
wandb:   train/train_steps_per_second 0.107
wandb: 
wandb: üöÄ View run en.layer12024-02-27 08:41:50 at: https://wandb.ai/ferrazzipietro/Llama-2-13b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002_simplePromptFormat/runs/wgggo7l4
wandb: Ô∏è‚ö° View job at https://wandb.ai/ferrazzipietro/Llama-2-13b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002_simplePromptFormat/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MzE5MjE3OA==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240227_084150-wgggo7l4/logs
wandb: Appending key for api.wandb.ai to your netrc file: /home/ferrazzi/.netrc
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /extra/ferrazzi/llm/mistral_finetuning/wandb/run-20240227_085300-eujpeeyz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run en.layer12024-02-27 08:53:00
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ferrazzipietro/Llama-2-13b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_8_0.0002_simplePromptFormat
wandb: üöÄ View run at https://wandb.ai/ferrazzipietro/Llama-2-13b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_8_0.0002_simplePromptFormat/runs/eujpeeyz
wandb: üöÄ View run en.layer12024-02-27 08:53:00 at: https://wandb.ai/ferrazzipietro/Llama-2-13b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_8_0.0002_simplePromptFormat/runs/eujpeeyz
wandb: Ô∏è‚ö° View job at https://wandb.ai/ferrazzipietro/Llama-2-13b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_8_0.0002_simplePromptFormat/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MzQwNTE0Ng==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240227_085300-eujpeeyz/logs
Traceback (most recent call last):
  File "finetuning_v2_iterative_llama2.py", line 210, in <module>
    main(ADAPTERS_CHECKPOINT,
  File "finetuning_v2_iterative_llama2.py", line 70, in main
    model = AutoModelForCausalLM.from_pretrained(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py", line 561, in from_pretrained
    return model_class.from_pretrained(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3452, in from_pretrained
    hf_quantizer.validate_environment(device_map=device_map)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/quantizers/quantizer_bnb_8bit.py", line 86, in validate_environment
    raise ValueError(
ValueError: 
                    Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the
                    quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules
                    in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to
                    `from_pretrained`. Check
                    https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu
                    for more details.
                    
