{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def refine_output_df(res):\n",
    "\n",
    "    def clean_quantization(example):\n",
    "        if example['quantization'] == 'NoQuantbit':\n",
    "            example['quantization'] = 'NoQuant'\n",
    "        elif example['quantization'] == '4bits':\n",
    "            example['quantization'] = '4bit'\n",
    "        elif example['quantization'] == 'noInstr':\n",
    "            example['quantization'] = example['fine_tuning']\n",
    "            example['fine_tuning'] = 'unsure'\n",
    "        return example\n",
    "    \n",
    "    to_drop = ['Unnamed: 0', 'similar_is_equal_threshold', 'similar_is_equal']\n",
    "    res = res.drop([col for col in to_drop if col in res.columns], axis = 1)\n",
    "    res = res.apply(lambda x: clean_quantization(x), axis = 1)\n",
    "    return res\n",
    "\n",
    "def extract_params_from_file_name(df: pd.DataFrame):\n",
    "\n",
    "    def adjust_model_size_format(model_size_str):\n",
    "        try:\n",
    "            int(model_size_str)\n",
    "        except ValueError:\n",
    "            model_size_str = model_size_str.replace('b', '').replace('B', '')\n",
    "        return model_size_str\n",
    "    \n",
    "    def extract_if_noInstruct(string):\n",
    "        if 'noInstr' in string:\n",
    "            return 'notInstructed'\n",
    "        return 'instructed'\n",
    "\n",
    "\n",
    "    df['model_type'] = df['file'].apply(lambda x: str(x.split('/')[1]))\n",
    "    df['instructed'] = df['file'].apply(lambda x: extract_if_noInstruct(x))\n",
    "    df['model_configurations'] = df['file'].apply(lambda x: str(x.split('/')[2]))\n",
    "    if df['model_type'][0] in ['mistral', 'zefiro', 'phi3']:\n",
    "        df['model_size'] = '7'\n",
    "        if df['model_type'][0] == 'phi3':\n",
    "            df['model_size'] = '3'\n",
    "        df['quantization'] = df['model_configurations'].apply(lambda x: str(x.split('_')[0]))\n",
    "        try:\n",
    "            df['fine_tuning'] = df['model_configurations'].apply(lambda x: str(x.split('_')[1]))\n",
    "        except IndexError:\n",
    "            df['fine_tuning'] = 'FT'\n",
    "    else:\n",
    "        df['model_size'] = df['model_configurations'].apply(lambda x: adjust_model_size_format(x.split('_')[0]))\n",
    "        df['quantization'] = df['model_configurations'].apply(lambda x: str(x.split('_')[1]))\n",
    "        df['fine_tuning'] = df['model_configurations'].apply(lambda x: str(x.split('_')[2]))\n",
    "\n",
    "    df['maxNewTokensFactor'] = df['file'].apply(lambda x: re.search(r'maxNewTokensFactor(\\d+)', x).group(1))\n",
    "    df['nShotsInference'] = df['file'].apply(lambda x: re.search(r'nShotsInference(\\d+)', x).group(1))\n",
    "    #\n",
    "    # df['layer'] = df['file'].apply(lambda x: re.search(r'adapters_(\\s+)', x).group(1))\n",
    "    df['model'] = df['file'].apply(lambda x: str(x.split('_adapters_')[0].split('nShotsInference')[1][2:].replace('.csv', '')))\n",
    "    if df['fine_tuning'][0] == 'FT':\n",
    "        df['training_params_string'] = df['file'].apply(lambda x: x.split('adapters_')[1])\n",
    "        # df['nbit'] = df['training_params_string'].apply(lambda x: x.split('_')[1])\n",
    "        df['bnb_4bit_compute_dtype'] = df['training_params_string'].apply(lambda x: x.split('_')[2])\n",
    "        df['r'] = df['training_params_string'].apply(lambda x: int(x.split('_')[3]))\n",
    "        df['lora_alpha'] = df['training_params_string'].apply(lambda x: int(x.split('_')[4]))\n",
    "        df['lora_dropout'] = df['training_params_string'].apply(lambda x: float(x.split('_')[5]))\n",
    "        df['gradient_accumulation_steps'] = df['training_params_string'].apply(lambda x: int(x.split('_')[6]))\n",
    "        df['learning_rate'] = df['training_params_string'].apply(lambda x: x.split('_')[7].replace('.csv', ''))\n",
    "    elif df['fine_tuning'][0] in ['base', 'NoFT']:\n",
    "        df['training_params_string'] = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:  data/evaluation_results/zefiro_4bit_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_noInstr_8bit_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/zefiro_NoQuant_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/zefiro_8bit_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_14B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama3_8B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_14B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistralNoQuant_FT_bfloat_ora.csv\n",
      "SOURCE:  data/evaluation_results/mistral_NoQuant_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_NoQuant_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama13B_8bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama13B_NoQuant_FT_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_NoQuant_FT.csv\n",
      "SOURCE:  data/evaluation_results/phi3_3B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistra_8bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_NoQuant_FT_float32.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_NoQuantbit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_8bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_8bit_FT_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_4bit_FT.csv\n",
      "SOURCE:  data/evaluation_results/mistral_NoQuant_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_NoQuantbit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_NoQuant_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_14B_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_8bit_base_wordsLevelTrue_evaluation.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>model_type</th>\n",
       "      <th>instructed</th>\n",
       "      <th>model_configurations</th>\n",
       "      <th>model_size</th>\n",
       "      <th>quantization</th>\n",
       "      <th>fine_tuning</th>\n",
       "      <th>maxNewTokensFactor</th>\n",
       "      <th>nShotsInference</th>\n",
       "      <th>model</th>\n",
       "      <th>training_params_string</th>\n",
       "      <th>bnb_4bit_compute_dtype</th>\n",
       "      <th>r</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>lora_dropout</th>\n",
       "      <th>gradient_accumulation_steps</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/zefiro/4bit_FT/maxNewTokensFactor8_nShots...</td>\n",
       "      <td>0.430096</td>\n",
       "      <td>0.419648</td>\n",
       "      <td>0.441077</td>\n",
       "      <td>zefiro</td>\n",
       "      <td>instructed</td>\n",
       "      <td>4bit_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>4bit</td>\n",
       "      <td>FT</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>zefiro-7b-base-ita_</td>\n",
       "      <td>it.layer1_4_torch.bfloat16_32_64_0.01_8_0.0002...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>32.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/zefiro/4bit_FT/maxNewTokensFactor4_nShots...</td>\n",
       "      <td>0.371730</td>\n",
       "      <td>0.441663</td>\n",
       "      <td>0.320915</td>\n",
       "      <td>zefiro</td>\n",
       "      <td>instructed</td>\n",
       "      <td>4bit_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>4bit</td>\n",
       "      <td>FT</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>zefiro-7b-base-ita_</td>\n",
       "      <td>it.layer1_4_torch.bfloat16_16_32_0.01_8_0.0002...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/zefiro/4bit_FT/maxNewTokensFactor4_nShots...</td>\n",
       "      <td>0.412787</td>\n",
       "      <td>0.438221</td>\n",
       "      <td>0.390144</td>\n",
       "      <td>zefiro</td>\n",
       "      <td>instructed</td>\n",
       "      <td>4bit_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>4bit</td>\n",
       "      <td>FT</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>zefiro-7b-base-ita_</td>\n",
       "      <td>it.layer1_4_torch.bfloat16_16_64_0.01_2_0.0002...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/zefiro/4bit_FT/maxNewTokensFactor8_nShots...</td>\n",
       "      <td>0.210648</td>\n",
       "      <td>0.290101</td>\n",
       "      <td>0.165359</td>\n",
       "      <td>zefiro</td>\n",
       "      <td>instructed</td>\n",
       "      <td>4bit_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>4bit</td>\n",
       "      <td>FT</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>zefiro-7b-base-ita_</td>\n",
       "      <td>it.layer1_4_torch.bfloat16_16_64_0.01_4_0.0002...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/zefiro/4bit_FT/maxNewTokensFactor8_nShots...</td>\n",
       "      <td>0.364657</td>\n",
       "      <td>0.406798</td>\n",
       "      <td>0.330427</td>\n",
       "      <td>zefiro</td>\n",
       "      <td>instructed</td>\n",
       "      <td>4bit_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>4bit</td>\n",
       "      <td>FT</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>zefiro-7b-base-ita_</td>\n",
       "      <td>it.layer1_4_torch.bfloat16_64_32_0.01_8_0.0002...</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/qwen/7B_8bit_base/maxNewTokensFactor4_nSh...</td>\n",
       "      <td>0.497369</td>\n",
       "      <td>0.556719</td>\n",
       "      <td>0.449454</td>\n",
       "      <td>qwen</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_8bit_base</td>\n",
       "      <td>7</td>\n",
       "      <td>8bit</td>\n",
       "      <td>base</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>BaseModel_Qwen1.5-7B-Chat_8</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/qwen/7B_8bit_base/maxNewTokensFactor8_nSh...</td>\n",
       "      <td>0.520493</td>\n",
       "      <td>0.555185</td>\n",
       "      <td>0.489881</td>\n",
       "      <td>qwen</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_8bit_base</td>\n",
       "      <td>7</td>\n",
       "      <td>8bit</td>\n",
       "      <td>base</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>BaseModel_Qwen1.5-7B-Chat_8</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/qwen/7B_8bit_base/maxNewTokensFactor4_nSh...</td>\n",
       "      <td>0.509443</td>\n",
       "      <td>0.575241</td>\n",
       "      <td>0.457153</td>\n",
       "      <td>qwen</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_8bit_base</td>\n",
       "      <td>7</td>\n",
       "      <td>8bit</td>\n",
       "      <td>base</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>BaseModel_Qwen1.5-7B-Chat_8</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/qwen/7B_8bit_base/maxNewTokensFactor4_nSh...</td>\n",
       "      <td>0.508342</td>\n",
       "      <td>0.560437</td>\n",
       "      <td>0.465108</td>\n",
       "      <td>qwen</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_8bit_base</td>\n",
       "      <td>7</td>\n",
       "      <td>8bit</td>\n",
       "      <td>base</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>BaseModel_Qwen1.5-7B-Chat_8</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/qwen/7B_8bit_base/maxNewTokensFactor4_nSh...</td>\n",
       "      <td>0.467672</td>\n",
       "      <td>0.565293</td>\n",
       "      <td>0.398802</td>\n",
       "      <td>qwen</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_8bit_base</td>\n",
       "      <td>7</td>\n",
       "      <td>8bit</td>\n",
       "      <td>base</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>BaseModel_Qwen1.5-7B-Chat_8</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2337 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  f1_score  precision  \\\n",
       "0   data/zefiro/4bit_FT/maxNewTokensFactor8_nShots...  0.430096   0.419648   \n",
       "1   data/zefiro/4bit_FT/maxNewTokensFactor4_nShots...  0.371730   0.441663   \n",
       "2   data/zefiro/4bit_FT/maxNewTokensFactor4_nShots...  0.412787   0.438221   \n",
       "3   data/zefiro/4bit_FT/maxNewTokensFactor8_nShots...  0.210648   0.290101   \n",
       "4   data/zefiro/4bit_FT/maxNewTokensFactor8_nShots...  0.364657   0.406798   \n",
       "..                                                ...       ...        ...   \n",
       "5   data/qwen/7B_8bit_base/maxNewTokensFactor4_nSh...  0.497369   0.556719   \n",
       "6   data/qwen/7B_8bit_base/maxNewTokensFactor8_nSh...  0.520493   0.555185   \n",
       "7   data/qwen/7B_8bit_base/maxNewTokensFactor4_nSh...  0.509443   0.575241   \n",
       "8   data/qwen/7B_8bit_base/maxNewTokensFactor4_nSh...  0.508342   0.560437   \n",
       "9   data/qwen/7B_8bit_base/maxNewTokensFactor4_nSh...  0.467672   0.565293   \n",
       "\n",
       "      recall model_type  instructed model_configurations model_size  \\\n",
       "0   0.441077     zefiro  instructed              4bit_FT          7   \n",
       "1   0.320915     zefiro  instructed              4bit_FT          7   \n",
       "2   0.390144     zefiro  instructed              4bit_FT          7   \n",
       "3   0.165359     zefiro  instructed              4bit_FT          7   \n",
       "4   0.330427     zefiro  instructed              4bit_FT          7   \n",
       "..       ...        ...         ...                  ...        ...   \n",
       "5   0.449454       qwen  instructed         7B_8bit_base          7   \n",
       "6   0.489881       qwen  instructed         7B_8bit_base          7   \n",
       "7   0.457153       qwen  instructed         7B_8bit_base          7   \n",
       "8   0.465108       qwen  instructed         7B_8bit_base          7   \n",
       "9   0.398802       qwen  instructed         7B_8bit_base          7   \n",
       "\n",
       "   quantization fine_tuning maxNewTokensFactor nShotsInference  \\\n",
       "0          4bit          FT                  8               0   \n",
       "1          4bit          FT                  4               4   \n",
       "2          4bit          FT                  4               2   \n",
       "3          4bit          FT                  8               2   \n",
       "4          4bit          FT                  8               0   \n",
       "..          ...         ...                ...             ...   \n",
       "5          8bit        base                  4               3   \n",
       "6          8bit        base                  8               1   \n",
       "7          8bit        base                  4               1   \n",
       "8          8bit        base                  4               4   \n",
       "9          8bit        base                  4               0   \n",
       "\n",
       "                          model  \\\n",
       "0           zefiro-7b-base-ita_   \n",
       "1           zefiro-7b-base-ita_   \n",
       "2           zefiro-7b-base-ita_   \n",
       "3           zefiro-7b-base-ita_   \n",
       "4           zefiro-7b-base-ita_   \n",
       "..                          ...   \n",
       "5   BaseModel_Qwen1.5-7B-Chat_8   \n",
       "6   BaseModel_Qwen1.5-7B-Chat_8   \n",
       "7   BaseModel_Qwen1.5-7B-Chat_8   \n",
       "8   BaseModel_Qwen1.5-7B-Chat_8   \n",
       "9   BaseModel_Qwen1.5-7B-Chat_8   \n",
       "\n",
       "                               training_params_string bnb_4bit_compute_dtype  \\\n",
       "0   it.layer1_4_torch.bfloat16_32_64_0.01_8_0.0002...         torch.bfloat16   \n",
       "1   it.layer1_4_torch.bfloat16_16_32_0.01_8_0.0002...         torch.bfloat16   \n",
       "2   it.layer1_4_torch.bfloat16_16_64_0.01_2_0.0002...         torch.bfloat16   \n",
       "3   it.layer1_4_torch.bfloat16_16_64_0.01_4_0.0002...         torch.bfloat16   \n",
       "4   it.layer1_4_torch.bfloat16_64_32_0.01_8_0.0002...         torch.bfloat16   \n",
       "..                                                ...                    ...   \n",
       "5                                                None                    NaN   \n",
       "6                                                None                    NaN   \n",
       "7                                                None                    NaN   \n",
       "8                                                None                    NaN   \n",
       "9                                                None                    NaN   \n",
       "\n",
       "       r  lora_alpha  lora_dropout  gradient_accumulation_steps learning_rate  \n",
       "0   32.0        64.0          0.01                          8.0        0.0002  \n",
       "1   16.0        32.0          0.01                          8.0        0.0002  \n",
       "2   16.0        64.0          0.01                          2.0        0.0002  \n",
       "3   16.0        64.0          0.01                          4.0        0.0002  \n",
       "4   64.0        32.0          0.01                          8.0        0.0002  \n",
       "..   ...         ...           ...                          ...           ...  \n",
       "5    NaN         NaN           NaN                          NaN           NaN  \n",
       "6    NaN         NaN           NaN                          NaN           NaN  \n",
       "7    NaN         NaN           NaN                          NaN           NaN  \n",
       "8    NaN         NaN           NaN                          NaN           NaN  \n",
       "9    NaN         NaN           NaN                          NaN           NaN  \n",
       "\n",
       "[2337 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "#from utils.calculate_results_helper import extract_params_from_file_name, refine_output_df\n",
    "\n",
    "csv_files = glob.glob('data/evaluation_results/*.csv')\n",
    "dfs = []\n",
    "empty_n = 0\n",
    "for file in csv_files:\n",
    "    if file =='data/evaluation_results/joint_results.csv':\n",
    "        continue\n",
    "    tmp_df = pd.read_csv(file)\n",
    "    if tmp_df.shape[0] == 0:\n",
    "        print('EMPTY FILE: ', file)\n",
    "        empty_n += 1\n",
    "        continue\n",
    "    print('SOURCE: ', file)\n",
    "    tmp_df = extract_params_from_file_name(tmp_df)\n",
    "    dfs.append(tmp_df)\n",
    "\n",
    "res = pd.concat(dfs)\n",
    "res = refine_output_df(res)\n",
    "display(res)\n",
    "res.to_csv('data/evaluation_results/joint_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col:  model_type vals:  ['zefiro' 'mistral' 'qwen' 'llama']\n",
      "col:  instructed vals:  ['instructed' 'notInstructed']\n",
      "col:  model_size vals:  ['7' '14' '13']\n",
      "col:  quantization vals:  ['4bit' '8bit' 'NoQuant']\n",
      "col:  fine_tuning vals:  ['FT' 'unsure' 'base']\n",
      "col:  maxNewTokensFactor vals:  ['8' '4' '2']\n",
      "col:  nShotsInference vals:  ['4' '0' '2' '1' '3']\n",
      "col:  bnb_4bit_compute_dtype vals:  ['torch.bfloat16' nan]\n",
      "col:  r vals:  [32. 64. 16. nan]\n",
      "col:  lora_alpha vals:  [64. 32. nan]\n",
      "col:  lora_dropout vals:  [0.01  nan 0.05]\n",
      "col:  gradient_accumulation_steps vals:  [ 4.  8.  2. nan 16.]\n",
      "col:  learning_rate vals:  ['0.0002' nan '0.0008' '0.002']\n"
     ]
    }
   ],
   "source": [
    "for col in res.columns:\n",
    "    if col in ['file', 'f1_score', 'recall', 'precision', 'model_configurations', 'model']:\n",
    "        continue\n",
    "    print('col: ', col, 'vals: ', res[col].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
