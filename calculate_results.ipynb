{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:  data/evaluation_results/qwen_7B_NoQuant_FT.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_14B_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama13B_NoQuant_FT_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_14B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_NoQuantbit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_NoQuant_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_4bits_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_NoQuantbit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/zefiro_NoQuant_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_8bit_FT_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/zefiro_8bit_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_NoQuant_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/zefiro_4bit_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_NoQuant_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama13B_8bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_NoQuant_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistral_noInstr_8bit_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_NoQuant_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/mistra_8bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/qwen_14B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_8bit_base_wordsLevelTrue_evaluation.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>model_type</th>\n",
       "      <th>instructed</th>\n",
       "      <th>model_configurations</th>\n",
       "      <th>model_size</th>\n",
       "      <th>quantization</th>\n",
       "      <th>fine_tuning</th>\n",
       "      <th>maxNewTokensFactor</th>\n",
       "      <th>nShotsInference</th>\n",
       "      <th>model</th>\n",
       "      <th>bnb_4bit_compute_dtype</th>\n",
       "      <th>r</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>lora_dropout</th>\n",
       "      <th>gradient_accumulation_steps</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/qwen/7B_NoQuant_FT/maxNewTokensFactor2_nS...</td>\n",
       "      <td>0.382627</td>\n",
       "      <td>0.593101</td>\n",
       "      <td>0.282409</td>\n",
       "      <td>qwen</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>FT</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>qwen1.5-7b-chat_</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>32.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/qwen/7B_NoQuant_FT/maxNewTokensFactor2_nS...</td>\n",
       "      <td>0.380591</td>\n",
       "      <td>0.583910</td>\n",
       "      <td>0.282295</td>\n",
       "      <td>qwen</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>FT</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>qwen1.5-7b-chat_</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/qwen/7B_NoQuant_FT/maxNewTokensFactor2_nS...</td>\n",
       "      <td>0.365971</td>\n",
       "      <td>0.563712</td>\n",
       "      <td>0.270932</td>\n",
       "      <td>qwen</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>FT</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>qwen1.5-7b-chat_</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>32.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/qwen/7B_NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>0.489239</td>\n",
       "      <td>0.570002</td>\n",
       "      <td>0.428523</td>\n",
       "      <td>qwen</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>FT</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>qwen1.5-7b-chat_</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/qwen/7B_NoQuant_FT/maxNewTokensFactor2_nS...</td>\n",
       "      <td>0.402171</td>\n",
       "      <td>0.606623</td>\n",
       "      <td>0.300794</td>\n",
       "      <td>qwen</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>FT</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>qwen1.5-7b-chat_</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>32.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor8_nS...</td>\n",
       "      <td>0.457107</td>\n",
       "      <td>0.555305</td>\n",
       "      <td>0.388421</td>\n",
       "      <td>llama</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_8bit_base</td>\n",
       "      <td>7</td>\n",
       "      <td>8bit</td>\n",
       "      <td>base</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>BaseModel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor2_nS...</td>\n",
       "      <td>0.366001</td>\n",
       "      <td>0.642696</td>\n",
       "      <td>0.255852</td>\n",
       "      <td>llama</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_8bit_base</td>\n",
       "      <td>7</td>\n",
       "      <td>8bit</td>\n",
       "      <td>base</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>BaseModel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor2_nS...</td>\n",
       "      <td>0.251434</td>\n",
       "      <td>0.664852</td>\n",
       "      <td>0.155032</td>\n",
       "      <td>llama</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_8bit_base</td>\n",
       "      <td>7</td>\n",
       "      <td>8bit</td>\n",
       "      <td>base</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>BaseModel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor2_nS...</td>\n",
       "      <td>0.276631</td>\n",
       "      <td>0.625473</td>\n",
       "      <td>0.177587</td>\n",
       "      <td>llama</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_8bit_base</td>\n",
       "      <td>7</td>\n",
       "      <td>8bit</td>\n",
       "      <td>base</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>BaseModel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor8_nS...</td>\n",
       "      <td>0.438213</td>\n",
       "      <td>0.555048</td>\n",
       "      <td>0.362012</td>\n",
       "      <td>llama</td>\n",
       "      <td>instructed</td>\n",
       "      <td>7B_8bit_base</td>\n",
       "      <td>7</td>\n",
       "      <td>8bit</td>\n",
       "      <td>base</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>BaseModel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2300 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  f1_score  precision  \\\n",
       "0   data/qwen/7B_NoQuant_FT/maxNewTokensFactor2_nS...  0.382627   0.593101   \n",
       "1   data/qwen/7B_NoQuant_FT/maxNewTokensFactor2_nS...  0.380591   0.583910   \n",
       "2   data/qwen/7B_NoQuant_FT/maxNewTokensFactor2_nS...  0.365971   0.563712   \n",
       "3   data/qwen/7B_NoQuant_FT/maxNewTokensFactor4_nS...  0.489239   0.570002   \n",
       "4   data/qwen/7B_NoQuant_FT/maxNewTokensFactor2_nS...  0.402171   0.606623   \n",
       "..                                                ...       ...        ...   \n",
       "10  data/llama/7B_8bit_base/maxNewTokensFactor8_nS...  0.457107   0.555305   \n",
       "11  data/llama/7B_8bit_base/maxNewTokensFactor2_nS...  0.366001   0.642696   \n",
       "12  data/llama/7B_8bit_base/maxNewTokensFactor2_nS...  0.251434   0.664852   \n",
       "13  data/llama/7B_8bit_base/maxNewTokensFactor2_nS...  0.276631   0.625473   \n",
       "14  data/llama/7B_8bit_base/maxNewTokensFactor8_nS...  0.438213   0.555048   \n",
       "\n",
       "      recall model_type  instructed model_configurations model_size  \\\n",
       "0   0.282409       qwen  instructed        7B_NoQuant_FT          7   \n",
       "1   0.282295       qwen  instructed        7B_NoQuant_FT          7   \n",
       "2   0.270932       qwen  instructed        7B_NoQuant_FT          7   \n",
       "3   0.428523       qwen  instructed        7B_NoQuant_FT          7   \n",
       "4   0.300794       qwen  instructed        7B_NoQuant_FT          7   \n",
       "..       ...        ...         ...                  ...        ...   \n",
       "10  0.388421      llama  instructed         7B_8bit_base          7   \n",
       "11  0.255852      llama  instructed         7B_8bit_base          7   \n",
       "12  0.155032      llama  instructed         7B_8bit_base          7   \n",
       "13  0.177587      llama  instructed         7B_8bit_base          7   \n",
       "14  0.362012      llama  instructed         7B_8bit_base          7   \n",
       "\n",
       "   quantization fine_tuning maxNewTokensFactor nShotsInference  \\\n",
       "0       NoQuant          FT                  2               2   \n",
       "1       NoQuant          FT                  2               2   \n",
       "2       NoQuant          FT                  2               4   \n",
       "3       NoQuant          FT                  4               2   \n",
       "4       NoQuant          FT                  2               0   \n",
       "..          ...         ...                ...             ...   \n",
       "10         8bit        base                  8               3   \n",
       "11         8bit        base                  2               3   \n",
       "12         8bit        base                  2               2   \n",
       "13         8bit        base                  2               1   \n",
       "14         8bit        base                  8               1   \n",
       "\n",
       "               model bnb_4bit_compute_dtype     r  lora_alpha  lora_dropout  \\\n",
       "0   qwen1.5-7b-chat_         torch.bfloat16  32.0        64.0          0.01   \n",
       "1   qwen1.5-7b-chat_         torch.bfloat16  16.0        32.0          0.01   \n",
       "2   qwen1.5-7b-chat_         torch.bfloat16  32.0        64.0          0.01   \n",
       "3   qwen1.5-7b-chat_         torch.bfloat16  16.0        32.0          0.01   \n",
       "4   qwen1.5-7b-chat_         torch.bfloat16  32.0        64.0          0.01   \n",
       "..               ...                    ...   ...         ...           ...   \n",
       "10         BaseModel                    NaN   NaN         NaN           NaN   \n",
       "11         BaseModel                    NaN   NaN         NaN           NaN   \n",
       "12         BaseModel                    NaN   NaN         NaN           NaN   \n",
       "13         BaseModel                    NaN   NaN         NaN           NaN   \n",
       "14         BaseModel                    NaN   NaN         NaN           NaN   \n",
       "\n",
       "    gradient_accumulation_steps learning_rate  \n",
       "0                           8.0        0.0002  \n",
       "1                           2.0        0.0002  \n",
       "2                           2.0        0.0002  \n",
       "3                           2.0        0.0002  \n",
       "4                           4.0        0.0002  \n",
       "..                          ...           ...  \n",
       "10                          NaN           NaN  \n",
       "11                          NaN           NaN  \n",
       "12                          NaN           NaN  \n",
       "13                          NaN           NaN  \n",
       "14                          NaN           NaN  \n",
       "\n",
       "[2300 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "from utils.calculate_results_helper import extract_params_from_file_name, refine_output_df\n",
    "\n",
    "csv_files = glob.glob('data/evaluation_results/*.csv')\n",
    "dfs = []\n",
    "empty_n = 0\n",
    "for file in csv_files:\n",
    "    if file =='data/evaluation_results/joint_results.csv':\n",
    "        continue\n",
    "    tmp_df = pd.read_csv(file)\n",
    "    if tmp_df.shape[0] == 0:\n",
    "        print('EMPTY FILE: ', file)\n",
    "        empty_n += 1\n",
    "        continue\n",
    "    print('SOURCE: ', file)\n",
    "    tmp_df = extract_params_from_file_name(tmp_df)\n",
    "    dfs.append(tmp_df)\n",
    "\n",
    "res = pd.concat(dfs)\n",
    "res = refine_output_df(res)\n",
    "display(res)\n",
    "res.to_csv('data/evaluation_results/joint_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col:  model_type vals:  ['zefiro' 'mistral' 'qwen' 'llama']\n",
      "col:  instructed vals:  ['instructed' 'notInstructed']\n",
      "col:  model_size vals:  ['7' '14' '13']\n",
      "col:  quantization vals:  ['4bit' '8bit' 'NoQuant']\n",
      "col:  fine_tuning vals:  ['FT' 'unsure' 'base']\n",
      "col:  maxNewTokensFactor vals:  ['8' '4' '2']\n",
      "col:  nShotsInference vals:  ['4' '0' '2' '1' '3']\n",
      "col:  bnb_4bit_compute_dtype vals:  ['torch.bfloat16' nan]\n",
      "col:  r vals:  [32. 64. 16. nan]\n",
      "col:  lora_alpha vals:  [64. 32. nan]\n",
      "col:  lora_dropout vals:  [0.01  nan 0.05]\n",
      "col:  gradient_accumulation_steps vals:  [ 4.  8.  2. nan 16.]\n",
      "col:  learning_rate vals:  ['0.0002' nan '0.0008' '0.002']\n"
     ]
    }
   ],
   "source": [
    "for col in res.columns:\n",
    "    if col in ['file', 'f1_score', 'recall', 'precision', 'model_configurations', 'model']:\n",
    "        continue\n",
    "    print('col: ', col, 'vals: ', res[col].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
