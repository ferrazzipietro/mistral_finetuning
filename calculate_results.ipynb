{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_params_from_file_name(df: pd.DataFrame):\n",
    "    df['maxNewTokensFactor'] = df['file'].apply(lambda x: re.search(r'maxNewTokensFactor(\\d+)', x).group(1))\n",
    "    df['nShotsInference'] = df['file'].apply(lambda x: re.search(r'nShotsInference(\\d+)', x).group(1))\n",
    "    #df['layer'] = df['file'].apply(lambda x: re.search(r'adapters_(\\s+)', x).group(1))\n",
    "    df['model'] = df['file'].apply(lambda x: str(x.split('_adapters_')[0].split('nShotsInference')[1][2:]))\n",
    "    df['training_params_string'] = df['file'].apply(lambda x: x.split('adapters_')[1])\n",
    "    df['nbit'] = df['training_params_string'].apply(lambda x: int(x.split('_')[1]))\n",
    "    df['bnb_4bit_compute_dtype'] = df['training_params_string'].apply(lambda x: x.split('_')[2])\n",
    "    df['r'] = df['training_params_string'].apply(lambda x: int(x.split('_')[3]))\n",
    "    df['lora_alpha'] = df['training_params_string'].apply(lambda x: int(x.split('_')[4]))\n",
    "    df['lora_dropout'] = df['training_params_string'].apply(lambda x: float(x.split('_')[5]))\n",
    "    df['gradient_accumulation_steps'] = df['training_params_string'].apply(lambda x: int(x.split('_')[6]))\n",
    "    df['learning_rate'] = df['training_params_string'].apply(lambda x: float(x.split('_')[7].split('.')[0]))\n",
    "    df = df.drop(columns=['training_params_string', 'file'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_13B_4bit_base_wordsLevelTrue_evaluation = pd.read_csv(\"data/evaluation_results/llama_13B_4bit_base_wordsLevelTrue_evaluation.csv\")\n",
    "llama_13B_8bit_base_wordsLevelTrue_evaluation = pd.read_csv(\"data/evaluation_results/llama_13B_8bit_base_wordsLevelTrue_evaluation.csv\")\n",
    "llama_7B_4bit_base_wordsLevelTrue_evaluation = pd.read_csv(\"data/evaluation_results/llama_7B_4bit_base_wordsLevelTrue_evaluation.csv\")\n",
    "llama_7B_8bit_base_wordsLevelTrue_evaluation = pd.read_csv(\"data/evaluation_results/llama_7B_8bit_base_wordsLevelTrue_evaluation.csv\")\n",
    "llama_7B_8bit_wordsLevelTrue_evaluation = pd.read_csv(\"data/evaluation_results/llama_7B_8bit_wordsLevelTrue_evaluation.csv\")\n",
    "mistral_4bit_base_wordsLevelTrue_evaluation = pd.read_csv(\"data/evaluation_results/mistral_4bit_base_wordsLevelTrue_evaluation.csv\")\n",
    "mistral_4bit_wordsLevelTrue_evaluation = pd.read_csv(\"data/evaluation_results/mistral_4bit_wordsLevelTrue_evaluation.csv\")\n",
    "mistral_noInstr_4bit_wordsLevelTrue_evaluation = pd.read_csv(\"data/evaluation_results/mistral_noInstr_4bit_wordsLevelTrue_evaluation.csv\")\n",
    "\n",
    "# llama_13B_4bit_base_wordsLevelTrue_evaluation = extract_params_from_file_name(llama_13B_4bit_base_wordsLevelTrue_evaluation)\n",
    "# llama_13B_8bit_base_wordsLevelTrue_evaluation = extract_params_from_file_name(llama_13B_8bit_base_wordsLevelTrue_evaluation)\n",
    "# llama_7B_4bit_base_wordsLevelTrue_evaluation = extract_params_from_file_name(llama_7B_4bit_base_wordsLevelTrue_evaluation)\n",
    "# llama_7B_8bit_base_wordsLevelTrue_evaluation = extract_params_from_file_name(llama_7B_8bit_base_wordsLevelTrue_evaluation)\n",
    "# llama_7B_8bit_wordsLevelTrue_evaluation = extract_params_from_file_name(llama_7B_8bit_wordsLevelTrue_evaluation)\n",
    "# mistral_4bit_base_wordsLevelTrue_evaluation = extract_params_from_file_name(mistral_4bit_base_wordsLevelTrue_evaluation)\n",
    "# mistral_4bit_wordsLevelTrue_evaluation = extract_params_from_file_name(mistral_4bit_wordsLevelTrue_evaluation)\n",
    "# mistral_noInstr_4bit_wordsLevelTrue_evaluation = extract_params_from_file_name(mistral_noInstr_4bit_wordsLevelTrue_evaluation)\n",
    "\n",
    "res = pd.concat([llama_13B_4bit_base_wordsLevelTrue_evaluation, llama_13B_8bit_base_wordsLevelTrue_evaluation,\n",
    "           llama_7B_4bit_base_wordsLevelTrue_evaluation, llama_7B_8bit_base_wordsLevelTrue_evaluation,\n",
    "           llama_7B_8bit_wordsLevelTrue_evaluation, mistral_4bit_base_wordsLevelTrue_evaluation,\n",
    "           mistral_4bit_wordsLevelTrue_evaluation, mistral_noInstr_4bit_wordsLevelTrue_evaluation])#.to_csv('data/evaluation_results/joint_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>similar_is_equal</th>\n",
       "      <th>similar_is_equal_threshold</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor8_nS...</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>0.352761</td>\n",
       "      <td>0.612789</td>\n",
       "      <td>0.247667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor8_nS...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>0.352761</td>\n",
       "      <td>0.612789</td>\n",
       "      <td>0.247667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor8_nS...</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>0.330451</td>\n",
       "      <td>0.336468</td>\n",
       "      <td>0.324645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor8_nS...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>0.330451</td>\n",
       "      <td>0.336468</td>\n",
       "      <td>0.324645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor8_nS...</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>0.272259</td>\n",
       "      <td>0.231052</td>\n",
       "      <td>0.331354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor8_nS...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>0.272259</td>\n",
       "      <td>0.231052</td>\n",
       "      <td>0.331354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor8_nS...</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>0.243041</td>\n",
       "      <td>0.214002</td>\n",
       "      <td>0.281197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/llama/7B_8bit_base/maxNewTokensFactor8_nS...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>0.243041</td>\n",
       "      <td>0.214002</td>\n",
       "      <td>0.281197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  similar_is_equal  \\\n",
       "2  data/llama/7B_8bit_base/maxNewTokensFactor8_nS...              True   \n",
       "3  data/llama/7B_8bit_base/maxNewTokensFactor8_nS...              True   \n",
       "4  data/llama/7B_8bit_base/maxNewTokensFactor8_nS...              True   \n",
       "5  data/llama/7B_8bit_base/maxNewTokensFactor8_nS...              True   \n",
       "0  data/llama/7B_8bit_base/maxNewTokensFactor8_nS...              True   \n",
       "1  data/llama/7B_8bit_base/maxNewTokensFactor8_nS...              True   \n",
       "6  data/llama/7B_8bit_base/maxNewTokensFactor8_nS...              True   \n",
       "7  data/llama/7B_8bit_base/maxNewTokensFactor8_nS...              True   \n",
       "\n",
       "   similar_is_equal_threshold  f1_score  precision    recall  \n",
       "2                          80  0.352761   0.612789  0.247667  \n",
       "3                         100  0.352761   0.612789  0.247667  \n",
       "4                          80  0.330451   0.336468  0.324645  \n",
       "5                         100  0.330451   0.336468  0.324645  \n",
       "0                          80  0.272259   0.231052  0.331354  \n",
       "1                         100  0.272259   0.231052  0.331354  \n",
       "6                          80  0.243041   0.214002  0.281197  \n",
       "7                         100  0.243041   0.214002  0.281197  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_7B_8bit_base_wordsLevelTrue_evaluation.sort_values(by='f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>similar_is_equal</th>\n",
       "      <th>similar_is_equal_threshold</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>0.463977</td>\n",
       "      <td>0.342892</td>\n",
       "      <td>0.717263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>0.463977</td>\n",
       "      <td>0.342892</td>\n",
       "      <td>0.717263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>0.459589</td>\n",
       "      <td>0.340422</td>\n",
       "      <td>0.707122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>0.459589</td>\n",
       "      <td>0.340422</td>\n",
       "      <td>0.707122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>0.444313</td>\n",
       "      <td>0.331432</td>\n",
       "      <td>0.673799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>0.292612</td>\n",
       "      <td>0.210670</td>\n",
       "      <td>0.478876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>0.290654</td>\n",
       "      <td>0.194485</td>\n",
       "      <td>0.574957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>0.290654</td>\n",
       "      <td>0.194485</td>\n",
       "      <td>0.574957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "      <td>0.279817</td>\n",
       "      <td>0.230591</td>\n",
       "      <td>0.355766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>0.279817</td>\n",
       "      <td>0.230591</td>\n",
       "      <td>0.355766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  similar_is_equal  \\\n",
       "67  data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...              True   \n",
       "66  data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...              True   \n",
       "2   data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...              True   \n",
       "3   data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...              True   \n",
       "50  data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...              True   \n",
       "..                                                ...               ...   \n",
       "13  data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...              True   \n",
       "55  data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...              True   \n",
       "54  data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...              True   \n",
       "14  data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...              True   \n",
       "15  data/llama/7B_8bit/maxNewTokensFactor8_nShotsI...              True   \n",
       "\n",
       "    similar_is_equal_threshold  f1_score  precision    recall  \n",
       "67                         100  0.463977   0.342892  0.717263  \n",
       "66                          80  0.463977   0.342892  0.717263  \n",
       "2                           80  0.459589   0.340422  0.707122  \n",
       "3                          100  0.459589   0.340422  0.707122  \n",
       "50                          80  0.444313   0.331432  0.673799  \n",
       "..                         ...       ...        ...       ...  \n",
       "13                         100  0.292612   0.210670  0.478876  \n",
       "55                         100  0.290654   0.194485  0.574957  \n",
       "54                          80  0.290654   0.194485  0.574957  \n",
       "14                          80  0.279817   0.230591  0.355766  \n",
       "15                         100  0.279817   0.230591  0.355766  \n",
       "\n",
       "[80 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_7B_8bit_wordsLevelTrue_evaluation.sort_values(by='f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/mistral/4bit/maxNewTokensFactor8_nShotsInference2_Mistral-7B-Instruct-v0.2_adapters_en.layer1_4_torch.bfloat16_32_32_0.05_8_0.0002.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = res[res['f1_score']>0.6].sort_values(by='f1_score', ascending=False) \n",
    "tmp[tmp['similar_is_equal_threshold'] > 80].loc[93]['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_params_from_file_name(df: pd.DataFrame):\n",
    "    if 'base' in df['file']:\n",
    "        df['model'] = df['file'].apply(lambda x: str(x.split('/')[0].split('nShotsInference')[1][2:]))\n",
    "    df['maxNewTokensFactor'] = df['file'].apply(lambda x: re.search(r'maxNewTokensFactor(\\d+)', x).group(1))\n",
    "    df['nShotsInference'] = df['file'].apply(lambda x: re.search(r'nShotsInference(\\d+)', x).group(1))\n",
    "    #df['layer'] = df['file'].apply(lambda x: re.search(r'adapters_(\\s+)', x).group(1))\n",
    "    df['model'] = df['file'].apply(lambda x: str(x.split('_adapters_')[0].split('nShotsInference')[1][2:]))\n",
    "    df['training_params_string'] = df['file'].apply(lambda x: x.split('adapters_')[1])\n",
    "    df['nbit'] = df['training_params_string'].apply(lambda x: int(x.split('_')[1]))\n",
    "    df['bnb_4bit_compute_dtype'] = df['training_params_string'].apply(lambda x: x.split('_')[2])\n",
    "    df['r'] = df['training_params_string'].apply(lambda x: int(x.split('_')[3]))\n",
    "    df['lora_alpha'] = df['training_params_string'].apply(lambda x: int(x.split('_')[4]))\n",
    "    df['lora_dropout'] = df['training_params_string'].apply(lambda x: float(x.split('_')[5]))\n",
    "    df['gradient_accumulation_steps'] = df['training_params_string'].apply(lambda x: int(x.split('_')[6]))\n",
    "    df['learning_rate'] = df['training_params_string'].apply(lambda x: float(x.split('_')[7].split('.')[0]))\n",
    "    df = df.drop(columns=['training_params_string', 'file'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/mistral/NoQuant_FT/maxNewTokensFactor4_nShotsInference0_mistral-7b-instruct-v0.2__adapters_en.layer1_NoQuant_torch.bfloat16_16_32_0.05_2_0.0002.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>similar_is_equal</th>\n",
       "      <th>similar_is_equal_threshold</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>model_type</th>\n",
       "      <th>model_configurations</th>\n",
       "      <th>model_size</th>\n",
       "      <th>quantization</th>\n",
       "      <th>...</th>\n",
       "      <th>nShotsInference</th>\n",
       "      <th>model</th>\n",
       "      <th>training_params_string</th>\n",
       "      <th>nbit</th>\n",
       "      <th>bnb_4bit_compute_dtype</th>\n",
       "      <th>r</th>\n",
       "      <th>lora_alpha</th>\n",
       "      <th>lora_dropout</th>\n",
       "      <th>gradient_accumulation_steps</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.656256</td>\n",
       "      <td>0.739292</td>\n",
       "      <td>0.589990</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_16_32_0.05_2_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.597800</td>\n",
       "      <td>0.699018</td>\n",
       "      <td>0.522187</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_64_32_0.01_8_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.627492</td>\n",
       "      <td>0.773682</td>\n",
       "      <td>0.527768</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_16_32_0.01_2_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.395873</td>\n",
       "      <td>0.733435</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_64_32_0.01_2_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.626986</td>\n",
       "      <td>0.687078</td>\n",
       "      <td>0.576560</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_32_32_0.01_8_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.537010</td>\n",
       "      <td>0.730178</td>\n",
       "      <td>0.424665</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_64_32_0.05_2_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.591828</td>\n",
       "      <td>0.705230</td>\n",
       "      <td>0.509845</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_64_32_0.05_4_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.629987</td>\n",
       "      <td>0.686247</td>\n",
       "      <td>0.582253</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_32_32_0.05_8_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.615546</td>\n",
       "      <td>0.670341</td>\n",
       "      <td>0.569032</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_16_32_0.05_2_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.606973</td>\n",
       "      <td>0.702269</td>\n",
       "      <td>0.534450</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_64_32_0.05_8_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.647370</td>\n",
       "      <td>0.727002</td>\n",
       "      <td>0.583461</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_32_32_0.05_4_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.532200</td>\n",
       "      <td>0.714500</td>\n",
       "      <td>0.424016</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_16_32_0.01_4_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.627675</td>\n",
       "      <td>0.719423</td>\n",
       "      <td>0.556682</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_16_32_0.05_4_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.567709</td>\n",
       "      <td>0.736027</td>\n",
       "      <td>0.462046</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_32_32_0.05_2_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.612050</td>\n",
       "      <td>0.669747</td>\n",
       "      <td>0.563504</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_16_32_0.05_4_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.640732</td>\n",
       "      <td>0.729008</td>\n",
       "      <td>0.571526</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_16_32_0.01_8_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.388911</td>\n",
       "      <td>0.759414</td>\n",
       "      <td>0.261386</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_32_32_0.01_4_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.592891</td>\n",
       "      <td>0.737982</td>\n",
       "      <td>0.495478</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_32_32_0.01_2_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0.599707</td>\n",
       "      <td>0.681153</td>\n",
       "      <td>0.535658</td>\n",
       "      <td>mistral</td>\n",
       "      <td>NoQuant_FT</td>\n",
       "      <td>7</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>mistral-7b-instruct-v0.2_</td>\n",
       "      <td>en.layer1_NoQuant_torch.bfloat16_16_32_0.05_8_...</td>\n",
       "      <td>NoQuant</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  similar_is_equal  \\\n",
       "0   data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "1   data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "2   data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "3   data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "4   data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "5   data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "6   data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "7   data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "8   data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "9   data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "10  data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "11  data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "12  data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "13  data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "14  data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "15  data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "16  data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "17  data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "18  data/mistral/NoQuant_FT/maxNewTokensFactor4_nS...               NaN   \n",
       "\n",
       "    similar_is_equal_threshold  f1_score  precision    recall model_type  \\\n",
       "0                          100  0.656256   0.739292  0.589990    mistral   \n",
       "1                          100  0.597800   0.699018  0.522187    mistral   \n",
       "2                          100  0.627492   0.773682  0.527768    mistral   \n",
       "3                          100  0.395873   0.733435  0.271100    mistral   \n",
       "4                          100  0.626986   0.687078  0.576560    mistral   \n",
       "5                          100  0.537010   0.730178  0.424665    mistral   \n",
       "6                          100  0.591828   0.705230  0.509845    mistral   \n",
       "7                          100  0.629987   0.686247  0.582253    mistral   \n",
       "8                          100  0.615546   0.670341  0.569032    mistral   \n",
       "9                          100  0.606973   0.702269  0.534450    mistral   \n",
       "10                         100  0.647370   0.727002  0.583461    mistral   \n",
       "11                         100  0.532200   0.714500  0.424016    mistral   \n",
       "12                         100  0.627675   0.719423  0.556682    mistral   \n",
       "13                         100  0.567709   0.736027  0.462046    mistral   \n",
       "14                         100  0.612050   0.669747  0.563504    mistral   \n",
       "15                         100  0.640732   0.729008  0.571526    mistral   \n",
       "16                         100  0.388911   0.759414  0.261386    mistral   \n",
       "17                         100  0.592891   0.737982  0.495478    mistral   \n",
       "18                         100  0.599707   0.681153  0.535658    mistral   \n",
       "\n",
       "   model_configurations  model_size quantization  ... nShotsInference  \\\n",
       "0            NoQuant_FT           7      NoQuant  ...               0   \n",
       "1            NoQuant_FT           7      NoQuant  ...               0   \n",
       "2            NoQuant_FT           7      NoQuant  ...               0   \n",
       "3            NoQuant_FT           7      NoQuant  ...               0   \n",
       "4            NoQuant_FT           7      NoQuant  ...               0   \n",
       "5            NoQuant_FT           7      NoQuant  ...               0   \n",
       "6            NoQuant_FT           7      NoQuant  ...               0   \n",
       "7            NoQuant_FT           7      NoQuant  ...               0   \n",
       "8            NoQuant_FT           7      NoQuant  ...               2   \n",
       "9            NoQuant_FT           7      NoQuant  ...               0   \n",
       "10           NoQuant_FT           7      NoQuant  ...               0   \n",
       "11           NoQuant_FT           7      NoQuant  ...               0   \n",
       "12           NoQuant_FT           7      NoQuant  ...               0   \n",
       "13           NoQuant_FT           7      NoQuant  ...               0   \n",
       "14           NoQuant_FT           7      NoQuant  ...               2   \n",
       "15           NoQuant_FT           7      NoQuant  ...               0   \n",
       "16           NoQuant_FT           7      NoQuant  ...               0   \n",
       "17           NoQuant_FT           7      NoQuant  ...               0   \n",
       "18           NoQuant_FT           7      NoQuant  ...               0   \n",
       "\n",
       "                        model  \\\n",
       "0   mistral-7b-instruct-v0.2_   \n",
       "1   mistral-7b-instruct-v0.2_   \n",
       "2   mistral-7b-instruct-v0.2_   \n",
       "3   mistral-7b-instruct-v0.2_   \n",
       "4   mistral-7b-instruct-v0.2_   \n",
       "5   mistral-7b-instruct-v0.2_   \n",
       "6   mistral-7b-instruct-v0.2_   \n",
       "7   mistral-7b-instruct-v0.2_   \n",
       "8   mistral-7b-instruct-v0.2_   \n",
       "9   mistral-7b-instruct-v0.2_   \n",
       "10  mistral-7b-instruct-v0.2_   \n",
       "11  mistral-7b-instruct-v0.2_   \n",
       "12  mistral-7b-instruct-v0.2_   \n",
       "13  mistral-7b-instruct-v0.2_   \n",
       "14  mistral-7b-instruct-v0.2_   \n",
       "15  mistral-7b-instruct-v0.2_   \n",
       "16  mistral-7b-instruct-v0.2_   \n",
       "17  mistral-7b-instruct-v0.2_   \n",
       "18  mistral-7b-instruct-v0.2_   \n",
       "\n",
       "                               training_params_string     nbit  \\\n",
       "0   en.layer1_NoQuant_torch.bfloat16_16_32_0.05_2_...  NoQuant   \n",
       "1   en.layer1_NoQuant_torch.bfloat16_64_32_0.01_8_...  NoQuant   \n",
       "2   en.layer1_NoQuant_torch.bfloat16_16_32_0.01_2_...  NoQuant   \n",
       "3   en.layer1_NoQuant_torch.bfloat16_64_32_0.01_2_...  NoQuant   \n",
       "4   en.layer1_NoQuant_torch.bfloat16_32_32_0.01_8_...  NoQuant   \n",
       "5   en.layer1_NoQuant_torch.bfloat16_64_32_0.05_2_...  NoQuant   \n",
       "6   en.layer1_NoQuant_torch.bfloat16_64_32_0.05_4_...  NoQuant   \n",
       "7   en.layer1_NoQuant_torch.bfloat16_32_32_0.05_8_...  NoQuant   \n",
       "8   en.layer1_NoQuant_torch.bfloat16_16_32_0.05_2_...  NoQuant   \n",
       "9   en.layer1_NoQuant_torch.bfloat16_64_32_0.05_8_...  NoQuant   \n",
       "10  en.layer1_NoQuant_torch.bfloat16_32_32_0.05_4_...  NoQuant   \n",
       "11  en.layer1_NoQuant_torch.bfloat16_16_32_0.01_4_...  NoQuant   \n",
       "12  en.layer1_NoQuant_torch.bfloat16_16_32_0.05_4_...  NoQuant   \n",
       "13  en.layer1_NoQuant_torch.bfloat16_32_32_0.05_2_...  NoQuant   \n",
       "14  en.layer1_NoQuant_torch.bfloat16_16_32_0.05_4_...  NoQuant   \n",
       "15  en.layer1_NoQuant_torch.bfloat16_16_32_0.01_8_...  NoQuant   \n",
       "16  en.layer1_NoQuant_torch.bfloat16_32_32_0.01_4_...  NoQuant   \n",
       "17  en.layer1_NoQuant_torch.bfloat16_32_32_0.01_2_...  NoQuant   \n",
       "18  en.layer1_NoQuant_torch.bfloat16_16_32_0.05_8_...  NoQuant   \n",
       "\n",
       "   bnb_4bit_compute_dtype   r lora_alpha  lora_dropout  \\\n",
       "0          torch.bfloat16  16         32          0.05   \n",
       "1          torch.bfloat16  64         32          0.01   \n",
       "2          torch.bfloat16  16         32          0.01   \n",
       "3          torch.bfloat16  64         32          0.01   \n",
       "4          torch.bfloat16  32         32          0.01   \n",
       "5          torch.bfloat16  64         32          0.05   \n",
       "6          torch.bfloat16  64         32          0.05   \n",
       "7          torch.bfloat16  32         32          0.05   \n",
       "8          torch.bfloat16  16         32          0.05   \n",
       "9          torch.bfloat16  64         32          0.05   \n",
       "10         torch.bfloat16  32         32          0.05   \n",
       "11         torch.bfloat16  16         32          0.01   \n",
       "12         torch.bfloat16  16         32          0.05   \n",
       "13         torch.bfloat16  32         32          0.05   \n",
       "14         torch.bfloat16  16         32          0.05   \n",
       "15         torch.bfloat16  16         32          0.01   \n",
       "16         torch.bfloat16  32         32          0.01   \n",
       "17         torch.bfloat16  32         32          0.01   \n",
       "18         torch.bfloat16  16         32          0.05   \n",
       "\n",
       "    gradient_accumulation_steps  learning_rate  \n",
       "0                             2         0.0002  \n",
       "1                             8         0.0002  \n",
       "2                             2         0.0002  \n",
       "3                             2         0.0002  \n",
       "4                             8         0.0002  \n",
       "5                             2         0.0002  \n",
       "6                             4         0.0002  \n",
       "7                             8         0.0002  \n",
       "8                             2         0.0002  \n",
       "9                             8         0.0002  \n",
       "10                            4         0.0002  \n",
       "11                            4         0.0002  \n",
       "12                            4         0.0002  \n",
       "13                            2         0.0002  \n",
       "14                            4         0.0002  \n",
       "15                            8         0.0002  \n",
       "16                            4         0.0002  \n",
       "17                            2         0.0002  \n",
       "18                            8         0.0002  \n",
       "\n",
       "[19 rows x 22 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_params_from_file_name(df: pd.DataFrame):\n",
    "    df['model_type'] = df['file'].apply(lambda x: str(x.split('/')[1]))\n",
    "    df['model_configurations'] = df['file'].apply(lambda x: str(x.split('/')[2]))\n",
    "    if df['model_type'][0] == 'mistral':\n",
    "        df['model_size'] = 7\n",
    "        df['quantization'] = df['model_configurations'].apply(lambda x: str(x.split('_')[0]))\n",
    "        try:\n",
    "            df['fine_tuning'] = df['model_configurations'].apply(lambda x: str(x.split('_')[1]))\n",
    "        except IndexError:\n",
    "            df['fine_tuning'] = 'FT'\n",
    "    else:\n",
    "        df['model_size'] = df['model_configurations'].apply(lambda x: str(x.split('_')[0]))\n",
    "        df['quantization'] = df['model_configurations'].apply(lambda x: str(x.split('_')[1]))\n",
    "        df['fine_tuning'] = df['model_configurations'].apply(lambda x: str(x.split('_')[2]))\n",
    "\n",
    "    df['maxNewTokensFactor'] = df['file'].apply(lambda x: re.search(r'maxNewTokensFactor(\\d+)', x).group(1))\n",
    "    df['nShotsInference'] = df['file'].apply(lambda x: re.search(r'nShotsInference(\\d+)', x).group(1))\n",
    "    #\n",
    "    # df['layer'] = df['file'].apply(lambda x: re.search(r'adapters_(\\s+)', x).group(1))\n",
    "    df['model'] = df['file'].apply(lambda x: str(x.split('_adapters_')[0].split('nShotsInference')[1][2:].replace('.csv', '')))\n",
    "    if df['fine_tuning'][0] == 'FT':\n",
    "        df['training_params_string'] = df['file'].apply(lambda x: x.split('adapters_')[1])\n",
    "        df['nbit'] = df['training_params_string'].apply(lambda x: x.split('_')[1])\n",
    "        df['bnb_4bit_compute_dtype'] = df['training_params_string'].apply(lambda x: x.split('_')[2])\n",
    "        df['r'] = df['training_params_string'].apply(lambda x: int(x.split('_')[3]))\n",
    "        df['lora_alpha'] = df['training_params_string'].apply(lambda x: int(x.split('_')[4]))\n",
    "        df['lora_dropout'] = df['training_params_string'].apply(lambda x: float(x.split('_')[5]))\n",
    "        df['gradient_accumulation_steps'] = df['training_params_string'].apply(lambda x: int(x.split('_')[6]))\n",
    "        df['learning_rate'] = df['training_params_string'].apply(lambda x: x.split('_')[7].replace('.csv', ''))\n",
    "    elif df['fine_tuning'][0] in ['base', 'NoFT']:\n",
    "        df['training_params_string'] = None\n",
    "\n",
    "\n",
    "    return df\n",
    "file = 'data/evaluation_results/mistral_NoQuant_FT_wordsLevelTrue_evaluation.csv' # 'data/evaluation_results/llama_13B_8bit_base_wordsLevelTrue_evaluation.csv'\n",
    "tmp_df = pd.read_csv(file)\n",
    "print(tmp_df['file'][0])\n",
    "extract_params_from_file_name(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/evaluation_results/llama_13B_8bit_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/qwen_14B_8bit_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/mistral_8bit_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/qwen_7B_8bit_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/llama_13B_NoQuantbit_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/qwen_7B_4bit_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/mistral_4bits_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/llama_7B_NoQuantbit_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/evaluation_results_old_version_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/llama_13B_4bit_FT_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/mistral_4bit_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/llama_7B_4bit_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/qwen_7B_4bit_FT_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/mistral_NoQuant_FT_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/llama_13B_4bit_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/mistral_NoQuant_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/mistral_8bit_base_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/mistral_noInstr_8bit_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/llama_7B_4bit_FT_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/llama_7B_NoQuant_FT_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/qwen_14B_4bit_FT_wordsLevelTrue_evaluation.csv',\n",
       " 'data/evaluation_results/llama_7B_8bit_base_wordsLevelTrue_evaluation.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE:  data/evaluation_results/llama_13B_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "data/llama/13B_8bit_base/maxNewTokensFactor8_nShotsInference0_BaseModel.csv\n",
      "SOURCE:  data/evaluation_results/qwen_14B_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "data/qwen/14B_8bit_base/maxNewTokensFactor4_nShotsInference0_BaseModel.csv\n",
      "SOURCE:  data/evaluation_results/mistral_8bit_wordsLevelTrue_evaluation.csv\n",
      "data/mistral/8bit/maxNewTokensFactor8_nShotsInference2_Mistral-7B-Instruct-v0.2_adapters_en.layer1_8_torch.bfloat16_64_32_0.01_8_0.0002.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "data/qwen/7B_8bit_base/maxNewTokensFactor4_nShotsInference0_BaseModel.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_NoQuantbit_base_wordsLevelTrue_evaluation.csv\n",
      "data/llama/13B_NoQuantbit_base/maxNewTokensFactor2_nShotsInference4_BaseModel_Llama-2-13b-chat-hf_NoQuant.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "data/qwen/7B_4bit_base/maxNewTokensFactor4_nShotsInference0_BaseModel.csv\n",
      "SOURCE:  data/evaluation_results/mistral_4bits_base_wordsLevelTrue_evaluation.csv\n",
      "data/mistral/4bits_base/maxNewTokensFactor8_nShotsInference0_BaseModel.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_NoQuantbit_base_wordsLevelTrue_evaluation.csv\n",
      "data/llama/7B_NoQuantbit_base/maxNewTokensFactor2_nShotsInference0_BaseModel_Llama-2-7b-chat-hf_NoQuant.csv\n",
      "EMPTY FILE:  data/evaluation_results/evaluation_results_old_version_wordsLevelTrue_evaluation.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "data/llama/13B_4bit_FT/maxNewTokensFactor2_nShotsInference2_llama-2-13b-chat-hf_adapters_en.layer1_4_torch.bfloat16_32_32_0.01_8_0.0002.csv\n",
      "SOURCE:  data/evaluation_results/mistral_4bit_wordsLevelTrue_evaluation.csv\n",
      "data/mistral/4bit/maxNewTokensFactor4_nShotsInference2_Mistral-7B-Instruct-v0.2_adapters_en.layer1_4_torch.bfloat16_64_32_0.01_8_0.002.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "data/llama/7B_4bit_base/maxNewTokensFactor8_nShotsInference0_BaseModel.csv\n",
      "SOURCE:  data/evaluation_results/qwen_7B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "data/qwen/7B_4bit_FT/maxNewTokensFactor4_nShotsInference2_qwen1.5-7b-chat__adapters_en.layer1_4_torch.bfloat16_64_64_0.01_4_0.0002.csv\n",
      "SOURCE:  data/evaluation_results/mistral_NoQuant_FT_wordsLevelTrue_evaluation.csv\n",
      "data/mistral/NoQuant_FT/maxNewTokensFactor4_nShotsInference0_mistral-7b-instruct-v0.2__adapters_en.layer1_NoQuant_torch.bfloat16_16_32_0.05_2_0.0002.csv\n",
      "SOURCE:  data/evaluation_results/llama_13B_4bit_base_wordsLevelTrue_evaluation.csv\n",
      "data/llama/13B_4bit_base/maxNewTokensFactor8_nShotsInference0_BaseModel.csv\n",
      "SOURCE:  data/evaluation_results/mistral_NoQuant_base_wordsLevelTrue_evaluation.csv\n",
      "data/mistral/NoQuant_base/maxNewTokensFactor2_nShotsInference0_BaseModel.csv\n",
      "SOURCE:  data/evaluation_results/mistral_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "data/mistral/8bit_base/maxNewTokensFactor8_nShotsInference0_BaseModel.csv\n",
      "SOURCE:  data/evaluation_results/mistral_noInstr_8bit_wordsLevelTrue_evaluation.csv\n",
      "data/mistral/noInstr_8bit/maxNewTokensFactor8_nShotsInference0_Mistral-7B-v0.1_simplest_prompt_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_8_0.0002.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "data/llama/7B_4bit_FT/maxNewTokensFactor8_nShotsInference2_llama-2-7b-chat-hf_adapters_en.layer1_4_torch.bfloat16_16_32_0.01_4_0.0002.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_NoQuant_FT_wordsLevelTrue_evaluation.csv\n",
      "data/llama/7B_NoQuant_FT/maxNewTokensFactor8_nShotsInference2_llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_32_32_0.01_4_0.0002.csv\n",
      "SOURCE:  data/evaluation_results/qwen_14B_4bit_FT_wordsLevelTrue_evaluation.csv\n",
      "data/qwen/14B_4bit_FT/maxNewTokensFactor8_nShotsInference0_qwen1.5-14b-chat__adapters_en.layer1_4_torch.bfloat16_32_32_0.05_16_0.0002.csv\n",
      "SOURCE:  data/evaluation_results/llama_7B_8bit_base_wordsLevelTrue_evaluation.csv\n",
      "data/llama/7B_8bit_base/maxNewTokensFactor8_nShotsInference0_BaseModel.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(tmp_df)\n\u001b[1;32m     17\u001b[0m res \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs)\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/evaluation_results/joint_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilar_is_equal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilar_is_equal_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(empty_n)\n\u001b[1;32m     20\u001b[0m res\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "csv_files = glob.glob('data/evaluation_results/*.csv')\n",
    "dfs = []\n",
    "empty_n = 0\n",
    "for file in csv_files:\n",
    "    tmp_df = pd.read_csv(file)\n",
    "    if tmp_df.shape[0] == 0:\n",
    "        print('EMPTY FILE: ', file)\n",
    "        empty_n += 1\n",
    "        continue\n",
    "    print('SOURCE: ', file)\n",
    "    print(tmp_df['file'][0])\n",
    "    tmp_df = extract_params_from_file_name(tmp_df)\n",
    "    dfs.append(tmp_df)\n",
    "\n",
    "res = pd.concat(dfs)#.to_csv('data/evaluation_results/joint_results.csv')\n",
    "res = res.drop(columns=['similar_is_equal', 'similar_is_equal_threshold'])\n",
    "print(empty_n)\n",
    "res.sort_values(by='f1_score', ascending=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
