{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/main/peft#add-additional-trainable-layers-to-a-peft-adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.evaluator import Evaluator\n",
    "from config.finetuning import config\n",
    "from config import postprocessing_params_mistral as postprocessing\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "import pandas as pd\n",
    "from log import mistral_8bits as models_params\n",
    "from utils.generate_ft_adapters_list import generate_ft_adapters_list\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "max_new_tokens_factor_list = [8]#postprocessing.max_new_tokens_factor_list\n",
    "n_shots_inference_list = [2]#postprocessing.n_shots_inference_list\n",
    "layer = 'it.layer1'\n",
    "language = layer.split('.')[0]\n",
    "\n",
    "BASE_MODEL_CHECKPOINT = \"mii-community/zefiro-7b-sft-ITA\"\n",
    "\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n",
    "                                \n",
    "preprocessor = DataPreprocessor(model_checkpoint=BASE_MODEL_CHECKPOINT, \n",
    "                                tokenizer = BASE_MODEL_CHECKPOINT)\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset,\n",
    "                                                instruction_on_response_format=models_params.instruction_on_response_format,\n",
    "                                                 simplest_prompt=models_params.simplest_prompt)\n",
    "_, val_data, _ = preprocessor.split_layer_into_train_val_test_(dataset, layer)\n",
    "\n",
    "load_in_8bit = not models_params.load_in_4bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = models_params.load_in_4bit[0],\n",
    "            load_in_8bit = load_in_8bit,\n",
    "            bnb_4bit_use_double_quant = models_params.bnb_4bit_use_double_quant,\n",
    "            bnb_4bit_quant_type = models_params.bnb_4bit_quant_type[0],\n",
    "            bnb_4bit_compute_dtype = models_params.bnb_4bit_compute_dtype[0],\n",
    "            llm_int8_threshold = models_params.llm_int8_threshold[0],\n",
    "            llm_int8_has_fp16_weight = models_params.llm_int8_has_fp16_weight,\n",
    "            llm_int8_skip_modules = models_params.llm_int8_skip_modules\n",
    "            )\n",
    "\n",
    "\n",
    "adapters_list = generate_ft_adapters_list(\"mistral_8bits\", simplest_prompt=models_params.simplest_prompt)\n",
    "adapters = adapters_list[0]\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    models_params.BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "    quantization_config = bnb_config,\n",
    "    return_dict=True,  \n",
    "    #torch_dtype=torch.float16,\n",
    "    device_map= \"auto\")\n",
    "merged_model = PeftModel.from_pretrained(base_model, adapters_list[0], token=HF_TOKEN, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model2 = PeftModel.from_pretrained(merged_model, adapters_list[1], token=HF_TOKEN, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model3 = merged_model.load_adapter(adapters_list[3], adapter_name='adapt2', token=HF_TOKEN, device_map='auto', bias='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "postprocessor = TestDataProcessor(test_data=val_data.select(range(12)), \n",
    "                                              preprocessor=preprocessor, \n",
    "                                              n_shots_inference=1, \n",
    "                                              language=language, \n",
    "                                              tokenizer=tokenizer)\n",
    "postprocessor.add_inference_prompt_column(simplest_prompt=models_params.simplest_prompt)\n",
    "postprocessor.add_ground_truth_column()\n",
    "postprocessor.add_responses_column(model=merged_model, \n",
    "                                                tokenizer=tokenizer, \n",
    "                                                batch_size=6, \n",
    "                                                max_new_tokens_factor=4)\n",
    "res1 = postprocessor.test_data\n",
    "\n",
    "postprocessor = TestDataProcessor(test_data=val_data.select(range(12)), \n",
    "                                              preprocessor=preprocessor, \n",
    "                                              n_shots_inference=1, \n",
    "                                              language=language, \n",
    "                                              tokenizer=tokenizer)\n",
    "postprocessor.add_inference_prompt_column(simplest_prompt=models_params.simplest_prompt)\n",
    "postprocessor.add_ground_truth_column()\n",
    "postprocessor.add_responses_column(model=merged_model, \n",
    "                                                tokenizer=tokenizer, \n",
    "                                                batch_size=6, \n",
    "                                                max_new_tokens_factor=4)\n",
    "res2 = postprocessor.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1['model_responses']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
