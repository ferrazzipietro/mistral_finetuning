2024-02-27 13:30:12.692434: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-27 13:30:12.746844: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-27 13:30:16.995689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
wandb: Currently logged in as: ferrazzipietro. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/ferrazzi/.netrc
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /extra/ferrazzi/llm/mistral_finetuning/wandb/run-20240227_133034-gr5zht2z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run en.layer12024-02-27 13:30:34
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ferrazzipietro/Mistral-7B-v0.1_adapters_en.layer1_4_torch.bfloat16_32_32_0.01_4_0.0002
wandb: üöÄ View run at https://wandb.ai/ferrazzipietro/Mistral-7B-v0.1_adapters_en.layer1_4_torch.bfloat16_32_32_0.01_4_0.0002/runs/gr5zht2z
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:25<00:25, 25.50s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:37<00:00, 17.65s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:37<00:00, 18.83s/it]
Map:   0%|          | 0/1520 [00:00<?, ? examples/s]Map:  13%|‚ñà‚ñé        | 202/1520 [00:00<00:00, 1641.25 examples/s]Map:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 806/1520 [00:00<00:00, 3999.57 examples/s]Map:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1331/1520 [00:00<00:00, 4130.05 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 3479.25 examples/s]
Map:   0%|          | 0/1520 [00:00<?, ? examples/s]Map:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1000/1520 [00:00<00:00, 5344.63 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 4353.28 examples/s]
Map:   0%|          | 0/170 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [00:00<00:00, 2359.05 examples/s]
Map:   0%|          | 0/669 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 669/669 [00:00<00:00, 5495.01 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 669/669 [00:00<00:00, 4765.79 examples/s]
Map:   0%|          | 0/681 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:00<00:00, 6110.39 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:00<00:00, 4818.95 examples/s]
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:284: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/63 [00:00<?, ?it/s]  2%|‚ñè         | 1/63 [00:08<08:48,  8.53s/it]  3%|‚ñé         | 2/63 [00:13<06:23,  6.29s/it]  5%|‚ñç         | 3/63 [00:17<05:16,  5.28s/it]  6%|‚ñã         | 4/63 [00:21<04:35,  4.67s/it]  8%|‚ñä         | 5/63 [00:24<03:59,  4.13s/it] 10%|‚ñâ         | 6/63 [00:31<04:47,  5.04s/it] 11%|‚ñà         | 7/63 [00:35<04:35,  4.92s/it] 13%|‚ñà‚ñé        | 8/63 [00:39<04:17,  4.68s/it] 14%|‚ñà‚ñç        | 9/63 [00:43<03:57,  4.39s/it] 16%|‚ñà‚ñå        | 10/63 [00:46<03:34,  4.04s/it]                                                16%|‚ñà‚ñå        | 10/63 [00:46<03:34,  4.04s/it] 17%|‚ñà‚ñã        | 11/63 [00:54<04:33,  5.27s/it] 19%|‚ñà‚ñâ        | 12/63 [00:59<04:21,  5.13s/it] 21%|‚ñà‚ñà        | 13/63 [01:03<04:02,  4.85s/it] 22%|‚ñà‚ñà‚ñè       | 14/63 [01:07<03:42,  4.53s/it] 24%|‚ñà‚ñà‚ñç       | 15/63 [01:11<03:19,  4.16s/it] 25%|‚ñà‚ñà‚ñå       | 16/63 [01:18<03:56,  5.03s/it] 27%|‚ñà‚ñà‚ñã       | 17/63 [01:22<03:48,  4.96s/it] 29%|‚ñà‚ñà‚ñä       | 18/63 [01:27<03:32,  4.73s/it] 30%|‚ñà‚ñà‚ñà       | 19/63 [01:30<03:14,  4.42s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [01:34<02:55,  4.08s/it]                                                32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [01:34<02:55,  4.08s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [01:38<03:01,  4.32s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [01:46<03:39,  5.35s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [01:51<03:25,  5.13s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [01:55<03:08,  4.82s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [01:59<02:51,  4.50s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [02:02<02:33,  4.14s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [02:09<02:58,  4.95s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [02:14<02:52,  4.94s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [02:18<02:41,  4.76s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [02:22<02:27,  4.48s/it]                                                48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [02:22<02:27,  4.48s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [02:25<02:11,  4.12s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [02:33<02:45,  5.34s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [02:38<02:36,  5.22s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [02:43<02:22,  4.92s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [02:46<02:07,  4.56s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [02:49<01:52,  4.16s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [02:57<02:13,  5.13s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [03:02<02:06,  5.05s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [03:06<01:55,  4.81s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [03:10<01:43,  4.51s/it]                                                63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [03:10<01:43,  4.51s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [03:13<01:31,  4.15s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [03:18<01:30,  4.29s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [03:26<01:49,  5.48s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [03:31<01:40,  5.27s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [03:35<01:29,  4.96s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [03:39<01:19,  4.65s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [03:42<01:08,  4.27s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [03:49<01:16,  5.11s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [03:54<01:10,  5.04s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [03:58<01:02,  4.79s/it]                                                79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [03:58<01:02,  4.79s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 51/63 [04:02<00:53,  4.48s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 52/63 [04:05<00:45,  4.09s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 53/63 [04:12<00:48,  4.89s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 54/63 [04:17<00:43,  4.87s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 55/63 [04:21<00:37,  4.66s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/63 [04:25<00:30,  4.39s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 57/63 [04:28<00:24,  4.06s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [04:35<00:24,  5.00s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 59/63 [04:40<00:19,  4.98s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [04:45<00:14,  4.75s/it]                                                95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [04:45<00:14,  4.75s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 61/63 [04:48<00:08,  4.45s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 62/63 [04:52<00:04,  4.08s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [04:58<00:00,  4.80s/it]                                               100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [04:58<00:00,  4.80s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [04:58<00:00,  4.74s/it]
adapter_model.safetensors:   0%|          | 0.00/185M [00:00<?, ?B/s]adapter_model.safetensors:   0%|          | 16.4k/185M [00:00<20:15, 152kB/s]adapter_model.safetensors:   1%|          | 1.95M/185M [00:00<00:18, 9.95MB/s]adapter_model.safetensors:   2%|‚ñè         | 2.92M/185M [00:00<00:18, 9.76MB/s]adapter_model.safetensors:   2%|‚ñè         | 3.88M/185M [00:00<00:31, 5.73MB/s]adapter_model.safetensors:   3%|‚ñé         | 5.10M/185M [00:00<00:25, 7.13MB/s]adapter_model.safetensors:   4%|‚ñé         | 6.57M/185M [00:00<00:20, 8.67MB/s]adapter_model.safetensors:   4%|‚ñç         | 8.06M/185M [00:00<00:18, 9.64MB/s]adapter_model.safetensors:   5%|‚ñå         | 9.54M/185M [00:01<00:17, 10.3MB/s]adapter_model.safetensors:   6%|‚ñå         | 11.0M/185M [00:01<00:16, 10.7MB/s]adapter_model.safetensors:   7%|‚ñã         | 12.5M/185M [00:01<00:15, 11.0MB/s]adapter_model.safetensors:   8%|‚ñä         | 14.0M/185M [00:01<00:15, 11.2MB/s]adapter_model.safetensors:   8%|‚ñä         | 15.5M/185M [00:01<00:14, 11.4MB/s]adapter_model.safetensors:   9%|‚ñâ         | 16.7M/185M [00:01<00:23, 7.17MB/s]adapter_model.safetensors:  10%|‚ñà         | 18.7M/185M [00:02<00:20, 8.20MB/s]adapter_model.safetensors:  11%|‚ñà         | 19.7M/185M [00:02<00:22, 7.22MB/s]adapter_model.safetensors:  11%|‚ñà         | 20.5M/185M [00:02<00:23, 6.91MB/s]adapter_model.safetensors:  12%|‚ñà‚ñè        | 21.9M/185M [00:02<00:21, 7.69MB/s]adapter_model.safetensors:  13%|‚ñà‚ñé        | 23.5M/185M [00:02<00:18, 8.92MB/s]adapter_model.safetensors:  14%|‚ñà‚ñé        | 24.9M/185M [00:02<00:16, 9.65MB/s]adapter_model.safetensors:  14%|‚ñà‚ñç        | 26.4M/185M [00:02<00:15, 10.2MB/s]adapter_model.safetensors:  15%|‚ñà‚ñå        | 27.8M/185M [00:03<00:14, 10.6MB/s]adapter_model.safetensors:  16%|‚ñà‚ñå        | 29.3M/185M [00:03<00:14, 10.9MB/s]adapter_model.safetensors:  17%|‚ñà‚ñã        | 30.8M/185M [00:03<00:14, 10.9MB/s]adapter_model.safetensors:  17%|‚ñà‚ñã        | 32.0M/185M [00:03<00:22, 6.73MB/s]adapter_model.safetensors:  19%|‚ñà‚ñâ        | 34.8M/185M [00:03<00:15, 9.66MB/s]adapter_model.safetensors:  20%|‚ñà‚ñâ        | 36.2M/185M [00:04<00:15, 9.48MB/s]adapter_model.safetensors:  20%|‚ñà‚ñà        | 37.7M/185M [00:04<00:15, 9.69MB/s]adapter_model.safetensors:  21%|‚ñà‚ñà        | 39.1M/185M [00:04<00:14, 10.1MB/s]adapter_model.safetensors:  22%|‚ñà‚ñà‚ñè       | 40.6M/185M [00:04<00:13, 10.4MB/s]adapter_model.safetensors:  23%|‚ñà‚ñà‚ñé       | 42.1M/185M [00:04<00:13, 10.7MB/s]adapter_model.safetensors:  24%|‚ñà‚ñà‚ñé       | 43.6M/185M [00:04<00:12, 11.0MB/s]adapter_model.safetensors:  24%|‚ñà‚ñà‚ñç       | 45.0M/185M [00:04<00:12, 11.2MB/s]adapter_model.safetensors:  25%|‚ñà‚ñà‚ñå       | 46.5M/185M [00:04<00:12, 11.4MB/s]adapter_model.safetensors:  26%|‚ñà‚ñà‚ñå       | 48.0M/185M [00:05<00:11, 11.5MB/s]adapter_model.safetensors:  27%|‚ñà‚ñà‚ñã       | 49.2M/185M [00:05<00:17, 7.76MB/s]adapter_model.safetensors:  28%|‚ñà‚ñà‚ñä       | 50.8M/185M [00:05<00:16, 8.05MB/s]adapter_model.safetensors:  28%|‚ñà‚ñà‚ñä       | 51.7M/185M [00:05<00:19, 6.99MB/s]adapter_model.safetensors:  28%|‚ñà‚ñà‚ñä       | 52.5M/185M [00:05<00:19, 6.65MB/s]adapter_model.safetensors:  29%|‚ñà‚ñà‚ñâ       | 53.7M/185M [00:06<00:17, 7.30MB/s]adapter_model.safetensors:  30%|‚ñà‚ñà‚ñâ       | 55.1M/185M [00:06<00:15, 8.43MB/s]adapter_model.safetensors:  31%|‚ñà‚ñà‚ñà       | 56.6M/185M [00:06<00:13, 9.30MB/s]adapter_model.safetensors:  31%|‚ñà‚ñà‚ñà‚ñè      | 58.1M/185M [00:06<00:12, 9.97MB/s]adapter_model.safetensors:  32%|‚ñà‚ñà‚ñà‚ñè      | 59.6M/185M [00:06<00:11, 10.5MB/s]adapter_model.safetensors:  33%|‚ñà‚ñà‚ñà‚ñé      | 61.0M/185M [00:06<00:11, 10.8MB/s]adapter_model.safetensors:  34%|‚ñà‚ñà‚ñà‚ñç      | 62.5M/185M [00:06<00:11, 11.1MB/s]adapter_model.safetensors:  35%|‚ñà‚ñà‚ñà‚ñç      | 64.0M/185M [00:06<00:10, 11.3MB/s]adapter_model.safetensors:  35%|‚ñà‚ñà‚ñà‚ñå      | 65.1M/185M [00:07<00:15, 7.82MB/s]adapter_model.safetensors:  36%|‚ñà‚ñà‚ñà‚ñå      | 66.8M/185M [00:07<00:14, 8.03MB/s]adapter_model.safetensors:  37%|‚ñà‚ñà‚ñà‚ñã      | 67.7M/185M [00:07<00:16, 6.95MB/s]adapter_model.safetensors:  37%|‚ñà‚ñà‚ñà‚ñã      | 68.4M/185M [00:07<00:17, 6.55MB/s]adapter_model.safetensors:  38%|‚ñà‚ñà‚ñà‚ñä      | 69.6M/185M [00:07<00:15, 7.25MB/s]adapter_model.safetensors:  38%|‚ñà‚ñà‚ñà‚ñä      | 71.0M/185M [00:07<00:13, 8.40MB/s]adapter_model.safetensors:  39%|‚ñà‚ñà‚ñà‚ñâ      | 72.5M/185M [00:08<00:12, 9.25MB/s]adapter_model.safetensors:  40%|‚ñà‚ñà‚ñà‚ñà      | 73.9M/185M [00:08<00:11, 9.95MB/s]adapter_model.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà      | 75.4M/185M [00:08<00:10, 10.5MB/s]adapter_model.safetensors:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 76.9M/185M [00:08<00:09, 10.9MB/s]adapter_model.safetensors:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 78.4M/185M [00:08<00:09, 11.1MB/s]adapter_model.safetensors:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 79.8M/185M [00:08<00:09, 11.3MB/s]adapter_model.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 81.0M/185M [00:08<00:13, 7.57MB/s]adapter_model.safetensors:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 82.8M/185M [00:09<00:11, 9.02MB/s]adapter_model.safetensors:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 84.2M/185M [00:09<00:10, 9.69MB/s]adapter_model.safetensors:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 85.6M/185M [00:09<00:09, 10.2MB/s]adapter_model.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 87.1M/185M [00:09<00:09, 10.6MB/s]adapter_model.safetensors:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 88.6M/185M [00:09<00:08, 10.9MB/s]adapter_model.safetensors:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 90.1M/185M [00:09<00:08, 11.2MB/s]adapter_model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 91.5M/185M [00:09<00:08, 11.3MB/s]adapter_model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 92.9M/185M [00:09<00:08, 11.4MB/s]adapter_model.safetensors:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 94.4M/185M [00:10<00:07, 11.5MB/s]adapter_model.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 96.0M/185M [00:10<00:12, 6.98MB/s]adapter_model.safetensors:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 98.7M/185M [00:10<00:09, 8.80MB/s]adapter_model.safetensors:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 99.8M/185M [00:10<00:10, 7.90MB/s]adapter_model.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 101M/185M [00:11<00:11, 7.56MB/s] adapter_model.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 102M/185M [00:11<00:10, 8.01MB/s]adapter_model.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 103M/185M [00:11<00:09, 8.92MB/s]adapter_model.safetensors:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 105M/185M [00:11<00:08, 9.61MB/s]adapter_model.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 106M/185M [00:11<00:07, 10.2MB/s]adapter_model.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 108M/185M [00:11<00:07, 10.6MB/s]adapter_model.safetensors:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 109M/185M [00:11<00:06, 10.8MB/s]adapter_model.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 111M/185M [00:11<00:06, 11.2MB/s]adapter_model.safetensors:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 112M/185M [00:12<00:10, 6.95MB/s]adapter_model.safetensors:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 115M/185M [00:12<00:07, 9.00MB/s]adapter_model.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 116M/185M [00:12<00:08, 7.96MB/s]adapter_model.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 117M/185M [00:12<00:08, 7.74MB/s]adapter_model.safetensors:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 118M/185M [00:12<00:08, 7.85MB/s]adapter_model.safetensors:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 119M/185M [00:13<00:07, 8.87MB/s]adapter_model.safetensors:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 121M/185M [00:13<00:06, 9.62MB/s]adapter_model.safetensors:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 122M/185M [00:13<00:06, 10.2MB/s]adapter_model.safetensors:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 124M/185M [00:13<00:05, 10.6MB/s]adapter_model.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 125M/185M [00:13<00:05, 11.0MB/s]adapter_model.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 127M/185M [00:13<00:05, 11.2MB/s]adapter_model.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 128M/185M [00:14<00:08, 6.82MB/s]adapter_model.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 131M/185M [00:14<00:06, 8.93MB/s]adapter_model.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 132M/185M [00:14<00:06, 7.86MB/s]adapter_model.safetensors:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 133M/185M [00:14<00:06, 7.49MB/s]adapter_model.safetensors:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 134M/185M [00:14<00:06, 7.59MB/s]adapter_model.safetensors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 135M/185M [00:14<00:05, 8.60MB/s]adapter_model.safetensors:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 137M/185M [00:15<00:05, 9.41MB/s]adapter_model.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 138M/185M [00:15<00:04, 10.0MB/s]adapter_model.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 140M/185M [00:15<00:04, 10.5MB/s]adapter_model.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 141M/185M [00:15<00:04, 10.9MB/s]adapter_model.safetensors:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 142M/185M [00:15<00:03, 11.1MB/s]adapter_model.safetensors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 144M/185M [00:15<00:03, 11.3MB/s]adapter_model.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 145M/185M [00:15<00:05, 7.57MB/s]adapter_model.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 147M/185M [00:16<00:04, 7.83MB/s]adapter_model.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 148M/185M [00:16<00:05, 6.82MB/s]adapter_model.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 148M/185M [00:16<00:05, 6.90MB/s]adapter_model.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 150M/185M [00:16<00:04, 7.58MB/s]adapter_model.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 151M/185M [00:16<00:03, 8.68MB/s]adapter_model.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 153M/185M [00:16<00:03, 9.51MB/s]adapter_model.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 154M/185M [00:16<00:03, 10.1MB/s]adapter_model.safetensors:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 156M/185M [00:17<00:02, 10.6MB/s]adapter_model.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 157M/185M [00:17<00:02, 10.9MB/s]adapter_model.safetensors:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 158M/185M [00:17<00:02, 11.1MB/s]adapter_model.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 160M/185M [00:17<00:02, 11.3MB/s]adapter_model.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 161M/185M [00:17<00:03, 7.54MB/s]adapter_model.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 163M/185M [00:17<00:02, 8.81MB/s]adapter_model.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 164M/185M [00:18<00:02, 9.53MB/s]adapter_model.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 166M/185M [00:18<00:01, 10.1MB/s]adapter_model.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 167M/185M [00:18<00:01, 10.6MB/s]adapter_model.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 169M/185M [00:18<00:01, 10.9MB/s]adapter_model.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 170M/185M [00:18<00:01, 11.1MB/s]adapter_model.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 171M/185M [00:18<00:01, 11.3MB/s]adapter_model.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 173M/185M [00:18<00:01, 11.4MB/s]adapter_model.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 174M/185M [00:18<00:00, 11.5MB/s]adapter_model.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 176M/185M [00:19<00:00, 11.6MB/s]adapter_model.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 177M/185M [00:19<00:00, 8.00MB/s]adapter_model.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 179M/185M [00:19<00:00, 9.23MB/s]adapter_model.safetensors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 180M/185M [00:19<00:00, 9.86MB/s]adapter_model.safetensors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 182M/185M [00:19<00:00, 10.3MB/s]adapter_model.safetensors:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 183M/185M [00:19<00:00, 10.7MB/s]adapter_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 185M/185M [00:19<00:00, 11.0MB/s]adapter_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 185M/185M [00:20<00:00, 9.12MB/s]
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.029 MB of 0.050 MB uploadedwandb: \ 0.029 MB of 0.050 MB uploadedwandb: | 0.050 MB of 0.050 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:                    train/epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà
wandb:              train/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà
wandb:                train/grad_norm ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ
wandb:            train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                     train/loss ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:               train/total_flos ‚ñÅ
wandb:               train/train_loss ‚ñÅ‚ñÅ
wandb:            train/train_runtime ‚ñÅ‚ñÅ
wandb: train/train_samples_per_second ‚ñÅ‚ñÅ
wandb:   train/train_steps_per_second ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    train/epoch 3.0
wandb:              train/global_step 63
wandb:                train/grad_norm 0.93674
wandb:            train/learning_rate 0.0002
wandb:                     train/loss 0.483
wandb:               train/total_flos 8049980074106880.0
wandb:               train/train_loss 0.90745
wandb:            train/train_runtime 298.5122
wandb: train/train_samples_per_second 6.723
wandb:   train/train_steps_per_second 0.211
wandb: 
wandb: üöÄ View run en.layer12024-02-27 13:30:34 at: https://wandb.ai/ferrazzipietro/Mistral-7B-v0.1_adapters_en.layer1_4_torch.bfloat16_32_32_0.01_4_0.0002/runs/gr5zht2z
wandb: Ô∏è‚ö° View job at https://wandb.ai/ferrazzipietro/Mistral-7B-v0.1_adapters_en.layer1_4_torch.bfloat16_32_32_0.01_4_0.0002/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MzQ3NTkyMg==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240227_133034-gr5zht2z/logs
wandb: Appending key for api.wandb.ai to your netrc file: /home/ferrazzi/.netrc
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /extra/ferrazzi/llm/mistral_finetuning/wandb/run-20240227_133726-4l1lb5rt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run en.layer12024-02-27 13:37:26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ferrazzipietro/Mistral-7B-v0.1_adapters_en.layer1_8_torch.bfloat16_32_32_0.01_4_0.0002
wandb: üöÄ View run at https://wandb.ai/ferrazzipietro/Mistral-7B-v0.1_adapters_en.layer1_8_torch.bfloat16_32_32_0.01_4_0.0002/runs/4l1lb5rt
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.53s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.54s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.84s/it]
Map:   0%|          | 0/681 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:00<00:00, 5205.24 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:00<00:00, 4631.58 examples/s]
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:284: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/63 [00:00<?, ?it/s]/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: üöÄ View run en.layer12024-02-27 13:37:26 at: https://wandb.ai/ferrazzipietro/Mistral-7B-v0.1_adapters_en.layer1_8_torch.bfloat16_32_32_0.01_4_0.0002/runs/4l1lb5rt
wandb: Ô∏è‚ö° View job at https://wandb.ai/ferrazzipietro/Mistral-7B-v0.1_adapters_en.layer1_8_torch.bfloat16_32_32_0.01_4_0.0002/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MzQ3NjIwOA==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240227_133726-4l1lb5rt/logs
Traceback (most recent call last):
  File "finetuning_iterative_mistral.py", line 210, in <module>
    main(ADAPTERS_CHECKPOINT,
  File "finetuning_iterative_mistral.py", line 171, in main
    trainer.train()
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/trl/trainer/sft_trainer.py", line 317, in train
    output = super().train(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/trainer.py", line 1615, in train
    return inner_training_loop(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/trainer.py", line 1961, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/trainer.py", line 2902, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/trainer.py", line 2925, in compute_loss
    outputs = model(**inputs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/utils/operations.py", line 687, in forward
    return model_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/utils/operations.py", line 675, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 14, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/peft_model.py", line 1003, in forward
    return self.base_model(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/peft/tuners/tuners_utils.py", line 107, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1157, in forward
    outputs = self.model(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 1032, in forward
    layer_outputs = self._gradient_checkpointing_func(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 249, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 107, in forward
    outputs = run_function(*args)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 770, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py", line 179, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/bitsandbytes/nn/modules.py", line 450, in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py", line 562, in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py", line 421, in forward
    output += torch.matmul(subA, state.subB)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 23.69 GiB total capacity; 20.87 GiB already allocated; 23.69 MiB free; 22.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread IntMsgThr:
Traceback (most recent call last):
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/threading.py", line 870, in run
    self.run()
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/threading.py", line 870, in run
    self._target(*self._args, **self._kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 300, in check_internal_messages
    self._target(*self._args, **self._kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 268, in check_network_status
    self._loop_check_status(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    self._loop_check_status(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/wandb_run.py", line 224, in _loop_check_status
    local_handle = request()
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 758, in deliver_network_status
    local_handle = request()
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/interface/interface.py", line 766, in deliver_internal_messages
    return self._deliver_internal_messages(internal_message)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 490, in _deliver_internal_messages
    return self._deliver_network_status(status)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 484, in _deliver_network_status
    return self._deliver_record(record)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 437, in _deliver_record
    return self._deliver_record(record)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py", line 437, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    interface._publish(record)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self._sock_client.send_record_publish(record)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self.send_server_request(server_req)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._send_message(msg)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    self._sendall_with_error_handle(header + data)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
