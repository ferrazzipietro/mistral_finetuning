2024-02-02 12:15:56.073407: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-02-02 12:15:56.172152: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-02-02 12:15:58.965583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
wandb: Currently logged in as: ferrazzipietro. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/ferrazzi/.netrc
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /extra/ferrazzi/mistral_finetuning/wandb/run-20240202_121612-9i2b2xph
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-dew-44
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ferrazzipietro/Fine%20tuning%20en.layer1
wandb: üöÄ View run at https://wandb.ai/ferrazzipietro/Fine%20tuning%20en.layer1/runs/9i2b2xph

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:07<00:15,  7.72s/it]
Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:15<00:07,  7.78s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:22<00:00,  7.51s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:22<00:00,  7.58s/it]

Map:   0%|          | 0/170 [00:00<?, ? examples/s]
Map:   0%|          | 0/170 [00:00<?, ? examples/s]
wandb: - 0.002 MB of 0.012 MB uploaded
wandb: \ 0.010 MB of 0.012 MB uploaded
wandb: üöÄ View run light-dew-44 at: https://wandb.ai/ferrazzipietro/Fine%20tuning%20en.layer1/runs/9i2b2xph
wandb: Ô∏è‚ö° View job at https://wandb.ai/ferrazzipietro/Fine%20tuning%20en.layer1/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNTY2MjQxNg==/version_details/v2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240202_121612-9i2b2xph/logs
Traceback (most recent call last):
  File "finetuning_v2.py", line 65, in <module>
    train_data, val_data, test_data = preprocessor.split_layer_into_train_val_test_(dataset, config.TRAIN_LAYER)
  File "/extra/ferrazzi/mistral_finetuning/utils/data_preprocessor.py", line 245, in split_layer_into_train_val_test_
    test_data.map(lambda x: remove_answer_from_prompt(x, prompt_template), batched=True)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 591, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 556, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3089, in map
    for rank, done, content in Dataset._map_single(**dataset_kwargs):
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3466, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3356, in apply_function_on_filtered_inputs
    validate_function_output(processed_inputs, indices)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3322, in validate_function_output
    raise TypeError(
TypeError: Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'list'>, <class 'str'>, <class 'list'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>, <class 'pandas.core.series.Series'>, <class 'tensorflow.python.framework.ops.Tensor'>, <class 'torch.Tensor'>)`.
