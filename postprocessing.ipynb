{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_model_tokenizer' from 'utils.load_merged_model_tokenizer' (/home/pferrazzi/mistral_finetuning/utils/load_merged_model_tokenizer.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Evaluator\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinetuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_merged_model_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model_tokenizer\n\u001b[1;32m      8\u001b[0m HF_TOKEN \u001b[38;5;241m=\u001b[39m dotenv_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.env.base\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHF_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m adapters_checkpoints \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mferrazzipietro/Mistral-7B-Instruct-v0.2_adapters_en.layer1__v0.2_wandblog\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_model_tokenizer' from 'utils.load_merged_model_tokenizer' (/home/pferrazzi/mistral_finetuning/utils/load_merged_model_tokenizer.py)"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.evaluator import Evaluator\n",
    "from config.finetuning import config\n",
    "from utils.load_merged_model_tokenizer import load_mergedModel_tokenizer\n",
    "from config import postprocessing\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "adapters_checkpoints_list = postprocessing.adapters_checkpoints_list\n",
    "base_models_list = postprocessing.base_models_list\n",
    "splits_list = postprocessing.splits_list\n",
    "max_new_tokens_factor_list = postprocessing.max_new_tokens_factor_list\n",
    "n_shots_inference_list = postprocessing.n_shots_inference_list\n",
    "language = postprocessing.splits_list[0].split('.')[0]\n",
    "\n",
    "\n",
    "merged_model, tokenizer = load_mergedModel_tokenizer(adapters_checkpoints_list[0], base_models_list[0])\n",
    "\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[splits_list[0]]\n",
    "preprocessor = DataPreprocessor()\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset)\n",
    "_, val_data, _ = preprocessor.split_layer_into_train_val_test_(dataset, splits_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_new_tokens_factor in max_new_tokens_factor_list:\n",
    "    for n_shots_inference in n_shots_inference_list:\n",
    "        postprocessor = TestDataProcessor(test_data=val_data, preprocessor=preprocessor, \n",
    "                                          n_shots_inference=n_shots_inference, \n",
    "                                          language=language, \n",
    "                                          tokenizer=tokenizer)\n",
    "        postprocessor.add_inference_prompt_column()\n",
    "        postprocessor.add_ground_truth_column()\n",
    "        postprocessor.add_responses_column(model=merged_model, \n",
    "                                           tokenizer=tokenizer, \n",
    "                                           batch_size=24, \n",
    "                                           max_new_tokens_factor=max_new_tokens_factor)\n",
    "        postprocessor.test_data.to_csv(f\"data/test_data_processed/{language}_nShots{n_shots_inference}_maxNewTokensFactor{max_new_tokens_factor}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONE RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating responses:   4%|â–Ž         | 24/681 [00:46<21:16,  1.94s/it]"
     ]
    }
   ],
   "source": [
    "from_backup_file = False\n",
    "if from_backup_file:\n",
    "    tmp_data = pd.read_csv(f\"data/test_data_processed/en_nShots{n_shots_inference}_maxNewTokensFactor{max_new_tokens_factor}.csv\")\n",
    "    tmp_data = Dataset.from_pandas(tmp_data)\n",
    "    postprocessor = TestDataProcessor(test_data=tmp_data, preprocessor=preprocessor, n_shots_inference=n_shots_inference, language=splits[0].split('.')[0], tokenizer=tokenizer)\n",
    "\n",
    "if not from_backup_file:\n",
    "    postprocessor = TestDataProcessor(test_data=val_data, preprocessor=preprocessor, n_shots_inference=n_shots_inference, language=splits[0].split('.')[0], tokenizer=tokenizer)\n",
    "    postprocessor.add_inference_prompt_column()\n",
    "    postprocessor.add_ground_truth_column()\n",
    "    postprocessor.add_responses_column(model=merged_model, tokenizer=tokenizer, batch_size=24, max_new_tokens_factor=max_new_tokens_factor)\n",
    "    postprocessor.test_data.to_csv(f\"data/test_data_processed/{splits[0].split('.')[0]}_nShots{n_shots_inference}_maxNewTokensFactor{max_new_tokens_factor}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
