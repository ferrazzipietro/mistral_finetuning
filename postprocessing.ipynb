{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TestDataProcessor():\n",
    "    def __init__(self, test_data: Dataset, preprocessor:DataPreprocessor, n_shots_inference:int, language:str, tokenizer) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the TestDataProcessor class.\n",
    "        pass to this the same DataPreprocessor used for the training data. This will ensure that the inference prompt is formatted in the same way as the training prompt.\n",
    "        \"\"\"\n",
    "        self.test_data = test_data\n",
    "        self.preprocessor = preprocessor\n",
    "        self.language = language\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_type = preprocessor.model_type\n",
    "        self.input_sentence_field = 'sentence'\n",
    "        self.few_shots_dict = {'en':{'questions':['We present a case of a 32-year-old woman with a history of gradual enlargement of the anterior neck.',\n",
    "                                                   'Patient information: a 9-month-old boy presented to the emergency room with a 3-day history of refusal to bear weight on the right lower extremity and febrile peaks of up to 38.5°C for 24 hours.',\n",
    "                                                   'There was no evidence of lung lesions.',\n",
    "                                                   'Locally diminished actin coloration indicated atrophy of smooth muscle fibers.'],\n",
    "                                        'responses':['[{\"entity\": \"present\"}, {\"entity\": \"history\"}, {\"entity\": \"enlargement\"}]',\n",
    "                                                     '[{\"entity\": \"presented\"}, {\"entity\": \"refusal\"}, {\"entity\": \"bear\"}, {\"entity\": \"peaks\"}]',\n",
    "                                                      '[{\"entity\": \"evidence\"}, {\"entity\": \"lung lesions\"]',\n",
    "                                                      '[{\"entity\": \"coloration\"}, {\"entity\": \"indicated\"}, {\"entity\": \"atrophy\"}, {\"entity\": \"atrophy of smooth muscle fibers\"}, {\"entity\": \"smooth muscle fibers\"'],\n",
    "                                        'responses_offset': ['[{\"entity\": \"present\", \"offset\": [3, 10]}, {\"entity\": \"history\", \"offset\": [48, 55]}, {\"entity\": \"enlargement\", \"offset\": [67, 78]}]',\n",
    "                                                             '[{\"entity\": \"presented\", \"offset\": [39, 48]}, {\"entity\": \"refusal\", \"offset\": [95, 102]}, {\"entity\": \"bear\", \"offset\": [106, 110]}, {\"entity\": \"peaks\", \"offset\": [159, 164]}]',\n",
    "                                                             '[{ \"entity\": \"evidence\", \"offsets\": [13, 21]}, {\"entity\": \"lung lesions\", \"offsets\": [25, 37]} ]',\n",
    "                                                             '[{\"entity\": \"coloration\", \"offsets\": [25, 35]}, {\"entity\": \"indicated\", \"offsets\": [36, 45]}, {\"entity\": \"atrophy\",\"offsets\": [46, 53]}, {\"entity\": \"atrophy of smooth muscle fibers\", \"offsets\": [46, 77]}, {\"entity\": \"smooth muscle fibers\", \"offsets\": [57, 77]} ]'],\n",
    "                                    },\n",
    "                                'it':{'questions':['In considerazione dell’inefficacia della terapia somministrata, in assenza di ulteriori opzioni terapeutiche standard potenzialmente efficaci e dopo colloquio con i genitori si decide di avviare la paziente a trapianto aploidentico, possibilmente NK allo reattivo, da genitore.',\n",
    "                                                    'L’esame istologico dimostrava mucosa gastrica atrofica con flogosi cronica, marcato edema ed incremento del connettivo del corion, focale metaplasia intestinale, il tutto sovrastante un tessuto fibromuscolare.',\n",
    "                                                    'Giunge nel nostro reparto per stranguria in assenza di altri sintomi.',\n",
    "                                                    'All’età di 16 mesi, nuovo ricovero per febbre (39°C) e stato di abbattimento.'],\n",
    "                                       'responses':['[{\"entity\": \"inefficacia\"}, {\"entity\": \"opzioni\"}, {\"entity\": \"colloquio\"}, {\"entity\": \"avviare\"}, {\"entity\": \"trapianto\"}, {\"entity\": \"genitori\"}, {\"entity\": \"paziente\"}, {\"entity\": \"genitore\"}]',\n",
    "                                                    '[{\"entity\": \"mucosa gastrica atrofica\"}, {\"entity\": \"flogosi\\r\\cronica\"}]',\n",
    "                                                    '[{\"entity\": \"Giunge\"}, {\"entity\": \"stranguria\"}, {\"entity\": \"sintomi\"}, {\"entity\": \"stranguria\"}]',\n",
    "                                                    '[{\"entity\": \"ricovero\"}, {\"entity\": \"febbre\"}, {\"entity\": \"stato\"}, {\"entity\": \"febbre\"}, {\"entity\": \"39°C\"} ]'],\n",
    "                                       'responses_offset':['[{\"entity\": \"inefficacia\", \"offset\": [23, 34]}, {\"entity\": \"opzioni\", \"offset\": [88,95]}, {\"entity\": \"colloquio\", \"offset\": [149,158]}, {\"entity\": \"avviare\", \"offset\": [187,194]}, {\"entity\": \"trapianto\", \"offset\": [209,218]}, {\"entity\": \"genitori\", \"offset\": [163,173]}, {\"entity\": \"paziente\", \"offset\": [195,106]}, {\"entity\": \"genitore\", \"offset\": [268,276]}]',\n",
    "                                                           '[{\"entity\": \"mucosa gastrica atrofica\", \"offset\": [30,54]}, {\"entity\": \"flogosi\\r\\cronica\", \"offset\": [59,75]}]',\n",
    "                                                           '[{\"entity\": \"Giunge\", \"offset\": [0,6]}, {\"entity\": \"stranguria\", \"offset\": [30,40]}, { \"entity\": \"sintomi\", \"offset\": [61,68]}, {\"entity\": \"stranguria\", \"offset\": [ 30, 40 ]} ]',\n",
    "                                                           '[{\"entity\": \"ricovero\", \"offset\": [26,34]}, {\"entity\": \"febbre\", \"offset\": [ 39, 45 ]}, {\"entity\": \"stato\", \"offset\": [ 55, 60 ]}, {\"entity\": \"febbre\", \"offset\": [ 39, 45 ]}, {\"entity\": \"39°C\", \"offset\": [47,51]} ]']},\n",
    "                                'slo': {'questions':[],\n",
    "                                       'responses':[],\n",
    "                                       'responses_offset':[]}}\n",
    "        if len(self.few_shots_dict[self.language]['questions']) < n_shots_inference:\n",
    "            raise ValueError(f'The number of shots for the inference prompt is greater than the number of examples available.')\n",
    "        if len(self.few_shots_dict[self.language]['responses']) < n_shots_inference:\n",
    "            raise ValueError(f'The number of shots for the inference prompt is greater than the number of responses available.')\n",
    "        self.n_shots_inference = n_shots_inference\n",
    "    \n",
    "    def _extract_ground_truth(self, prompt:str) -> str:\n",
    "        # print('PROMPT: ', prompt)\n",
    "        end_of_prompt_string = self.preprocessor.special_tokens_instruction['user_end'] + self.preprocessor.special_tokens_instruction['model_start']\n",
    "        # print('end_of_prompt_string: ', end_of_prompt_string)\n",
    "        out = prompt.split(end_of_prompt_string, 1)\n",
    "        out = out[1].strip().replace(self.preprocessor.special_tokens_instruction['model_start'], '').replace(self.preprocessor.special_tokens_instruction['model_end'], '')\n",
    "        # print('OUT: ', out)\n",
    "        return {'ground_truth': out}\n",
    "    \n",
    "    def _format_prompt_inference(self, input: str, instruction_on_response_format:str, n_shots:int, offset: bool, simplest_prompt:bool, output:str='', list_of_examples: [str]=[], list_of_responses:[str]=[]) -> str:\n",
    "        \"\"\"\n",
    "        Format the input and output into a prompt for the finetuning\n",
    "\n",
    "        Args:\n",
    "            task: the task for which the prompt is generated, either 'finetuning' or 'inference'\n",
    "            input: the input text\n",
    "            instruction_on_response_format: the instruction on the response format. E.g. \"The response must be a list of dictionaries, where each dictionary contains the keys 'text' and 'offset'\"\n",
    "            n_shots: the number of examples to provide as few shot prompting\n",
    "            offset: whether to require the offset in the response\n",
    "            tokenizer: the tokenizer to use\n",
    "            output: the output text\n",
    "            list_of_examples: the list of examples to provide as few shot prompting\n",
    "            list_of_responses: the list of responses to provide as few shot prompting\n",
    "\n",
    "        Returns:\n",
    "            the formatted prompt\n",
    "        \"\"\"\n",
    "        if output != '':\n",
    "            raise ValueError(\"The output must be an empty string when generating prompts for the inference\")\n",
    "\n",
    "        if len(list_of_examples) != len(list_of_responses):\n",
    "            raise ValueError(\"The number of examples and responses must be the same\")\n",
    "        if n_shots != len(list_of_examples):\n",
    "            raise ValueError(\"The number of examples and shots must be the same\")\n",
    "        if n_shots != len(list_of_responses):\n",
    "            raise ValueError(\"The number of responses and shots must be the same\")\n",
    "        \n",
    "        if simplest_prompt:\n",
    "            base_prompt = self.preprocessor._simplest_base_prompt_input(input)\n",
    "        elif not simplest_prompt:\n",
    "            base_prompt = self.preprocessor._base_prompt_input(input, instruction_on_response_format)\n",
    "\n",
    "        one_shot_example = self.preprocessor.one_shot_example_no_offset if not offset else self.preprocessor.one_shot_example\n",
    "            \n",
    "        prompt = ''\n",
    "        for shot_example in range(n_shots):\n",
    "            prompt += one_shot_example.format(\n",
    "                instruction_on_response_format=instruction_on_response_format, \n",
    "                example_query=list_of_examples[shot_example], \n",
    "                example_response=list_of_responses[shot_example],\n",
    "                user_start=self.preprocessor.special_tokens_instruction['user_start'],\n",
    "                user_end=self.preprocessor.special_tokens_instruction['user_end'],\n",
    "                model_start=self.preprocessor.special_tokens_instruction['model_start'],\n",
    "                model_end=self.preprocessor.special_tokens_instruction['model_end'])\n",
    "        \n",
    "        bos_token = self.preprocessor.tokenizer.bos_token\n",
    "        if self.model_type == 'qwen':\n",
    "            bos_token = ''\n",
    "        prompt = bos_token + prompt + base_prompt + output \n",
    "                            \n",
    "        return prompt\n",
    "    \n",
    "    def _extract_inference_prompt(self, sentence:str, simplest_prompt:bool) -> str:\n",
    "        if self.preprocessor.offset:\n",
    "            few_shots_responses = self.few_shots_dict[self.language]['responses_offset']\n",
    "        else:\n",
    "            few_shots_responses = self.few_shots_dict[self.language]['responses']\n",
    "        if self.n_shots_inference == 0:\n",
    "            list_of_examples = []\n",
    "            list_of_responses = []\n",
    "        else:\n",
    "            list_of_examples = self.few_shots_dict[self.language]['questions'][0:self.n_shots_inference]\n",
    "            list_of_responses = few_shots_responses[0:self.n_shots_inference]\n",
    "        inference_prompt = self._format_prompt_inference(input=sentence, \n",
    "                                                        instruction_on_response_format=self.preprocessor.instruction_on_response_format,\n",
    "                                                        offset=self.preprocessor.offset,\n",
    "                                                        output='',\n",
    "                                                        n_shots=self.n_shots_inference,\n",
    "                                                        simplest_prompt=simplest_prompt,\n",
    "                                                        list_of_examples=list_of_examples,\n",
    "                                                        list_of_responses=list_of_responses)\n",
    "        return {'inference_prompt': inference_prompt}\n",
    "    \n",
    "    def add_inference_prompt_column(self, simplest_prompt:bool) -> None:\n",
    "        \"\"\"\n",
    "        Add the inferencePrompt and groundTruth columns to the test_data dataframe.\n",
    "        \"\"\"\n",
    "        self.test_data = self.test_data.map(lambda x: self._extract_inference_prompt(x[self.input_sentence_field], simplest_prompt=simplest_prompt))\n",
    "    \n",
    "    def add_ground_truth_column(self) -> None:\n",
    "        \"\"\"\n",
    "        Add the groundTruth column to the test_data dataframe.\n",
    "        \"\"\"\n",
    "        self.test_data = self.test_data.map(lambda x: self._extract_ground_truth(x['prompt']))\n",
    "\n",
    "    def _generate_model_response(self, examples, model, tokenizer, max_new_tokens_factor:float, stopping_criteria=[], temperature:float=1.0) -> str:\n",
    "        device = \"cuda\"\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        # if self.model_type == 'qwen':\n",
    "        #     tokenizer.pad_token = '<unk>' # tokenizer.special_tokens['<extra_0>']\n",
    "        input_sentences = examples[self.input_sentence_field]\n",
    "        prompts = examples['inference_prompt']\n",
    "        input_sentences_tokenized = tokenizer(input_sentences, return_tensors=\"pt\", padding=True)\n",
    "        max_new_tokens = int(len(max(input_sentences_tokenized, key=len)) * max_new_tokens_factor)\n",
    "        # if self.preprocessor.model_type == 'gemma':\n",
    "        #     add_special_tokens = True\n",
    "        encodeds = tokenizer(prompts, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "        model_inputs = encodeds.to(device)\n",
    "        if len(stopping_criteria)>0:\n",
    "            generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=max_new_tokens,  \n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        temperature = temperature,\n",
    "                                        stopping_criteria = stopping_criteria\n",
    "                                        ) # max_new_tokens=max_new_tokens,\n",
    "        else:\n",
    "            generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=max_new_tokens,  \n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        temperature = temperature) \n",
    "        #print('generated_ids: ', generated_ids)\n",
    "        generated_ids = generated_ids[:, encodeds.input_ids.shape[1]:]\n",
    "        decoded = tokenizer.batch_decode(generated_ids)\n",
    "        # decoded = [self._postprocess_model_output(i) for i in decoded]\n",
    "        return (decoded)\n",
    "                \n",
    "    def add_responses_column(self, model, tokenizer, batch_size:int, max_new_tokens_factor:float, stopping_criteria:list, temperature:float=1.0) -> None:\n",
    "        \"\"\"\n",
    "        Adds a column with the response of the model to the actual query.\n",
    "        \n",
    "        params:\n",
    "        model: the model to use to generate the response\n",
    "        tokenizer: the tokenizer to use to generate the response\n",
    "        batch_size: the batch size to use to process the examples. Increasing this makes it faster but requires more GPU. Default is 8.\n",
    "        max_new_tokens_factor: the factor conotrolling the number of new tokens to generate. This is a factor of the length of the input sentence.\n",
    "        \"\"\"\n",
    "        responses_col = []\n",
    "        total_rows = len(self.test_data)\n",
    "        indexes = [i for i in range(len(self.test_data)) if i % batch_size == 0]\n",
    "        max_index = self.test_data.shape[0]\n",
    "\n",
    "\n",
    "        with tqdm(total=total_rows, desc=\"generating responses\") as pbar:\n",
    "            for i, idx in enumerate(indexes[:-1]):\n",
    "                indici = list(range(idx, indexes[i+1]))\n",
    "                tmp = self._generate_model_response(self.test_data.select(indici), model, tokenizer, max_new_tokens_factor, stopping_criteria, temperature=temperature)\n",
    "                responses_col.extend(tmp)\n",
    "                pbar.update(batch_size)\n",
    "            indici = list(range(indexes[len(indexes[:-1])], max_index))\n",
    "            tmp = self._generate_model_response(self.test_data.select(indici), model, tokenizer, max_new_tokens_factor, stopping_criteria, temperature=temperature)\n",
    "            responses_col.extend(tmp)\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "        self.test_data = self.test_data.add_column('model_responses', responses_col)\n",
    "    \n",
    "    def _postprocess_model_output_deprecated(self, model_output: str) -> str:\n",
    "        \"\"\"\n",
    "        Postprocess the model output to remove the instruction and return the model response.\n",
    "\n",
    "        Args:\n",
    "        model_output (str): the model output as it is returned by the model. The processing of the output is done in the function\n",
    "\n",
    "        return:\n",
    "        str: the model response, i.e. the model output without the instruction\n",
    "\n",
    "        \"\"\"\n",
    "        end_of_prompt_string = self.preprocessor.special_tokens_instruction['user_end'] + self.preprocessor.special_tokens_instruction['model_start']\n",
    "        return model_output.split(end_of_prompt_string, 1)[-1].strip()\n",
    "    \n",
    "    \n",
    "\n",
    "class TestDataProcessSlovenian(TestDataProcessor):\n",
    "    def __init__(self, test_data: Dataset, preprocessor:DataPreprocessor, n_shots_inference:int, language:str, tokenizer) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the TestDataProcessor class.\n",
    "        pass to this the same DataPreprocessor used for the training data. This will ensure that the inference prompt is formatted in the same way as the training prompt.\n",
    "        \"\"\"\n",
    "        super().__init__(test_data, preprocessor, n_shots_inference, language, tokenizer)\n",
    "        self.input_sentence_field = 'sentence'\n",
    "        \n",
    "\n",
    "    def _extract_ground_truth(self, prompt:str) -> str:\n",
    "        # print('PROMPT: ', prompt)\n",
    "        end_of_prompt_string = self.preprocessor.special_tokens_instruction['user_end'] + self.preprocessor.special_tokens_instruction['model_start']\n",
    "        # print('end_of_prompt_string: ', end_of_prompt_string)\n",
    "        out = prompt.split(end_of_prompt_string, 1)\n",
    "        out = out[1].strip().replace(self.preprocessor.special_tokens_instruction['model_start'], '').replace(self.preprocessor.special_tokens_instruction['model_end'], '')\n",
    "\n",
    "        if out=='] </s>':\n",
    "            out='[]'\n",
    "        # print('OUT: ', out)\n",
    "        return {'ground_truth': out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_16_16_0.02_1_0.0002_clent', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_16_32_0.02_1_0.0002_clent', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_32_16_0.02_1_0.0002_clent', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_32_32_0.02_1_0.0002_clent', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_64_16_0.02_1_0.0002_clent', 'ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_64_32_0.02_1_0.0002_clent']\n",
      "MODEL TYPE: llama\n",
      "val_data: {'sentence': ['A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia.', 'Hormonal study and dynamic biochemical tests performed indicated ECS.', 'Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.'], 'entities': [[{'id': '1996', 'offsets': [91, 118], 'role': '', 'semantic_type_id': 'C0743128', 'text': 'new-onset diabetes mellitus', 'type': 'CLINENTITY'}], [], [{'id': '2010', 'offsets': [57, 89], 'role': '', 'semantic_type_id': 'C1306459', 'text': 'primary right parotid malignancy', 'type': 'CLINENTITY'}, {'id': '2017', 'offsets': [95, 111], 'role': '', 'semantic_type_id': 'C0494165', 'text': 'liver metastases', 'type': 'CLINENTITY'}]], 'original_text': ['A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia. Hormonal study and dynamic biochemical tests performed indicated ECS. Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases. Somatostatin receptor scintigraphy has shown an increased uptake in the parotid gland and mild expression in liver metastasis. The patient underwent right parotidectomy, and histopathologic examination confirmed ACC. Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide. Despite chemotherapy onset, a rapid disease progression and clinical course deterioration was observed.\\r\\n', 'A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia. Hormonal study and dynamic biochemical tests performed indicated ECS. Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases. Somatostatin receptor scintigraphy has shown an increased uptake in the parotid gland and mild expression in liver metastasis. The patient underwent right parotidectomy, and histopathologic examination confirmed ACC. Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide. Despite chemotherapy onset, a rapid disease progression and clinical course deterioration was observed.\\r\\n', 'A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia. Hormonal study and dynamic biochemical tests performed indicated ECS. Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases. Somatostatin receptor scintigraphy has shown an increased uptake in the parotid gland and mild expression in liver metastasis. The patient underwent right parotidectomy, and histopathologic examination confirmed ACC. Meanwhile, hypercortisolism was managed with metyrapone, ketoconazole, and lanreotide. Despite chemotherapy onset, a rapid disease progression and clinical course deterioration was observed.\\r\\n'], 'original_id': ['EN101783', 'EN101783', 'EN101783'], 'prompt': ['<s>[INST] Extract the CLINICAL ENTITIES contained in the text. Do not extract any entity which is not clinical.\\nReturn the result in a json format: [{\"entity\":\"clinical_entity_name\"}]. <<A 46-year-old man with hypertension and dyslipidemia diagnosed 4-months before, as well as new-onset diabetes mellitus unveiled 1-month earlier, was referred to emergency department for hypokalemia.>>> [/INST][{\"entity\": \"new-onset diabetes mellitus\"}] </s>', '<s>[INST] Extract the CLINICAL ENTITIES contained in the text. Do not extract any entity which is not clinical.\\nReturn the result in a json format: [{\"entity\":\"clinical_entity_name\"}]. <<Hormonal study and dynamic biochemical tests performed indicated ECS.>>> [/INST][{{\"entity\": \"\"}}]</s>', '<s>[INST] Extract the CLINICAL ENTITIES contained in the text. Do not extract any entity which is not clinical.\\nReturn the result in a json format: [{\"entity\":\"clinical_entity_name\"}]. <<Imaging and cytological findings pointed toward a likely primary right parotid malignancy with liver metastases.>>> [/INST][{\"entity\": \"primary right parotid malignancy\"}, {\"entity\": \"liver metastases\"}] </s>']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapters_list:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING: ferrazzipietro/llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_16_16_0.02_1_0.0002_clent n_shots_inference: 0 max_new_tokens_factor: 6\n",
      "QUANTIZATION\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b65195591743eba6558115a073c0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_ids: [[29914, 25580, 29962, 29871], [29914, 25580, 29962, 29871], [29914, 25580, 29962, 29871], [29914, 25580, 29962, 29871]]\n",
      "last_ids: [[25580, 29962, 29871, 518], [25580, 29962, 29871, 518], [25580, 29962, 29871, 518], [25580, 29962, 29871, 518]]\n",
      "last_ids: [[29962, 29871, 518, 6377], [29962, 29871, 518, 6377], [29962, 29871, 518, 6377], [29962, 29871, 518, 6377]]\n",
      "last_ids: [[29871, 518, 6377, 10041], [29871, 518, 6377, 10041], [29871, 518, 6377, 10041], [29871, 518, 6377, 10041]]\n",
      "last_ids: [[518, 6377, 10041, 1115], [518, 6377, 10041, 1115], [518, 6377, 10041, 1115], [518, 6377, 10041, 1115]]\n",
      "last_ids: [[6377, 10041, 1115, 5124], [6377, 10041, 1115, 5124], [6377, 10041, 1115, 376], [6377, 10041, 1115, 5124]]\n",
      "last_ids: [[10041, 1115, 5124, 6525], [10041, 1115, 5124, 6525], [10041, 1115, 376, 16072], [10041, 1115, 5124, 6525]]\n",
      "last_ids: [[1115, 5124, 6525, 2], [1115, 5124, 6525, 2], [1115, 376, 16072, 1492], [1115, 5124, 6525, 2]]\n",
      "last_ids: [[5124, 6525, 2, 2], [5124, 6525, 2, 2], [376, 16072, 1492, 610], [5124, 6525, 2, 2]]\n",
      "last_ids: [[6525, 2, 2, 2], [6525, 2, 2, 2], [16072, 1492, 610, 327], [6525, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [1492, 610, 327, 333], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [610, 327, 333, 286], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [327, 333, 286, 2520], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [333, 286, 2520, 6906], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [286, 2520, 6906, 411], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [2520, 6906, 411, 619], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [6906, 411, 619, 369], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [411, 619, 369, 1539], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [619, 369, 1539, 579], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [369, 1539, 579, 2129], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [1539, 579, 2129, 29908], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [579, 2129, 29908, 6525], [2, 2, 2, 2]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [2129, 29908, 6525, 29871], [2, 2, 2, 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [29908, 6525, 29871, 2], [2, 2, 2, 2]]\n",
      "last_ids: [[29914, 25580, 29962, 29871], [29914, 25580, 29962, 29871], [29914, 25580, 29962, 29871], [29914, 25580, 29962, 29871]]\n",
      "last_ids: [[25580, 29962, 29871, 518], [25580, 29962, 29871, 518], [25580, 29962, 29871, 518], [25580, 29962, 29871, 518]]\n",
      "last_ids: [[29962, 29871, 518, 6377], [29962, 29871, 518, 6377], [29962, 29871, 518, 6377], [29962, 29871, 518, 6377]]\n",
      "last_ids: [[29871, 518, 6377, 10041], [29871, 518, 6377, 10041], [29871, 518, 6377, 10041], [29871, 518, 6377, 10041]]\n",
      "last_ids: [[518, 6377, 10041, 1115], [518, 6377, 10041, 1115], [518, 6377, 10041, 1115], [518, 6377, 10041, 1115]]\n",
      "last_ids: [[6377, 10041, 1115, 5124], [6377, 10041, 1115, 5124], [6377, 10041, 1115, 376], [6377, 10041, 1115, 376]]\n",
      "last_ids: [[10041, 1115, 5124, 6525], [10041, 1115, 5124, 6525], [10041, 1115, 376, 29924], [10041, 1115, 376, 25379]]\n",
      "last_ids: [[1115, 5124, 6525, 2], [1115, 5124, 6525, 2], [1115, 376, 29924, 9312], [1115, 376, 25379, 29908]]\n",
      "last_ids: [[5124, 6525, 2, 2], [5124, 6525, 2, 2], [376, 29924, 9312, 29908], [376, 25379, 29908, 6525]]\n",
      "last_ids: [[6525, 2, 2, 2], [6525, 2, 2, 2], [29924, 9312, 29908, 6525], [25379, 29908, 6525, 29871]]\n",
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [9312, 29908, 6525, 29871], [29908, 6525, 29871, 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating responses: 100%|██████████| 8/8 [00:39<00:00,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_ids: [[2, 2, 2, 2], [2, 2, 2, 2], [29908, 6525, 29871, 2], [6525, 29871, 2, 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "adapters_list: 100%|██████████| 1/1 [00:45<00:00, 45.08s/it]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "from utils.generate_ft_adapters_list import generate_ft_adapters_list\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import postprocessing_params_llama as postprocessing\n",
    "from log import llama_7B_NoQuant_1epoch as models_params\n",
    "adapters_list = generate_ft_adapters_list(\"llama_7B_NoQuant_1epoch\", simplest_prompt=models_params.simplest_prompt)\n",
    "print(adapters_list)\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "\n",
    "max_new_tokens_factor_list = postprocessing.max_new_tokens_factor_list\n",
    "n_shots_inference_list = postprocessing.n_shots_inference_list\n",
    "layer = models_params.TRAIN_LAYER\n",
    "language = layer.split('.')[0]\n",
    "\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n",
    "tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, add_eos_token=False,\n",
    "                                         token=LLAMA_TOKEN)\n",
    "preprocessor = DataPreprocessor(model_checkpoint=models_params.BASE_MODEL_CHECKPOINT, \n",
    "                                tokenizer = tokenizer, clen=models_params.clent)\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset,\n",
    "                                                 models_params.instruction_on_response_format)\n",
    "_, val_data, _ = preprocessor.split_layer_into_train_val_test_(dataset, layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import StoppingCriteria\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, eos_sequence = [518, 29914, 25580, 29962]):\n",
    "        self.eos_sequence = eos_sequence\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "        print('last_ids:', last_ids)\n",
    "        eos_cond = 2 in last_ids\n",
    "        return (self.eos_sequence in last_ids) or eos_cond\n",
    "\n",
    "\n",
    "\n",
    "print(\"val_data:\", val_data[0:3])\n",
    "\n",
    "for max_new_tokens_factor in [6]:#max_new_tokens_factor_list\n",
    "    for n_shots_inference in [0]:#n_shots_inference_list:\n",
    "        for adapters in tqdm(adapters_list[:1], desc=\"adapters_list\"):\n",
    "            if adapters.endswith(\"0.0008\"):\n",
    "                continue\n",
    "            print(\"PROCESSING:\", adapters, \"n_shots_inference:\", n_shots_inference, \"max_new_tokens_factor:\", max_new_tokens_factor)\n",
    "            if False:#not models_params.quantization:\n",
    "                print(\"NO QUANTIZATION\")\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    models_params.BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "                    return_dict=True,  \n",
    "                    torch_dtype=postprocessing.torch_dtype,\n",
    "                    device_map= \"auto\",\n",
    "                    token=LLAMA_TOKEN)    \n",
    "            else:\n",
    "                print(\"QUANTIZATION\")\n",
    "                # load_in_8bit = not models_params.load_in_4bit[0]\n",
    "                # load_in_4bit = models_params.load_in_4bit[0]\n",
    "                # load_in_8bit = not load_in_4bit\n",
    "                bnb_4bit_use_double_quant = models_params.bnb_4bit_use_double_quant\n",
    "                bnb_4bit_quant_type = models_params.bnb_4bit_quant_type[0]\n",
    "                bnb_4bit_compute_dtype = models_params.bnb_4bit_compute_dtype[0]\n",
    "                # llm_int8_threshold = models_params.llm_int8_threshold[0]\n",
    "                # llm_int8_has_fp16_weight = models_params.llm_int8_has_fp16_weight AVOID IT AT INFERENCE TIME!\n",
    "                # llm_int8_skip_modules = models_params.llm_int8_skip_modules AVOID IT AT INFERENCE TIME!\n",
    "\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                            load_in_4bit=True,\n",
    "                            # load_in_8bit=load_in_8bit,\n",
    "                            bnb_4bit_use_double_quant=bnb_4bit_use_double_quant,\n",
    "                            bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "                            bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "                            # llm_int8_threshold=llm_int8_threshold ,\n",
    "                            # llm_int8_has_fp16_weight =True #,AVOID IT AT INFERENCE TIME!\n",
    "                            # llm_int8_skip_modules=llm_int8_skip_modules AVOID IT AT INFERENCE TIME!\n",
    "                            )\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    models_params.BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "                    quantization_config = bnb_config,\n",
    "                    return_dict=True,  \n",
    "                    device_map= \"auto\",\n",
    "                    cache_dir ='/data/disk1/share/pferrazzi/.cache',\n",
    "                    token=LLAMA_TOKEN)\n",
    "            merged_model = PeftModel.from_pretrained(base_model, \n",
    "                                                     adapters, \n",
    "                                                     token=HF_TOKEN, \n",
    "                                                     device_map='auto',\n",
    "                                                     is_trainable = False)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, \n",
    "                                                      add_eos_token=False,\n",
    "                                                      token=LLAMA_TOKEN)\n",
    "            tokenizer.pad_token = tokenizer.eos_token# \"<pad>\" #tokenizer.eos_token\n",
    "            tokenizer.padding_side = \"left\"\n",
    "#            tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, add_eos_token=True, token=LLAMA_TOKEN)\n",
    "#            tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "#            merged_model.resize_token_embeddings(len(tokenizer))\n",
    "#            print('tokenizer.pad_token_id:', tokenizer.pad_token_id)\n",
    "#            merged_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "            postprocessor = TestDataProcessor(test_data=val_data.select(range(8)), \n",
    "                                              preprocessor=preprocessor, \n",
    "                                              n_shots_inference=n_shots_inference, \n",
    "                                              language=language, \n",
    "                                              tokenizer=tokenizer)\n",
    "            postprocessor.add_inference_prompt_column(simplest_prompt=False)\n",
    "\n",
    "            # tmp = []\n",
    "            # for example in postprocessor.test_data:\n",
    "            #     tmp.append(example)\n",
    "            # import pandas as pd\n",
    "            # tmp = pd.DataFrame(tmp)\n",
    "            # tmp = tmp.iloc[tmp['inference_prompt'].str.len().argsort()]\n",
    "            # postprocessor.test_data = Dataset.from_pandas(tmp)\n",
    "\n",
    "            postprocessor.add_ground_truth_column()\n",
    "            #try:\n",
    "            postprocessor.add_responses_column(model=merged_model, \n",
    "                                            tokenizer=tokenizer, \n",
    "                                            batch_size=4, \n",
    "                                            max_new_tokens_factor=max_new_tokens_factor,\n",
    "                                            stopping_criteria = [EosListStoppingCriteria()],\n",
    "                                            temperature=1)\n",
    "            # postprocessor.test_data.to_csv(f\"{postprocessing.save_directory}maxNewTokensFactor{max_new_tokens_factor}_nShotsInference{n_shots_inference}_{adapters.split('/')[1]}.csv\", index=False)\n",
    "            # except RuntimeError as e:\n",
    "                # print(\"ERROR IN PROCESSING: \", e, adapters)\n",
    "                # print(e.message)\n",
    "            del merged_model\n",
    "            if models_params.quantization: del base_model\n",
    "            del tokenizer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entity\": \"\"}]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(models_params.BASE_MODEL_CHECKPOINT, \n",
    "                                                      add_eos_token=False,\n",
    "                                                      token=LLAMA_TOKEN)\n",
    "tokenizer.decode([10041, 1115, 5124, 6525])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[{\"entity\": \"new-onset diabetes mellitus\"}] </s>',\n",
       "  '[{{\"entity\": \"\"}}]</s>',\n",
       "  '[{\"entity\": \"primary right parotid malignancy\"}, {\"entity\": \"liver metastases\"}] </s>',\n",
       "  '[{\"entity\": \"ACC\"}] </s>',\n",
       "  '[{\"entity\": \"hypercortisolism\"}] </s>',\n",
       "  '[{\"entity\": \"cervical mass\"}] </s>',\n",
       "  '[{\"entity\": \"MNG\"}] </s>',\n",
       "  '[{\"entity\": \"mass\"}, {\"entity\": \"cervical compression\"}, {\"entity\": \"respiratory, digestive, laryngeal, vascular or neurologic signs\"}, {\"entity\": \"digestive, laryngeal, vascular or neurologic signs\"}, {\"entity\": \"laryngeal, vascular or neurologic signs\"}, {\"entity\": \"vascular or neurologic signs\"}, {\"entity\": \"neurologic signs\"}] </s>'],\n",
       " [' [{\"entity\": \"\"}]</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>',\n",
       "  ' [{\"entity\": \"\"}]</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>',\n",
       "  ' [{\"entity\": \"primary right parotid malignancy with liver metastases\"}] </s>',\n",
       "  ' [{\"entity\": \"\"}]</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>',\n",
       "  ' [{\"entity\": \"\"}]</s></s></s></s></s>',\n",
       "  ' [{\"entity\": \"\"}]</s></s></s></s></s>',\n",
       "  ' [{\"entity\": \"MNG\"}] </s>',\n",
       "  ' [{\"entity\": \"mass\"}] </s></s>'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data['ground_truth'], postprocessor.test_data['model_responses'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[{\"entity\": \"new-onset diabetes mellitus\"}] </s>',\n",
       "  '[{{\"entity\": \"\"}}]</s>',\n",
       "  '[{\"entity\": \"primary right parotid malignancy\"}, {\"entity\": \"liver metastases\"}] </s>',\n",
       "  '[{\"entity\": \"ACC\"}] </s>'],\n",
       " [' [{\"entity\": \"hypertension\"}, {\"entity',\n",
       "  ' [{\"entity\": \"ECS\"}] </s>',\n",
       "  ' [{\"entity\": \"primary right parotid m',\n",
       "  ' [{\"entity\": \"ACC\"}] </s>'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data['ground_truth'], postprocessor.test_data['model_responses'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[{\"entity\": \"new-onset diabetes mellitus\"}] </s>',\n",
       "  '[{{\"entity\": \"\"}}]</s>',\n",
       "  '[{\"entity\": \"primary right parotid malignancy\"}, {\"entity\": \"liver metastases\"}] </s>',\n",
       "  '[{\"entity\": \"ACC\"}] </s>'],\n",
       " [' [{\"entity\": \"hypertension\"}, {\"entity\": \"dyslipidemia\"}, {\"entity\": \"diabetes mellitus\"}]  [{\"entity\": \"hypokalemia\"}]  [{\"entity\": \"diabetes mellitus\"}]  [{\"entity\": \"hypokalemia\"}]  [{\"entity\": \"diabetes',\n",
       "  ' [{\"entity\": \"ECS\"}] </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>',\n",
       "  ' [{\"entity\": \"primary right parotid malignancy\"}]  [{\"entity\": \"liver metastases\"}]  [{\"entity\": \"malignancy\"}]  [{\"entity\": \"metastases\"}]</s>',\n",
       "  ' The result is: [{\"entity\": \"ACC\"}] </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data['ground_truth'], postprocessor.test_data['model_responses'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[{\"entity\": \"new-onset diabetes mellitus\"}] </s>',\n",
       "  '[{{\"entity\": \"\"}}]</s>',\n",
       "  '[{\"entity\": \"primary right parotid malignancy\"}, {\"entity\": \"liver metastases\"}] </s>',\n",
       "  '[{\"entity\": \"ACC\"}] </s>'],\n",
       " [' [{\"entity\": \"hypertension\"}, {\"entity\": \"dyslipidemia\"}]  [{\"entity\":',\n",
       "  ' The result of the test indicated ECS.\\n\\nReturned result: [{\"entity\": \"ECS\"}] </s>',\n",
       "  ' [{\"entity\": \"primary right parotid malignancy\"}, {\"entity\": \"liver metastases\"}] </s>',\n",
       "  ' [{\"entity\": \"\"}]  return [{\"entity\": \"\"}]  return [{\"entity\": \"ACC\"}]  return'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data['ground_truth'], postprocessor.test_data['model_responses'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/llama/7B_NoQuant_FT_cl_v2prompt/maxNewTokensFactor6_nShotsInference0_llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_64_64_0.01_1_0.0002_clent.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{postprocessing.save_directory}maxNewTokensFactor{max_new_tokens_factor}_nShotsInference{n_shots_inference}_{adapters.split('/')[1]}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISTRAL INSTRUCT simplest_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pferrazzi/miniconda3/envs/lm_finetune_env/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.evaluator import Evaluator\n",
    "from config import postprocessing\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "import pandas as pd\n",
    "from utils.generate_ft_adapters_list import generate_ft_adapters_list\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "adapters = 'ferrazzipietro/Mistral-7B-v0.1_simplest_prompt_adapters_en.layer1_4_torch.bfloat16_32_32_0.01_4_0.0002'\n",
    "model_checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "layer = 'en.layer1'\n",
    "language = layer.split('.')[0]\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n",
    "preprocessor = DataPreprocessor(model_checkpoint=model_checkpoint, \n",
    "                                tokenizer = model_checkpoint)\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset,\n",
    "                                                 instruction_on_response_format='Return the result in a json format: [{\"entity\":\"entity_name\"}].',\n",
    "                                                 simplest_prompt=True)\n",
    "_, val_data, _ = preprocessor.split_layer_into_train_val_test_(dataset, layer)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            #load_in_8bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # llm_int8_threshold= 6.0,\n",
    "            # llm_int8_has_fp16_weight = False,\n",
    "            # llm_int8_skip_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"],\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.40s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_checkpoint, low_cpu_mem_usage=True,\n",
    "    quantization_config = bnb_config,\n",
    "    return_dict=True, \n",
    "    #torch_dtype=torch.float16,\n",
    "    device_map= \"auto\")\n",
    "merged_model = PeftModel.from_pretrained(base_model, adapters, token=HF_TOKEN, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_eos_token=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating responses:   0%|          | 0/681 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating responses: 704it [06:34,  1.78it/s]                         \n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  4.74ba/s]\n"
     ]
    }
   ],
   "source": [
    "postprocessor = TestDataProcessor(test_data=val_data,\n",
    "                                  preprocessor=preprocessor, \n",
    "                                  n_shots_inference=0, \n",
    "                                  language=language, \n",
    "                                  tokenizer=tokenizer)\n",
    "postprocessor.add_inference_prompt_column()\n",
    "postprocessor.add_ground_truth_column()\n",
    "#try:\n",
    "postprocessor.add_responses_column(model=merged_model, \n",
    "                                tokenizer=tokenizer, \n",
    "                                batch_size=32, \n",
    "                                max_new_tokens_factor=2)\n",
    "postprocessor.test_data.to_csv(f\"data/test_data_processed/maxNewTokensFactor{6}_nShotsInference{0}_{adapters.split('/')[1]}.csv\", index=False)\n",
    "# except Exception as e:\n",
    "#     print(\"ERROR IN PROCESSING: \", Exception, adapters)\n",
    "del merged_model\n",
    "del base_model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  5.10ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2928976"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data.to_csv(f\"data/test_data_processed/maxNewTokensFactor{6}_nShotsInference{0}_{adapters.split('/')[1]}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[{\"entity\": \"46-year-old man\"}, {\"entity\": \"hypertension\"}, {\"entity\": \"dysl',\n",
       " '[{\"entity\": \"study\"}, {\"entity\": \"tests\"}, {\"entity\": \"indicated\"}, {\"entity\": \"ECS\"',\n",
       " '[{\"entity\": \"findings\"}, {\"entity\": \"malignancy\"}, {\"entity\": \"right parotid malignancy\"},',\n",
       " '[{\"entity\": \"parotidectomy\"}, {\"entity\": \"examination\"}, {\"entity\": \"confirmed\"}, {\"entity\":',\n",
       " '[{\"entity\": \"hypercortisolism\"}, {\"entity\": \" managed\"}, {\"entity\": \"metyrapone\"}, {\"',\n",
       " '[{\"entity\": \"50-years-old woman\"}, {\"entity\": \"hypertensive\"}, {\"entity\": \"hospital',\n",
       " '[{\"entity\": \"MNG\"}, {\"entity\": \"her mother\"}, {\"entity\": \"sisters\"}, {\"entity\": \"c',\n",
       " '[{\"entity\": \"signs of cervical compression\"}, {\"entity\": \"respiratory signs\"}, {\"entity\": \"digest',\n",
       " '[{\"entity\": \"thyroid dysfunction\"}]</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>',\n",
       " '[{\"entity\": \"mass\"}, {\"entity\": \"took\"}, {\"entity\": \"the front and the two sides of the neck\"',\n",
       " '[{\"entity\": \"surface\"}, {\"entity\": \"embossed\"}, {\"entity\": \"covered\"}, {\"entity\": \"skin\"}]',\n",
       " '[{\"entity\": \"veins of the collateral circulation\"}, {\"entity\": \"limited to the neck\"}] [{\"entity\": \"']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data['model_responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[{\"entity\": \"diagnosed\"}, {\"entity\": \"unveiled\"}, {\"entity\": \"referred\"}, {\"entity\": \"',\n",
       " '[{\"entity\": \"study\"}, {\"entity\": \"tests\"}, {\"entity\": \"indicated\"}, {\"entity\": \"ECS\"',\n",
       " '[{\"entity\": \"findings\"}, {\"entity\": \"cavity\"}, {\"entity\": \"malignancy\"}, {\"entity\": \"met',\n",
       " '[{\"entity\": \"parotidectomy\"}, {\"entity\": \"examination\"}, {\"entity\": \"confirmed\"}, {\"entity\":',\n",
       " '][{\"entity\": \"hypercortisolism\"}, {\"entity\": \"managed\"}] ---------- ------------ The entities contained in the',\n",
       " '[{\"entity\": \"hypertensive\"}, {\"entity\": \"hospitalized\"}, {\"entity\": \"mass\"}, {\"entity\": \"appe',\n",
       " '][{\"entity\": \"history\"}, {\"entity\": \"surgery\"}, {\"entity\": \"MNG\"}, {\"entity\": \"her mother\"},',\n",
       " '][{\"entity\": \"compression\"}, {\"entity\": \"signs\"}, {\"entity\": \"cervical compression\"}, {\"entity\": \"',\n",
       " '[{\"entity\": \"dysfunction\"}, {\"entity\": \"thyroid dysfunction\"}] \\n[{\"entity\": \"she\"',\n",
       " '[{\"entity\": \"The mass\"}, {\"entity\": \"the neck\"}] ][]][//][{\"entity\": \"The',\n",
       " '[{\"entity\": \"covered\"}, {\"entity\": \"surface\"}, {\"entity\": \"thin normal skin\"}] ][{\"entity\":',\n",
       " '[{\"entity\": \"limited\"}, {\"entity\": \"the neck\"}] ][{\"entity\": \"veins of the collateral circulation']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data['model_responses']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. diagnosed: diagnose\\n 2. mellitus: diabetes mellitus\\n 3. hypertension',\n",
       " '{\"Text\": \"study\", \"study\": \"Hormonal study\"}][{\"Entity\": \"study\"}, {\"Entity\":',\n",
       " '1. findings\\n 2. pointed\\n 3. malignancy\\n 4. metastases\\n 5. liver',\n",
       " 'Entities: \"parotidectomy\", \"examination\", \"confirmed\", \"ACC\", \"The patient\"\\n Types:',\n",
       " 'Entities: hypercortisolism, managed, metyrapone, ketoconazole, lanreotide,',\n",
       " '1. hypertensive \\n2. hospitalized\\n3. cervical mass\\n4. appeared\\n5. a',\n",
       " '{\"family history\", \"surgery\", \"MNG\", \"mother\", \"sisters\", \"cousins\"} ]]]>',\n",
       " '1. signs of cervical compression\\n 2. signs of respiratory compression\\n 3. signs of digestive compression',\n",
       " \"{'She': 'PERSON'} [{'thyroid dysfunction': 'disorder'}]</s></s></s></s></s></s></s></s></s>\",\n",
       " '{\"The mass\"} ]</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>',\n",
       " '{\"surface\": \"embossed and covered by a thin normal skin\"}][{\"surface\": \"covered by a thin normal skin\"}, {\"',\n",
       " 'Entities: {\"Some veins of the collateral circulation\"}[{\"veins of the collateral circulation\"}]\\n\\n{\"coll']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data['model_responses']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZEFIRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/mistral_finetuning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "/home/pferrazzi/mistral_finetuning/.venv/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.17it/s]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.evaluator import Evaluator\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "import pandas as pd\n",
    "from utils.generate_ft_adapters_list import generate_ft_adapters_list\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "model_checkpoint = \"mii-community/zefiro-7b-base-ITA\" \n",
    "\n",
    "layer = 'it.layer1'\n",
    "language = layer.split('.')[0]\n",
    "\n",
    "adapters = 'ferrazzipietro/zefiro-7b-base-ITA_adapters_it.layer1'\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n",
    "preprocessor = DataPreprocessor(model_checkpoint=model_checkpoint, \n",
    "                                tokenizer = model_checkpoint)\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset,\n",
    "                                                 instruction_on_response_format='Estrai le entità contenute nel testo.\\nRiporta i risultati in formato json: [{\"entity\":\"nome_entità\"}].',\n",
    "                                                 simplest_prompt=True)\n",
    "_, val_data, _ = preprocessor.split_layer_into_train_val_test_(dataset, layer)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            #load_in_8bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # llm_int8_threshold= 6.0,\n",
    "            # llm_int8_has_fp16_weight = False,\n",
    "            # llm_int8_skip_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"],\n",
    "            )\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_checkpoint, low_cpu_mem_usage=True,\n",
    "    quantization_config = bnb_config,\n",
    "    return_dict=True, \n",
    "    #torch_dtype=torch.float16,\n",
    "    device_map= \"auto\",\n",
    "    cache_dir='/data/disk1/share/pferrazzi/.cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = PeftModel.from_pretrained(base_model, adapters, token=HF_TOKEN, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_eos_token=False)\n",
    "# tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6/6 [00:00<00:00, 965.72 examples/s]\n",
      "Map: 100%|██████████| 6/6 [00:00<00:00, 1021.71 examples/s]\n",
      "generating responses:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating responses: 100%|██████████| 6/6 [00:41<00:00,  6.86s/it]\n"
     ]
    }
   ],
   "source": [
    "postprocessor = TestDataProcessor(test_data=val_data.select(range(6)),\n",
    "                                  preprocessor=preprocessor, \n",
    "                                  n_shots_inference=0, \n",
    "                                  language=language, \n",
    "                                  tokenizer=tokenizer)\n",
    "postprocessor.add_inference_prompt_column(simplest_prompt=False)\n",
    "postprocessor.add_ground_truth_column()\n",
    "#try:\n",
    "postprocessor.add_responses_column(model=merged_model, \n",
    "                                tokenizer=tokenizer, \n",
    "                                batch_size=3, \n",
    "                                max_new_tokens_factor=4)\n",
    "#postprocessor.test_data.to_csv(f\"data/TMPPmaxNewTokensFactor{4}_nShotsInference{0}_{adapters.split('/')[1]}.csv\", index=False)\n",
    "# except Exception as e:\n",
    "#     print(\"ERROR IN PROCESSING: \", Exception, adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Il caso riguarda un ragazzo di 12 anni, ricoverato presso l’UOC di Chirurgia Pediatrica di Treviso per addome acuto.',\n",
       " 'entities': [{'id': '5347',\n",
       "   'offsets': [103, 115],\n",
       "   'role': '',\n",
       "   'semantic_type_id': 'C0000727',\n",
       "   'text': 'addome acuto',\n",
       "   'type': 'CLINENTITY'}],\n",
       " 'original_text': 'Il caso riguarda un ragazzo di 12 anni, ricoverato presso l’UOC di Chirurgia Pediatrica di Treviso per addome acuto. Il ragazzo manifestava da circa una settimana vomiti ripetuti accompagnati da coliche addominali, inappetenza e vistoso calo ponderale (4 kg circa in una settimana). Al ricovero il paziente si presentava molto sofferente, astenico, disidratato, apiretico, con addome globoso, trattabile ma dolente alla palpazione profonda elettivamente in fossa iliaca destra; all’ascoltazione si percepiva una peristalsi metallica. Un’ecografia eseguita in pronto soccorso poneva la diagnosi di una peritonite da verosimile appendicite acuta complicata. Il ragazzo era quindi sottoposto in urgenza a una laparoscopia esplorativa, subito convertita per impossibilità di acquisire una camera laparoscopica sufficiente con le pressioni usuali, a causa dell’estrema distensione delle anse ileali, riscontrando una matassa ileale diffusamente dilatata e infiammata fino all’ileo terminale. A livello del medio-ileo si trovava un DM con al suo interno una massa palpabile occludente. Durante la resezione del diverticolo si apprezzava la fuoriuscita di abbondante materiale simil legnoso che successivamente risultava trattarsi di residui di semi di girasole che il ragazzo aveva ingerito interi volontariamente in grande quantità circa 10 giorni prima. L’intervento si concludeva con un’anastomosi ileo-ileale e con un’appendicectomia d’occasione. L’esame istologico del tratto intestinale asportato ha confermato trattarsi di una malformazione diverticolare del piccolo intestino. In quarta giornata post-operatoria il ragazzo manifestava nuovamente un quadro clinico addominale peritonitico da perforazione intestinale su deiscenza dell’anastomosi con abbondante dispersione di altro materiale ligneo nel peritoneo. Ripulita la cavità addominale e riconfezionata l’anastomosi intestinale, il ragazzo presentava in dodicesima giornata una fistola enterica con fuoriuscita di materiale biliare frammisto ancora a materiale ligneo. Il terzo approccio chirurgico evidenziava una peritonite plastica con parziale deiscenza dell’anastomosi. Il solo confezionamento di un’ileostomia a doppia canna di fucile a fronte delle precedenti anastomosi garantiva il successivo regolare decorso post-operatorio: all’alimentazione enterale era affiancata una nutrizione parenterale notturna con ripresa graduale del peso corporeo del paziente che a fronte dei 3 interventi aveva perso quasi 6 kg di peso corporeo. La ricanalizzazione intestinale veniva confezionata a distanza di altri 2 mesi dall’ultimo intervento.\\r\\n',\n",
       " 'original_id': 'IT101154',\n",
       " 'prompt': '<s><|user|>Il caso riguarda un ragazzo di 12 anni, ricoverato presso l’UOC di Chirurgia Pediatrica di Treviso per addome acuto.</s><|assistant|>[{\"entity\": \"addome acuto\"}] </s></s>',\n",
       " 'inference_prompt': '<s><|user|> Estrai le entità contenute nel testo.\\nRiporta i risultati in formato json: [{\"entity\":\"nome_entità\"}]. <<Il caso riguarda un ragazzo di 12 anni, ricoverato presso l’UOC di Chirurgia Pediatrica di Treviso per addome acuto.>>> </s><|assistant|>',\n",
       " 'ground_truth': '[{\"entity\": \"addome acuto\"}] ',\n",
       " 'model_responses': '[{\"entity\": \"ricoverato\"}, {\"entity\": \"addome\"}, {\"entity\": \"addome acuto\"}, {\"entity\": \"un ragazzo di 12 anni\"}] ][{\"entity\": \"addome acuto\"}, {\"entity\": \"un rag'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessor.test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-7B-Chat\", add_eos_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='Qwen/Qwen1.5-7B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([151646])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 1520/1520 [00:00<00:00, 5638.07 examples/s]\n",
      "Map: 100%|██████████| 170/170 [00:00<00:00, 7571.75 examples/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:02<00:00, 30.67s/it]\n",
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n",
      "adapter_config.json: 100%|██████████| 538/538 [00:00<00:00, 2.83MB/s]\n",
      "adapter_model.safetensors: 100%|██████████| 99.7M/99.7M [00:02<00:00, 39.9MB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.evaluator import Evaluator\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "import pandas as pd\n",
    "from utils.generate_ft_adapters_list import generate_ft_adapters_list\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from utils.output_cleaner import OutputCleaner\n",
    "from peft import PeftModel\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "max_new_tokens_factor_list = [6]\n",
    "n_shots_inference_list = [0]\n",
    "layer = 'en.layer1'\n",
    "language = layer.split('.')[0]\n",
    "\n",
    "BASE_MODEL_CHECKPOINT = 'Qwen/Qwen1.5-7B-Chat'\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n",
    "preprocessor = DataPreprocessor(BASE_MODEL_CHECKPOINT, BASE_MODEL_CHECKPOINT)\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset, instruction_on_response_format='Return the result in a json format: [{\"entity\":\"entity_name\"}].')\n",
    "_, val_data, _ = preprocessor.split_layer_into_train_val_test_(dataset, layer)\n",
    "\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            #load_in_8bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # llm_int8_threshold= 6.0,\n",
    "            # llm_int8_skip_modules= [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"],\n",
    "            )\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "    quantization_config = bnb_config,\n",
    "    return_dict=True,\n",
    "    device_map= 'auto',\n",
    "    cache_dir='/data/disk1/share/pferrazzi/.cache')\n",
    "\n",
    "\n",
    "adapters = 'ferrazzipietro/qwen1.5-7b-chat__adapters_en.layer1_8_torch.bfloat16_16_32_0.01_2_0.0002'\n",
    "merged_model = PeftModel.from_pretrained(base_model, adapters, token=HF_TOKEN, device_map='auto')\n",
    "#tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id: 151646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "print('tokenizer.pad_token_id:', tokenizer.pad_token_id)\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.embed_tokens = nn.Embedding(model.config.vocab_size, model.config.hidden_size, model.config.padding_idx)\n",
    "# tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.decode([151646])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from typing import Dict\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings_data = model.get_input_embeddings().weight.data\n",
    "        output_embeddings_data = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings_data[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings_data[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings_data[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings_data[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "        special_tokens_dict=dict(pad_token=\"<pad>\"),\n",
    "        tokenizer=tokenizer,\n",
    "        model=merged_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [{\"entity\": \"hypertension\"}, {\"entity\": \"dyslipidemia\"}, {\"entity\": \"diabetes mellitus\"}, {\"entity\": \"hypokalemia\"}, {\"entity\": \"A 46-year-old man\"}, {\"entity\": \"1-month\"}, {\"entity\": \"4-months\"}] </s>\n",
      " [{\"entity\": \"study\"}, {\"entity\": \"tests\"}, {\"entity\": \"ECS\"}, {\"entity\": \"ECS\"}, {\"entity\": \"indicated\"}] </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      " [{\"entity\": \"findings\"}, {\"entity\": \"parotid\"}, {\"entity\": \"malignancy\"}, {\"entity\": \"metastases\"}, {\"entity\": \"parotid\"}, {\"entity\": \"liver\"}] </s>\n",
      " [{\"entity\": \"parotidectomy\"}, {\"entity\": \"examination\"}, {\"entity\": \"confirmed\"}, {\"entity\": \"ACC\"}, {\"entity\": \"The patient\"}] </s></s></s></s></s></s>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "d = pd.read_csv('data/TMP_maxNewTokensFactor6_nShotsInference0_llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002.csv')\n",
    "for i, ex in d.iterrows():\n",
    "    print(ex['model_responses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_64_0.01_8_0.0002',\n",
       " '/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_64_0.01_4_0.0002',\n",
       " '/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_64_0.01_2_0.0002',\n",
       " '/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.01_8_0.0002',\n",
       " '/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.01_4_0.0002',\n",
       " '/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.01_2_0.0002',\n",
       " '/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_8_0.0002',\n",
       " '/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002',\n",
       " '/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_2_0.0002']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [\"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_64_0.01_8_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_64_0.01_4_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_64_0.01_2_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.01_8_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.01_4_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.01_2_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_64_0.01_8_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_64_0.01_4_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_64_0.01_2_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.01_8_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.01_4_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.01_2_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_64_0.01_8_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_64_0.01_4_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_64_0.01_2_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.01_8_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.01_4_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.01_2_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.01_8_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.01_4_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.01_2_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.05_8_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.05_8_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.05_4_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.05_4_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.05_2_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_64_32_0.05_2_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.01_8_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.01_4_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.01_2_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.05_8_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.05_8_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.05_4_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.05_4_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.05_2_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_32_32_0.05_2_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.01_8_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.01_4_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.01_2_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_8_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_8_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_2_0.0008\",\n",
    "    \"/ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.05_2_0.0002\"]\n",
    "ll = []\n",
    "for el in l:\n",
    "    if el.endswith('2') and '_16_' in el:\n",
    "        ll.append(el)\n",
    "len(set(l))\n",
    "print(len(ll))\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [{\"entity\": \"findings\"}, {\"entity\": \"parotid\"}, {\"entity\": \"malignancy\"}, {\"entity\": \"metastases\"}, {\"entity\": \"parotid\"}, {\"entity\": \"liver\"}] </s>\n",
      " [{\"entity\": \"parotidectomy\"}, {\"entity\": \"examination\"}, {\"entity\": \"confirmed\"}, {\"entity\": \"ACC\"}, {\"entity\": \"The patient\"}] </s></s></s></s></s></s>\n",
      " [{\"entity\": \"hypertension\"}, {\"entity\": \"dyslipidemia\"}, {\"entity\": \"diabetes mellitus\"}, {\"entity\": \"hypokalemia\"}, {\"entity\": \"A 46-year-old man\"}, {\"entity\": \"1-month\"}, {\"entity\": \"4-months\"}] </s>\n",
      " [{\"entity\": \"study\"}, {\"entity\": \"tests\"}, {\"entity\": \"ECS\"}, {\"entity\": \"ECS\"}, {\"entity\": \"indicated\"}] </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n"
     ]
    }
   ],
   "source": [
    "sorted_df = d.iloc[d['model_responses'].str.len().argsort()]\n",
    "for i, ex in sorted_df.iterrows():\n",
    "    print(ex['model_responses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZEFIRO 8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/mistral_finetuning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pferrazzi/mistral_finetuning/.venv/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ferrazzipietro/zefiro-7b-base-ITA__adapters_it.layer1_8_torch.bfloat16_32_64_0.01_2_0.0002\n",
      "QUANTIZATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from utils.evaluator import Evaluator\n",
    "from config.finetuning import config\n",
    "from config import postprocessing_params_mistral as postprocessing\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config.finetuning_zefiro import model_loading_params as models_params\n",
    "adapters = \"ferrazzipietro/zefiro-7b-base-ITA__adapters_it.layer1_8_torch.bfloat16_32_64_0.01_2_0.0002\" # \"ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_NoQuant_torch.bfloat16_16_32_0.01_2_0.0002\" # \"ferrazzipietro/Mistral-7B-Instruct-v0.2__adapters_en.layer1_NoQuant_torch.bfloat16_64_32_0.01_8_0.0002\"\n",
    "print(adapters)\n",
    "BASE_MODEL_CHECKPOINT = \"mii-community/zefiro-7b-base-ITA\"#\"Qwen/Qwen1.5-7B-Chat\"  # \"meta-llama/Llama-2-7b-chat-hf\"  # 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "layer = 'it.layer1' # 'en.layer1'\n",
    "quantization  = True\n",
    "\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "\n",
    "max_new_tokens_factor = 6\n",
    "n_shots_inference = 0\n",
    "language = layer.split('.')[0]\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n",
    "preprocessor = DataPreprocessor(model_checkpoint=BASE_MODEL_CHECKPOINT, \n",
    "                                tokenizer =BASE_MODEL_CHECKPOINT)\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset,\n",
    "                                                 simplest_prompt=False,\n",
    "                                                 instruction_on_response_format='Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}].')\n",
    "_, val_data, _ = preprocessor.split_layer_into_train_val_test_(dataset, layer)\n",
    "\n",
    "if not quantization:\n",
    "    print(\"NO QUANTIZATION\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "        return_dict=True,  \n",
    "        torch_dtype=postprocessing.torch_dtype,\n",
    "        device_map= \"auto\")    \n",
    "else:\n",
    "    print(\"QUANTIZATION\")\n",
    "    load_in_8bit = not models_params.load_in_4bit[0]\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit = False,# models_params.load_in_4bit[0],\n",
    "                load_in_8bit = True,# load_in_8bit,\n",
    "                # bnb_4bit_use_double_quant = models_params.bnb_4bit_use_double_quant,\n",
    "                # bnb_4bit_quant_type = models_params.bnb_4bit_quant_type[0],\n",
    "                # bnb_4bit_compute_dtype = models_params.bnb_4bit_compute_dtype[0],\n",
    "                llm_int8_threshold = models_params.llm_int8_threshold[0],\n",
    "                llm_int8_has_fp16_weight = False # models_params.llm_int8_has_fp16_weight,\n",
    "                # llm_int8_skip_modules = # models_params.llm_int8_skip_modules\n",
    "                )\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "        quantization_config = bnb_config,\n",
    "        return_dict=True,  \n",
    "        #torch_dtype=torch.float16,\n",
    "        device_map= \"auto\",\n",
    "        cache_dir='/data/disk1/share/pferrazzi/.cache'\n",
    "        )\n",
    "# merged_model = PeftModel.from_pretrained(base_model, adapters, \n",
    "#                                          token=HF_TOKEN, \n",
    "#                                          device_map='auto',\n",
    "#                                          is_trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_CHECKPOINT, add_eos_token=True)\n",
    "#tokenizer.pad_token = \"<unk>\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "input = \"\"\"<s><|user|> Estrai le entità contenute nel testo.\n",
    "Riporta i risultati in formato json: [{\"\"entity\"\":\"\"nome_entità\"\"}]. <<Il caso riguarda un ragazzo di 12 anni, ricoverato presso l’UOC di Chirurgia Pediatrica di Treviso per addome acuto.>>> </s><|assistant|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8041021440"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeds = tokenizer(input, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "model_inputs = encodeds.to('cuda')\n",
    "generated_ids = base_model.generate(**model_inputs, do_sample=True, max_new_tokens=100,  \n",
    "                                       pad_token_id=tokenizer.pad_token_id,\n",
    "                                       temperature = 1.0) # max_new_tokens=max_new_tokens,\n",
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pferrazzi/mistral_finetuning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUANTIZATION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n",
      "adapter_config.json: 100%|██████████| 547/547 [00:00<00:00, 2.59MB/s]\n",
      "adapter_model.safetensors: 100%|██████████| 98.1M/98.1M [00:02<00:00, 37.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import dotenv_values\n",
    "from datasets import load_dataset, Dataset\n",
    "from utils.data_preprocessor import DataPreprocessor\n",
    "from config import postprocessing_params_mistral as postprocessing\n",
    "from utils.test_data_processor import TestDataProcessor\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import gc\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from config.finetuning_llama2 import model_loading_params as models_params\n",
    "adapters = \"ferrazzipietro/Llama-2-7b-chat-hf_adapters_en.layer1_8_torch.bfloat16_16_32_0.01_4_0.0002\"\n",
    "BASE_MODEL_CHECKPOINT = \"meta-llama/Llama-2-7b-chat-hf\"#\"mii-community/zefiro-7b-base-ITA\"#\"Qwen/Qwen1.5-7B-Chat\"  # \"meta-llama/Llama-2-7b-chat-hf\"  # 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "layer = 'en.layer1' # 'en.layer1'\n",
    "quantization  = True\n",
    "\n",
    "\n",
    "HF_TOKEN = dotenv_values(\".env.base\")['HF_TOKEN']\n",
    "LLAMA_TOKEN = dotenv_values(\".env.base\")['LLAMA_TOKEN']\n",
    "\n",
    "max_new_tokens_factor = 6\n",
    "n_shots_inference = 0\n",
    "language = layer.split('.')[0]\n",
    "\n",
    "dataset = load_dataset(\"ferrazzipietro/e3c-sentences\", token=HF_TOKEN)\n",
    "dataset = dataset[layer]\n",
    "preprocessor = DataPreprocessor(model_checkpoint=BASE_MODEL_CHECKPOINT, \n",
    "                                tokenizer =BASE_MODEL_CHECKPOINT,\n",
    "                                token_llama = LLAMA_TOKEN,)\n",
    "dataset = preprocessor.preprocess_data_one_layer(dataset,\n",
    "                                                 simplest_prompt=False,\n",
    "                                                 instruction_on_response_format='Extract the entities contained in the text. Extract only entities contained in the text.\\nReturn the result in a json format: [{\"entity\":\"entity_name\"}].')\n",
    "_, val_data, _ = preprocessor.split_layer_into_train_val_test_(dataset, layer)\n",
    "\n",
    "if not quantization:\n",
    "    print(\"NO QUANTIZATION\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "        return_dict=True,  \n",
    "        torch_dtype=postprocessing.torch_dtype,\n",
    "        device_map= \"auto\",\n",
    "        cache_dir='/data/disk1/share/pferrazzi/.cache')    \n",
    "else:\n",
    "    print(\"QUANTIZATION\")\n",
    "    load_in_8bit = not models_params.load_in_4bit[0]\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit = False,# models_params.load_in_4bit[0],\n",
    "                load_in_8bit = True,# load_in_8bit,\n",
    "                # bnb_4bit_use_double_quant = models_params.bnb_4bit_use_double_quant,\n",
    "                # bnb_4bit_quant_type = models_params.bnb_4bit_quant_type[0],\n",
    "                # bnb_4bit_compute_dtype = models_params.bnb_4bit_compute_dtype[0],\n",
    "                llm_int8_threshold = models_params.llm_int8_threshold[0],\n",
    "                # llm_int8_has_fp16_weight = False # models_params.llm_int8_has_fp16_weight,\n",
    "                # llm_int8_skip_modules = models_params.llm_int8_skip_modules\n",
    "                )\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_CHECKPOINT, low_cpu_mem_usage=True,\n",
    "        quantization_config = bnb_config,\n",
    "        return_dict=True,  \n",
    "        # torch_dtype=torch.bf16,\n",
    "        device_map= \"auto\",\n",
    "        cache_dir='/data/disk1/share/pferrazzi/.cache',\n",
    "        token = LLAMA_TOKEN\n",
    "        )\n",
    "merged_model = PeftModel.from_pretrained(base_model, adapters, \n",
    "                                         token=HF_TOKEN, \n",
    "                                         device_map='auto',\n",
    "                                         is_trainable = False)\n",
    "# merged_model = base_model.load_adapter(adapters)\n",
    "# merged_model.enable_adapters()\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_CHECKPOINT, \n",
    "                                          add_eos_token=False,\n",
    "                                          token = LLAMA_TOKEN)\n",
    "#tokenizer.pad_token = \"<unk>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60/60 [00:00<00:00, 4998.28 examples/s]\n",
      "Map: 100%|██████████| 60/60 [00:00<00:00, 6228.86 examples/s]\n",
      "generating responses:  68%|██████▊   | 41/60 [08:24<03:53, 12.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m postprocessor\u001b[38;5;241m.\u001b[39madd_ground_truth_column()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mpostprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_responses_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmax_new_tokens_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m postprocessor\u001b[38;5;241m.\u001b[39mtest_data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/TMP_maxNewTokensFactor\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_new_tokens_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_nShotsInference\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_shots_inference\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapters\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/mistral_finetuning/utils/test_data_processor.py:183\u001b[0m, in \u001b[0;36mTestDataProcessor.add_responses_column\u001b[0;34m(self, model, tokenizer, batch_size, max_new_tokens_factor)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indexes[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    182\u001b[0m     indici \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(idx, indexes[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m--> 183\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_model_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindici\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     responses_col\u001b[38;5;241m.\u001b[39mextend(tmp)\n\u001b[1;32m    185\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(batch_size)\n",
      "File \u001b[0;32m~/mistral_finetuning/utils/test_data_processor.py:156\u001b[0m, in \u001b[0;36mTestDataProcessor._generate_model_response\u001b[0;34m(self, examples, model, tokenizer, max_new_tokens_factor)\u001b[0m\n\u001b[1;32m    154\u001b[0m encodeds \u001b[38;5;241m=\u001b[39m tokenizer(prompts, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    155\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m encodeds\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 156\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# max_new_tokens=max_new_tokens,\u001b[39;00m\n\u001b[1;32m    159\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m generated_ids[:, encodeds\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:]\n\u001b[1;32m    160\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids)\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/peft/peft_model.py:1060\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1764\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1757\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1758\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1759\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1761\u001b[0m     )\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1781\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1782\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1788\u001b[0m     )\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2861\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2862\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2866\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1181\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1178\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1181\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1068\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1059\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1060\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         use_cache,\n\u001b[1;32m   1066\u001b[0m     )\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1068\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:796\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    795\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:404\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     kv_seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mget_usable_length(kv_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx)\n\u001b[1;32m    403\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(value_states, seq_len\u001b[38;5;241m=\u001b[39mkv_seq_len)\n\u001b[0;32m--> 404\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos}  \u001b[38;5;66;03m# Specific to RoPE models\u001b[39;00m\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:234\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    232\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    233\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 234\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    235\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m~/mistral_finetuning/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:208\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    206\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    207\u001b[0m x2 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "postprocessor = TestDataProcessor(test_data=val_data.select(range(60)), \n",
    "                                  preprocessor=preprocessor, \n",
    "                                  n_shots_inference=n_shots_inference, \n",
    "                                  language=language, \n",
    "                                  tokenizer=tokenizer)\n",
    "postprocessor.add_inference_prompt_column(simplest_prompt=False)\n",
    "postprocessor.add_ground_truth_column()\n",
    "#try:\n",
    "postprocessor.add_responses_column(model=merged_model, \n",
    "                                        tokenizer=tokenizer, \n",
    "                                        batch_size=1, \n",
    "                                        max_new_tokens_factor=max_new_tokens_factor)\n",
    "postprocessor.test_data.to_csv(f\"data/TMP_maxNewTokensFactor{max_new_tokens_factor}_nShotsInference{n_shots_inference}_{adapters.split('/')[1]}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No parameters with NaN values found.\n"
     ]
    }
   ],
   "source": [
    "def check_nan_parameters(merged_model):\n",
    "    nan_found = False\n",
    "    for name, param in merged_model.named_parameters():\n",
    "        if torch.isnan(param).any():\n",
    "            print(f'Parameter {name} has NaN values!')\n",
    "            nan_found = True\n",
    "    if not nan_found:\n",
    "        print('No parameters with NaN values found.')\n",
    "\n",
    "check_nan_parameters(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmerged_model\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#display(param)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_model' is not defined"
     ]
    }
   ],
   "source": [
    "for name, param in merged_model.named_parameters():\n",
    "    print(f'Parameter {name}')\n",
    "    #display(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
