/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:09<01:04,  9.19s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:12<00:35,  5.90s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:16<00:24,  4.89s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:20<00:17,  4.46s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:24<00:12,  4.23s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:28<00:08,  4.20s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:31<00:03,  3.90s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:33<00:00,  3.20s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:33<00:00,  4.15s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
generating responses:   0%|          | 0/681 [00:00<?, ?it/s]2024-03-19 11:23:58.008707: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-19 11:23:58.118012: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-19 11:24:01.513189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
generating responses:   3%|▎         | 20/681 [00:17<09:30,  1.16it/s]generating responses:   6%|▌         | 40/681 [00:22<05:22,  1.99it/s]generating responses:   9%|▉         | 60/681 [00:26<03:45,  2.76it/s]generating responses:  12%|█▏        | 80/681 [00:34<03:47,  2.64it/s]generating responses:  15%|█▍        | 100/681 [00:42<03:45,  2.57it/s]generating responses:  18%|█▊        | 120/681 [00:50<03:43,  2.52it/s]generating responses:  21%|██        | 140/681 [00:59<03:38,  2.47it/s]generating responses:  23%|██▎       | 160/681 [01:07<03:32,  2.45it/s]generating responses:  26%|██▋       | 180/681 [01:15<03:24,  2.45it/s]generating responses:  29%|██▉       | 200/681 [01:23<03:12,  2.50it/s]generating responses:  32%|███▏      | 220/681 [01:32<03:12,  2.40it/s]generating responses:  35%|███▌      | 240/681 [01:40<03:02,  2.42it/s]generating responses:  38%|███▊      | 260/681 [01:48<02:55,  2.40it/s]generating responses:  41%|████      | 280/681 [01:57<02:48,  2.38it/s]generating responses:  44%|████▍     | 300/681 [02:05<02:39,  2.38it/s]generating responses:  47%|████▋     | 320/681 [02:11<02:17,  2.62it/s]generating responses:  50%|████▉     | 340/681 [02:20<02:14,  2.54it/s]generating responses:  53%|█████▎    | 360/681 [02:28<02:10,  2.46it/s]generating responses:  56%|█████▌    | 380/681 [02:37<02:03,  2.44it/s]generating responses:  59%|█████▊    | 400/681 [02:42<01:42,  2.75it/s]generating responses:  62%|██████▏   | 420/681 [02:51<01:41,  2.58it/s]generating responses:  65%|██████▍   | 440/681 [02:59<01:34,  2.54it/s]generating responses:  68%|██████▊   | 460/681 [03:07<01:28,  2.48it/s]generating responses:  70%|███████   | 480/681 [03:11<01:08,  2.93it/s]generating responses:  73%|███████▎  | 500/681 [03:20<01:05,  2.75it/s]generating responses:  76%|███████▋  | 520/681 [03:28<01:00,  2.66it/s]generating responses:  79%|███████▉  | 540/681 [03:36<00:54,  2.59it/s]generating responses:  82%|████████▏ | 560/681 [03:44<00:47,  2.53it/s]generating responses:  85%|████████▌ | 580/681 [03:53<00:40,  2.48it/s]generating responses:  88%|████████▊ | 600/681 [04:00<00:31,  2.55it/s]generating responses:  91%|█████████ | 620/681 [04:08<00:24,  2.50it/s]generating responses:  94%|█████████▍| 640/681 [04:17<00:16,  2.41it/s]generating responses:  97%|█████████▋| 660/681 [04:26<00:08,  2.41it/s]generating responses: 100%|█████████▉| 680/681 [04:35<00:00,  2.36it/s]generating responses: 700it [04:35,  3.32it/s]                         generating responses: 700it [04:35,  2.54it/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  4.51ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  4.51ba/s]
generating responses:   0%|          | 0/681 [00:00<?, ?it/s]generating responses:   3%|▎         | 20/681 [00:09<05:03,  2.18it/s]generating responses:   6%|▌         | 40/681 [00:18<04:51,  2.20it/s]generating responses:   9%|▉         | 60/681 [00:27<04:43,  2.19it/s]generating responses:  12%|█▏        | 80/681 [00:36<04:36,  2.17it/s]generating responses:  15%|█▍        | 100/681 [00:46<04:28,  2.16it/s]generating responses:  18%|█▊        | 120/681 [00:55<04:21,  2.15it/s]generating responses:  21%|██        | 140/681 [01:05<04:14,  2.13it/s]generating responses:  23%|██▎       | 160/681 [01:14<04:05,  2.12it/s]generating responses:  26%|██▋       | 180/681 [01:23<03:55,  2.13it/s]generating responses:  29%|██▉       | 200/681 [01:33<03:46,  2.12it/s]generating responses:  32%|███▏      | 220/681 [01:43<03:43,  2.06it/s]generating responses:  35%|███▌      | 240/681 [01:52<03:26,  2.13it/s]generating responses:  38%|███▊      | 260/681 [02:01<03:19,  2.11it/s]generating responses:  41%|████      | 280/681 [02:11<03:10,  2.11it/s]generating responses:  44%|████▍     | 300/681 [02:20<02:59,  2.12it/s]generating responses:  47%|████▋     | 320/681 [02:30<02:48,  2.14it/s]generating responses:  50%|████▉     | 340/681 [02:39<02:39,  2.13it/s]generating responses:  53%|█████▎    | 360/681 [02:49<02:31,  2.12it/s]generating responses:  56%|█████▌    | 380/681 [02:58<02:22,  2.11it/s]generating responses:  59%|█████▊    | 400/681 [03:07<02:12,  2.12it/s]generating responses:  62%|██████▏   | 420/681 [03:17<02:04,  2.10it/s]generating responses:  65%|██████▍   | 440/681 [03:26<01:53,  2.12it/s]generating responses:  68%|██████▊   | 460/681 [03:36<01:44,  2.12it/s]generating responses:  70%|███████   | 480/681 [03:45<01:34,  2.13it/s]generating responses:  73%|███████▎  | 500/681 [03:54<01:24,  2.13it/s]generating responses:  76%|███████▋  | 520/681 [04:04<01:14,  2.15it/s]generating responses:  79%|███████▉  | 540/681 [04:13<01:05,  2.16it/s]generating responses:  82%|████████▏ | 560/681 [04:22<00:56,  2.15it/s]generating responses:  85%|████████▌ | 580/681 [04:32<00:47,  2.14it/s]generating responses:  88%|████████▊ | 600/681 [04:41<00:37,  2.15it/s]generating responses:  91%|█████████ | 620/681 [04:50<00:28,  2.14it/s]generating responses:  94%|█████████▍| 640/681 [05:00<00:19,  2.11it/s]generating responses:  97%|█████████▋| 660/681 [05:09<00:09,  2.12it/s]generating responses: 100%|█████████▉| 680/681 [05:19<00:00,  2.09it/s]generating responses: 700it [05:21,  2.77it/s]                         generating responses: 700it [05:21,  2.18it/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  3.08ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  3.08ba/s]
generating responses:   0%|          | 0/681 [00:00<?, ?it/s]generating responses:   3%|▎         | 20/681 [00:10<05:49,  1.89it/s]generating responses:   6%|▌         | 40/681 [00:19<05:14,  2.04it/s]generating responses:   9%|▉         | 60/681 [00:30<05:14,  1.97it/s]generating responses:  12%|█▏        | 80/681 [00:40<05:10,  1.94it/s]generating responses:  15%|█▍        | 100/681 [00:51<05:03,  1.92it/s]generating responses:  18%|█▊        | 120/681 [01:02<05:01,  1.86it/s]generating responses:  21%|██        | 140/681 [01:13<04:51,  1.86it/s]generating responses:  23%|██▎       | 160/681 [01:24<04:40,  1.86it/s]generating responses:  26%|██▋       | 180/681 [01:35<04:28,  1.86it/s]generating responses:  29%|██▉       | 200/681 [01:46<04:18,  1.86it/s]generating responses:  32%|███▏      | 220/681 [01:58<04:17,  1.79it/s]generating responses:  35%|███▌      | 240/681 [02:08<04:00,  1.84it/s]generating responses:  38%|███▊      | 260/681 [02:19<03:51,  1.82it/s]generating responses:  41%|████      | 280/681 [02:30<03:42,  1.81it/s]generating responses:  44%|████▍     | 300/681 [02:41<03:28,  1.83it/s]generating responses:  47%|████▋     | 320/681 [02:51<03:13,  1.87it/s]generating responses:  50%|████▉     | 340/681 [03:02<03:04,  1.85it/s]generating responses:  53%|█████▎    | 360/681 [03:13<02:54,  1.84it/s]generating responses:  56%|█████▌    | 380/681 [03:24<02:43,  1.84it/s]generating responses:  59%|█████▊    | 400/681 [03:35<02:32,  1.85it/s]generating responses:  62%|██████▏   | 420/681 [03:46<02:23,  1.81it/s]generating responses:  65%|██████▍   | 440/681 [03:57<02:11,  1.83it/s]generating responses:  68%|██████▊   | 460/681 [04:08<02:01,  1.82it/s]generating responses:  70%|███████   | 480/681 [04:19<01:49,  1.84it/s]generating responses:  73%|███████▎  | 500/681 [04:28<01:33,  1.93it/s]generating responses:  76%|███████▋  | 520/681 [04:36<01:18,  2.04it/s]generating responses:  79%|███████▉  | 540/681 [04:47<01:10,  2.00it/s]generating responses:  82%|████████▏ | 560/681 [04:57<01:01,  1.97it/s]generating responses:  85%|████████▌ | 580/681 [05:08<00:52,  1.94it/s]generating responses:  88%|████████▊ | 600/681 [05:18<00:41,  1.96it/s]generating responses:  91%|█████████ | 620/681 [05:29<00:31,  1.93it/s]generating responses:  94%|█████████▍| 640/681 [05:40<00:21,  1.87it/s]generating responses:  97%|█████████▋| 660/681 [05:51<00:11,  1.87it/s]generating responses: 100%|█████████▉| 680/681 [06:03<00:00,  1.82it/s]generating responses: 700it [06:04,  2.48it/s]                         generating responses: 700it [06:04,  1.92it/s]
Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  3.78ba/s]Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  3.78ba/s]
generating responses:   0%|          | 0/681 [00:00<?, ?it/s]generating responses:   3%|▎         | 20/681 [00:11<06:13,  1.77it/s]generating responses:   6%|▌         | 40/681 [00:20<05:25,  1.97it/s]generating responses:   9%|▉         | 60/681 [00:32<05:36,  1.84it/s]generating responses:  12%|█▏        | 80/681 [00:44<05:38,  1.78it/s]generating responses:  15%|█▍        | 100/681 [00:55<05:31,  1.75it/s]generating responses:  18%|█▊        | 120/681 [01:07<05:24,  1.73it/s]generating responses:  21%|██        | 140/681 [01:19<05:15,  1.71it/s]generating responses:  23%|██▎       | 160/681 [01:31<05:06,  1.70it/s]generating responses:  26%|██▋       | 180/681 [01:43<04:55,  1.70it/s]generating responses:  29%|██▉       | 200/681 [01:55<04:44,  1.69it/s]generating responses:  29%|██▉       | 200/681 [01:58<04:45,  1.68it/s]
Traceback (most recent call last):
  File "get_results_base_model_qwen.py", line 77, in <module>
    postprocessor.add_responses_column(model=model, 
  File "/extra/ferrazzi/llm/mistral_finetuning/utils/test_data_processor.py", line 175, in add_responses_column
    tmp = self._generate_model_response(self.test_data.select(indici), model, tokenizer, max_new_tokens_factor)
  File "/extra/ferrazzi/llm/mistral_finetuning/utils/test_data_processor.py", line 150, in _generate_model_response
    generated_ids = model.generate(**model_inputs, do_sample=True, max_new_tokens=max_new_tokens,  pad_token_id=tokenizer.eos_token_id) # max_new_tokens=max_new_tokens,
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/generation/utils.py", line 1592, in generate
    return self.sample(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/generation/utils.py", line 2696, in sample
    outputs = self(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 1186, in forward
    logits = self.lm_head(hidden_states)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/bitsandbytes/nn/modules.py", line 256, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py", line 577, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py", line 516, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB (GPU 0; 47.54 GiB total capacity; 37.11 GiB already allocated; 854.69 MiB free; 45.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
