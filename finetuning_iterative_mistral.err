2024-03-11 09:49:15.973851: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-11 09:49:16.022018: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-11 09:49:17.396991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
wandb: Currently logged in as: ferrazzipietro. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/ferrazzi/.netrc
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /extra/ferrazzi/llm/mistral_finetuning/wandb/run-20240311_094926-enz0uowv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run en.layer12024-03-11 09:49:26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2__adapters_en.layer1_4_torch.bfloat16_16_32_0.05_2_0.0002
wandb: üöÄ View run at https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2__adapters_en.layer1_4_torch.bfloat16_16_32_0.05_2_0.0002/runs/enz0uowv
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:06<00:12,  6.42s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:12<00:06,  6.17s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:17<00:00,  5.74s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:17<00:00,  5.88s/it]
Map:   0%|          | 0/1520 [00:00<?, ? examples/s]Map:  23%|‚ñà‚ñà‚ñé       | 347/1520 [00:00<00:00, 3415.64 examples/s]Map:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 917/1520 [00:00<00:00, 4745.03 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 4093.65 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 4012.94 examples/s]
Map:   0%|          | 0/1520 [00:00<?, ? examples/s]Map:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1000/1520 [00:00<00:00, 3955.33 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 3961.39 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1520/1520 [00:00<00:00, 3564.54 examples/s]
Map:   0%|          | 0/170 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170/170 [00:00<00:00, 2508.25 examples/s]
Map:   0%|          | 0/669 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 669/669 [00:00<00:00, 4330.16 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 669/669 [00:00<00:00, 3679.06 examples/s]
Map:   0%|          | 0/681 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:00<00:00, 3950.41 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:00<00:00, 3633.88 examples/s]
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:284: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/126 [00:00<?, ?it/s]  1%|          | 1/126 [00:04<10:04,  4.84s/it]  2%|‚ñè         | 2/126 [00:07<06:49,  3.30s/it]  2%|‚ñè         | 3/126 [00:09<05:32,  2.70s/it]  3%|‚ñé         | 4/126 [00:10<04:48,  2.36s/it]  4%|‚ñç         | 5/126 [00:12<04:18,  2.14s/it]  5%|‚ñç         | 6/126 [00:14<03:55,  1.96s/it]  6%|‚ñå         | 7/126 [00:15<03:38,  1.84s/it]  6%|‚ñã         | 8/126 [00:17<03:23,  1.73s/it]  7%|‚ñã         | 9/126 [00:18<03:09,  1.62s/it]  8%|‚ñä         | 10/126 [00:19<02:53,  1.49s/it]                                                  8%|‚ñä         | 10/126 [00:19<02:53,  1.49s/it]  9%|‚ñä         | 11/126 [00:23<03:55,  2.04s/it] 10%|‚ñâ         | 12/126 [00:25<03:58,  2.09s/it] 10%|‚ñà         | 13/126 [00:27<03:51,  2.05s/it] 11%|‚ñà         | 14/126 [00:29<03:44,  2.00s/it] 12%|‚ñà‚ñè        | 15/126 [00:31<03:33,  1.93s/it] 13%|‚ñà‚ñé        | 16/126 [00:32<03:22,  1.84s/it] 13%|‚ñà‚ñé        | 17/126 [00:34<03:12,  1.77s/it] 14%|‚ñà‚ñç        | 18/126 [00:35<03:02,  1.69s/it] 15%|‚ñà‚ñå        | 19/126 [00:37<02:51,  1.60s/it] 16%|‚ñà‚ñå        | 20/126 [00:38<02:38,  1.49s/it]                                                 16%|‚ñà‚ñå        | 20/126 [00:38<02:38,  1.49s/it] 17%|‚ñà‚ñã        | 21/126 [00:42<04:02,  2.30s/it] 17%|‚ñà‚ñã        | 22/126 [00:44<03:59,  2.30s/it] 18%|‚ñà‚ñä        | 23/126 [00:46<03:48,  2.22s/it] 19%|‚ñà‚ñâ        | 24/126 [00:48<03:37,  2.13s/it] 20%|‚ñà‚ñâ        | 25/126 [00:50<03:23,  2.02s/it] 21%|‚ñà‚ñà        | 26/126 [00:52<03:10,  1.91s/it] 21%|‚ñà‚ñà‚ñè       | 27/126 [00:53<02:59,  1.82s/it] 22%|‚ñà‚ñà‚ñè       | 28/126 [00:55<02:48,  1.72s/it] 23%|‚ñà‚ñà‚ñé       | 29/126 [00:56<02:37,  1.63s/it] 24%|‚ñà‚ñà‚ñç       | 30/126 [00:58<02:26,  1.53s/it]                                                 24%|‚ñà‚ñà‚ñç       | 30/126 [00:58<02:26,  1.53s/it] 25%|‚ñà‚ñà‚ñç       | 31/126 [01:01<03:20,  2.11s/it] 25%|‚ñà‚ñà‚ñå       | 32/126 [01:03<03:21,  2.15s/it] 26%|‚ñà‚ñà‚ñå       | 33/126 [01:05<03:16,  2.11s/it] 27%|‚ñà‚ñà‚ñã       | 34/126 [01:07<03:07,  2.04s/it] 28%|‚ñà‚ñà‚ñä       | 35/126 [01:09<02:56,  1.94s/it] 29%|‚ñà‚ñà‚ñä       | 36/126 [01:11<02:47,  1.86s/it] 29%|‚ñà‚ñà‚ñâ       | 37/126 [01:12<02:37,  1.77s/it] 30%|‚ñà‚ñà‚ñà       | 38/126 [01:14<02:28,  1.68s/it] 31%|‚ñà‚ñà‚ñà       | 39/126 [01:15<02:19,  1.60s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 40/126 [01:16<02:08,  1.49s/it]                                                 32%|‚ñà‚ñà‚ñà‚ñè      | 40/126 [01:16<02:08,  1.49s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 41/126 [01:19<02:37,  1.85s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 42/126 [01:20<02:17,  1.64s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 43/126 [01:24<03:18,  2.39s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 44/126 [01:26<03:08,  2.30s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 45/126 [01:28<02:57,  2.19s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 46/126 [01:30<02:45,  2.07s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 47/126 [01:32<02:35,  1.96s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 48/126 [01:33<02:25,  1.87s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 49/126 [01:35<02:17,  1.79s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 50/126 [01:36<02:09,  1.70s/it]                                                 40%|‚ñà‚ñà‚ñà‚ñâ      | 50/126 [01:36<02:09,  1.70s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 51/126 [01:38<02:01,  1.62s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 52/126 [01:39<01:52,  1.52s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 53/126 [01:42<02:30,  2.06s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 54/126 [01:45<02:33,  2.13s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 55/126 [01:47<02:30,  2.11s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 56/126 [01:49<02:24,  2.06s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 57/126 [01:51<02:16,  1.98s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 58/126 [01:52<02:08,  1.89s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 59/126 [01:54<02:01,  1.81s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 60/126 [01:55<01:53,  1.73s/it]                                                 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 60/126 [01:55<01:53,  1.73s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 61/126 [01:57<01:46,  1.63s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 62/126 [01:58<01:37,  1.52s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 63/126 [02:02<02:26,  2.33s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 64/126 [02:05<02:24,  2.34s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 65/126 [02:07<02:18,  2.26s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 66/126 [02:09<02:09,  2.16s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 67/126 [02:10<02:00,  2.05s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 68/126 [02:12<01:52,  1.93s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 69/126 [02:14<01:44,  1.84s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 70/126 [02:15<01:36,  1.72s/it]                                                 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 70/126 [02:15<01:36,  1.72s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 71/126 [02:17<01:29,  1.63s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 72/126 [02:18<01:21,  1.51s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 73/126 [02:21<01:53,  2.14s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 74/126 [02:24<01:53,  2.19s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 75/126 [02:26<01:49,  2.15s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 76/126 [02:28<01:44,  2.09s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 77/126 [02:30<01:38,  2.00s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 78/126 [02:31<01:31,  1.91s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 79/126 [02:33<01:25,  1.82s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 80/126 [02:34<01:19,  1.72s/it]                                                 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 80/126 [02:34<01:19,  1.72s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 81/126 [02:36<01:13,  1.63s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 82/126 [02:37<01:06,  1.52s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 83/126 [02:40<01:17,  1.80s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 84/126 [02:41<01:08,  1.64s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 85/126 [02:45<01:40,  2.46s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 86/126 [02:47<01:35,  2.40s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 87/126 [02:49<01:29,  2.29s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 88/126 [02:51<01:22,  2.18s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 89/126 [02:53<01:16,  2.06s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 90/126 [02:55<01:09,  1.94s/it]                                                 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 90/126 [02:55<01:09,  1.94s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 91/126 [02:56<01:04,  1.85s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 92/126 [02:58<01:00,  1.77s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 93/126 [02:59<00:55,  1.68s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 94/126 [03:01<00:50,  1.57s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 95/126 [03:04<01:05,  2.11s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 96/126 [03:06<01:05,  2.17s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 97/126 [03:09<01:01,  2.13s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 98/126 [03:10<00:57,  2.07s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 99/126 [03:12<00:53,  1.98s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 100/126 [03:14<00:49,  1.89s/it]                                                  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 100/126 [03:14<00:49,  1.89s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 101/126 [03:16<00:45,  1.81s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 102/126 [03:17<00:41,  1.71s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 103/126 [03:18<00:37,  1.62s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 104/126 [03:20<00:33,  1.50s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 105/126 [03:23<00:42,  2.04s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 106/126 [03:25<00:42,  2.11s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 107/126 [03:27<00:39,  2.08s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 108/126 [03:29<00:36,  2.03s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 109/126 [03:31<00:32,  1.93s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 110/126 [03:32<00:29,  1.85s/it]                                                  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 110/126 [03:32<00:29,  1.85s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 111/126 [03:34<00:26,  1.78s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 112/126 [03:36<00:23,  1.69s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 113/126 [03:37<00:20,  1.61s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 114/126 [03:38<00:18,  1.50s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 115/126 [03:42<00:23,  2.12s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 116/126 [03:44<00:21,  2.16s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 117/126 [03:46<00:19,  2.13s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 118/126 [03:48<00:16,  2.08s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 119/126 [03:50<00:13,  1.99s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 120/126 [03:52<00:11,  1.90s/it]                                                  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 120/126 [03:52<00:11,  1.90s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 121/126 [03:53<00:09,  1.81s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 122/126 [03:55<00:06,  1.71s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 123/126 [03:56<00:04,  1.62s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 124/126 [03:57<00:03,  1.51s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 125/126 [04:01<00:02,  2.21s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [04:02<00:00,  1.96s/it]                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [04:03<00:00,  1.96s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [04:03<00:00,  1.93s/it]
adapter_model.safetensors:   0%|          | 0.00/92.3M [00:00<?, ?B/s]adapter_model.safetensors:   0%|          | 98.3k/92.3M [00:00<01:40, 922kB/s]adapter_model.safetensors:   3%|‚ñé         | 2.67M/92.3M [00:00<00:06, 13.1MB/s]adapter_model.safetensors:   5%|‚ñç         | 4.52M/92.3M [00:00<00:09, 8.79MB/s]adapter_model.safetensors:   6%|‚ñã         | 6.00M/92.3M [00:00<00:09, 8.94MB/s]adapter_model.safetensors:   8%|‚ñä         | 7.80M/92.3M [00:00<00:08, 10.0MB/s]adapter_model.safetensors:  10%|‚ñà         | 9.27M/92.3M [00:00<00:08, 10.0MB/s]adapter_model.safetensors:  12%|‚ñà‚ñè        | 10.8M/92.3M [00:01<00:07, 10.3MB/s]adapter_model.safetensors:  13%|‚ñà‚ñé        | 12.3M/92.3M [00:01<00:07, 10.6MB/s]adapter_model.safetensors:  15%|‚ñà‚ñç        | 13.8M/92.3M [00:01<00:07, 11.0MB/s]adapter_model.safetensors:  17%|‚ñà‚ñã        | 15.2M/92.3M [00:01<00:06, 11.2MB/s]adapter_model.safetensors:  18%|‚ñà‚ñä        | 16.4M/92.3M [00:01<00:12, 5.94MB/s]adapter_model.safetensors:  22%|‚ñà‚ñà‚ñè       | 20.1M/92.3M [00:02<00:06, 10.5MB/s]adapter_model.safetensors:  23%|‚ñà‚ñà‚ñé       | 21.7M/92.3M [00:02<00:06, 10.8MB/s]adapter_model.safetensors:  25%|‚ñà‚ñà‚ñå       | 23.1M/92.3M [00:02<00:06, 10.9MB/s]adapter_model.safetensors:  27%|‚ñà‚ñà‚ñã       | 24.6M/92.3M [00:02<00:06, 11.1MB/s]adapter_model.safetensors:  28%|‚ñà‚ñà‚ñä       | 26.0M/92.3M [00:02<00:05, 11.3MB/s]adapter_model.safetensors:  30%|‚ñà‚ñà‚ñâ       | 27.5M/92.3M [00:02<00:05, 11.4MB/s]adapter_model.safetensors:  31%|‚ñà‚ñà‚ñà‚ñè      | 29.0M/92.3M [00:02<00:05, 11.5MB/s]adapter_model.safetensors:  33%|‚ñà‚ñà‚ñà‚ñé      | 30.5M/92.3M [00:02<00:05, 11.6MB/s]adapter_model.safetensors:  35%|‚ñà‚ñà‚ñà‚ñç      | 32.0M/92.3M [00:03<00:05, 11.6MB/s]adapter_model.safetensors:  36%|‚ñà‚ñà‚ñà‚ñå      | 33.2M/92.3M [00:03<00:09, 6.54MB/s]adapter_model.safetensors:  39%|‚ñà‚ñà‚ñà‚ñâ      | 36.1M/92.3M [00:03<00:07, 7.22MB/s]adapter_model.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà      | 37.5M/92.3M [00:04<00:09, 6.00MB/s]adapter_model.safetensors:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 38.9M/92.3M [00:04<00:08, 6.00MB/s]adapter_model.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 40.4M/92.3M [00:04<00:07, 6.93MB/s]adapter_model.safetensors:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 41.9M/92.3M [00:04<00:06, 7.85MB/s]adapter_model.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 43.4M/92.3M [00:04<00:05, 8.65MB/s]adapter_model.safetensors:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 44.9M/92.3M [00:04<00:05, 9.43MB/s]adapter_model.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 46.3M/92.3M [00:05<00:04, 10.0MB/s]adapter_model.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 47.7M/92.3M [00:05<00:04, 10.4MB/s]adapter_model.safetensors:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 48.8M/92.3M [00:05<00:07, 6.14MB/s]adapter_model.safetensors:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 52.2M/92.3M [00:05<00:03, 10.1MB/s]adapter_model.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 53.6M/92.3M [00:05<00:03, 10.5MB/s]adapter_model.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 55.1M/92.3M [00:05<00:03, 10.8MB/s]adapter_model.safetensors:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 56.5M/92.3M [00:06<00:03, 11.0MB/s]adapter_model.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 58.0M/92.3M [00:06<00:03, 11.2MB/s]adapter_model.safetensors:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 59.5M/92.3M [00:06<00:02, 11.3MB/s]adapter_model.safetensors:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 61.0M/92.3M [00:06<00:02, 11.5MB/s]adapter_model.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 62.5M/92.3M [00:06<00:02, 11.6MB/s]adapter_model.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 63.9M/92.3M [00:06<00:02, 11.6MB/s]adapter_model.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 65.1M/92.3M [00:07<00:04, 6.44MB/s]adapter_model.safetensors:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 68.2M/92.3M [00:07<00:03, 6.43MB/s]adapter_model.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 69.6M/92.3M [00:08<00:04, 5.47MB/s]adapter_model.safetensors:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 71.1M/92.3M [00:08<00:03, 6.38MB/s]adapter_model.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 72.6M/92.3M [00:08<00:02, 7.29MB/s]adapter_model.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 74.1M/92.3M [00:08<00:02, 8.15MB/s]adapter_model.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 75.6M/92.3M [00:08<00:01, 8.96MB/s]adapter_model.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 77.0M/92.3M [00:08<00:01, 9.62MB/s]adapter_model.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 78.5M/92.3M [00:08<00:01, 10.2MB/s]adapter_model.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 80.0M/92.3M [00:08<00:01, 10.6MB/s]adapter_model.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 81.1M/92.3M [00:09<00:01, 6.35MB/s]adapter_model.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 84.2M/92.3M [00:09<00:01, 7.25MB/s]adapter_model.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 85.8M/92.3M [00:09<00:01, 6.54MB/s]adapter_model.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 87.1M/92.3M [00:10<00:00, 6.19MB/s]adapter_model.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 88.6M/92.3M [00:10<00:00, 7.11MB/s]adapter_model.safetensors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 90.1M/92.3M [00:10<00:00, 7.88MB/s]adapter_model.safetensors:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 91.7M/92.3M [00:10<00:00, 8.88MB/s]adapter_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92.3M/92.3M [00:11<00:00, 8.33MB/s]
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.027 MB of 0.048 MB uploaded (0.003 MB deduped)wandb: - 0.027 MB of 0.048 MB uploaded (0.003 MB deduped)wandb: \ 0.027 MB of 0.048 MB uploaded (0.003 MB deduped)wandb: | 0.048 MB of 0.048 MB uploaded (0.003 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 6.5%             
wandb: 
wandb: Run history:
wandb:                    train/epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:              train/global_step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:                train/grad_norm ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ  ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                     train/loss ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:               train/total_flos ‚ñÅ
wandb:               train/train_loss ‚ñÅ‚ñÅ
wandb:            train/train_runtime ‚ñÅ‚ñÅ
wandb: train/train_samples_per_second ‚ñÅ‚ñÅ
wandb:   train/train_steps_per_second ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    train/epoch 3.0
wandb:              train/global_step 126
wandb:                train/grad_norm 0.89241
wandb:            train/learning_rate 0.0002
wandb:                     train/loss 0.3087
wandb:               train/total_flos 1.1370193469079552e+16
wandb:               train/train_loss 0.56809
wandb:            train/train_runtime 243.0083
wandb: train/train_samples_per_second 8.259
wandb:   train/train_steps_per_second 0.519
wandb: 
wandb: üöÄ View run en.layer12024-03-11 09:49:26 at: https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2__adapters_en.layer1_4_torch.bfloat16_16_32_0.05_2_0.0002/runs/enz0uowv
wandb: Ô∏è‚ö° View job at https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2__adapters_en.layer1_4_torch.bfloat16_16_32_0.05_2_0.0002/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzYyMzkzMQ==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240311_094926-enz0uowv/logs
wandb: Appending key for api.wandb.ai to your netrc file: /home/ferrazzi/.netrc
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /extra/ferrazzi/llm/mistral_finetuning/wandb/run-20240311_095450-m0v5zl6r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run en.layer12024-03-11 09:54:50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2__adapters_en.layer1_4_torch.bfloat16_16_32_0.05_4_0.0002
wandb: üöÄ View run at https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2__adapters_en.layer1_4_torch.bfloat16_16_32_0.05_4_0.0002/runs/m0v5zl6r
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:05<00:11,  5.74s/it]Loading checkpoint shards:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:11<00:05,  5.68s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:16<00:00,  5.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:16<00:00,  5.39s/it]
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.
Map:   0%|          | 0/681 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:00<00:00, 3720.18 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681/681 [00:00<00:00, 3372.32 examples/s]
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:284: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.
  warnings.warn(
WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: üöÄ View run en.layer12024-03-11 09:54:50 at: https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2__adapters_en.layer1_4_torch.bfloat16_16_32_0.05_4_0.0002/runs/m0v5zl6r
wandb: Ô∏è‚ö° View job at https://wandb.ai/ferrazzipietro/Mistral-7B-Instruct-v0.2__adapters_en.layer1_4_torch.bfloat16_16_32_0.05_4_0.0002/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzYyNTY2Nw==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240311_095450-m0v5zl6r/logs
Traceback (most recent call last):
  File "finetuning_iterative_mistral.py", line 226, in <module>
    main(ADAPTERS_CHECKPOINT,
  File "finetuning_iterative_mistral.py", line 152, in main
    trainer = SFTTrainer(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/trl/trainer/sft_trainer.py", line 289, in __init__
    super().__init__(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/trainer.py", line 489, in __init__
    self._move_model_to_device(model, args.device)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/trainer.py", line 730, in _move_model_to_device
    model = model.to(device)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 6 more times]
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
NotImplementedError: Cannot copy out of meta tensor; no data!
