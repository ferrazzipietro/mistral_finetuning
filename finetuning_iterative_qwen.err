2024-03-11 08:56:27.119849: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-11 08:56:27.204138: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-11 08:56:29.359327: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'
  warnings.warn(
wandb: Currently logged in as: ferrazzipietro. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /home/ferrazzi/.netrc
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /extra/ferrazzi/llm/mistral_finetuning/wandb/run-20240311_085640-ea5rj7qx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run en.layer12024-03-11 08:56:40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ferrazzipietro/Qwen1.5-14B-Chat__adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002
wandb: üöÄ View run at https://wandb.ai/ferrazzipietro/Qwen1.5-14B-Chat__adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002/runs/ea5rj7qx
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|‚ñà‚ñé        | 1/8 [00:02<00:20,  2.96s/it]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:05<00:17,  2.91s/it]Loading checkpoint shards:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:08<00:14,  2.89s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:11<00:11,  2.90s/it]Loading checkpoint shards:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:14<00:08,  2.91s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:17<00:05,  2.92s/it]Loading checkpoint shards:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:19<00:02,  2.73s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:21<00:00,  2.25s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:21<00:00,  2.63s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/249 [00:00<?, ?it/s]/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
wandb: üöÄ View run en.layer12024-03-11 08:56:40 at: https://wandb.ai/ferrazzipietro/Qwen1.5-14B-Chat__adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002/runs/ea5rj7qx
wandb: Ô∏è‚ö° View job at https://wandb.ai/ferrazzipietro/Qwen1.5-14B-Chat__adapters_en.layer1_8_torch.bfloat16_16_32_0.05_4_0.0002/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjY4ODk0Ng==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240311_085640-ea5rj7qx/logs
Traceback (most recent call last):
  File "finetuning_iterative_qwen.py", line 220, in <module>
    main(ADAPTERS_CHECKPOINT,
  File "finetuning_iterative_qwen.py", line 173, in main
    trainer.train()
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/trl/trainer/sft_trainer.py", line 317, in train
    output = super().train(*args, **kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/trainer.py", line 1615, in train
    return inner_training_loop(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/trainer.py", line 1961, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/transformers/trainer.py", line 2911, in training_step
    self.accelerator.backward(loss)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/accelerate/accelerator.py", line 1962, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/home/ferrazzi/.conda/envs/gpu_venv_lates/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py", line 483, in backward
    undo_layout(state.CxB, state.tile_indices)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.90 GiB (GPU 0; 47.54 GiB total capacity; 41.61 GiB already allocated; 686.69 MiB free; 45.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
